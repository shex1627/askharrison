{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdf_path = '../data/pdfs/2311.01449.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.crawl.arXivPDFParser import ArXivPDFParser\n",
    "from askharrison.crawl.utils import request_soup, request_content\n",
    "from askharrison.crawl.html_to_text import html_to_text\n",
    "\n",
    "import pandas as pd\n",
    "# set pandas row width \n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arxiv_num = \"2309.15217\" # error\n",
    "arxiv_num = \"2303.03199\"\n",
    "arxiv_num = \"2310.06117\"\n",
    "arxiv_num = \"2212.00616\"\n",
    "arxiv_num = \"2212.06094\"\n",
    "arxiv_num = \"2310.20111\" # error\n",
    "arxiv_num = \"2307.11760\"\n",
    "arxiv_num = \"2403.10131\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186776"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = request_content(f\"https://www.arxiv-vanity.com/papers/{arxiv_num}/\").decode('utf-8')\n",
    "len(content)\n",
    "\n",
    "\n",
    "content = request_content(f\"https://arxiv.org/html/{arxiv_num}v1\").decode('utf-8')\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/html/{arxiv_num}.html\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/markdown/{arxiv_num}.md\", \"w\") as f:\n",
    "    f.write(html_to_text(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"../data/html/paper1.html\", \"r\") as f:\n",
    "#     content = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_markdown = html_to_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * RAFT: Adapting Language Model to Domain Specific RAG\n",
      "  * RAFT: Adapting Language Model to Domain Specific RAG\n",
      "    * 1 Introduction\n",
      "    * 2 LLMs for Open-Book Exam\n",
      "    * 3 RAFT\n",
      "    * 4 Evaluation\n",
      "      * 4.1 Datasets\n",
      "      * 4.2 Results\n",
      "      * 4.3 Effect of CoT\n",
      "      * 4.4 Qualitative Analysis\n",
      "      * 4.5 Should we train the LLM always with the oracle context for RAG?\n",
      "    * 5 RAFT Generalizes to Top-K RAG\n",
      "      * 5.1 Making Model Robust to top-K RAG\n",
      "    * 6 Related Works\n",
      "    * 7 Conclusion\n",
      "    * References\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from marko import Markdown\n",
    "\n",
    "markdown = Markdown(extensions=['toc'])\n",
    "\n",
    "markdown(content_markdown)\n",
    "print(html_to_text(markdown.renderer.render_toc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.crawl.markdownParser import MarkdownParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_parser = MarkdownParser()\n",
    "markdown_parser.parse_markdown_str(content_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Refer to caption', 'src': 'x1.png'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_parser.contents[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. [1 Introduction](https://arxiv.org/html/2403.10131v1#S1 \"1 Introduction ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  2. [2 LLMs for Open-Book Exam](https://arxiv.org/html/2403.10131v1#S2 \"2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    1. [Closed-Book Exam](https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px1 \"Closed-Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    2. [Open Book Exam](https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px2 \"Open Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    3. [Domain Specific Open-Book Exam](https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px3 \"Domain Specific Open-Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  3. [3 RAFT](https://arxiv.org/html/2403.10131v1#S3 \"3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  4. [4 Evaluation](https://arxiv.org/html/2403.10131v1#S4 \"4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    1. [4.1 Datasets](https://arxiv.org/html/2403.10131v1#S4.SS1 \"4.1 Datasets ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "      1. [Baselines](https://arxiv.org/html/2403.10131v1#S4.SS1.SSS0.Px1 \"Baselines ‣ 4.1 Datasets ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    2. [4.2 Results](https://arxiv.org/html/2403.10131v1#S4.SS2 \"4.2 Results ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    3. [4.3 Effect of CoT](https://arxiv.org/html/2403.10131v1#S4.SS3 \"4.3 Effect of CoT ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    4. [4.4 Qualitative Analysis](https://arxiv.org/html/2403.10131v1#S4.SS4 \"4.4 Qualitative Analysis ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    5. [4.5 Should we train the LLM always with the oracle context for RAG?](https://arxiv.org/html/2403.10131v1#S4.SS5 \"4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  5. [5 RAFT Generalizes to Top-K RAG](https://arxiv.org/html/2403.10131v1#S5 \"5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "    1. [5.1 Making Model Robust to top-K RAG](https://arxiv.org/html/2403.10131v1#S5.SS1 \"5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  6. [6 Related Works](https://arxiv.org/html/2403.10131v1#S6 \"6 Related Works ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "  7. [7 Conclusion](https://arxiv.org/html/2403.10131v1#S7 \"7 Conclusion ‣ RAFT: Adapting Language Model to Domain Specific RAG\")\n",
      "\n",
      "License: CC BY 4.0\n",
      "\n",
      "arXiv:2403.10131v1 [cs.CL] 15 Mar 2024\n",
      "\n",
      "# RAFT: Adapting Language Model to Domain Specific RAG\n",
      "\n",
      "Tianjun Zhang  Shishir G. Patil  Naman Jain  Sheng Shen  Matei Zaharia  Ion\n",
      "Stoica  Joseph E. Gonzalez\n",
      "\n",
      "# RAFT: Adapting Language Model to Domain Specific RAG\n",
      "\n",
      "Tianjun Zhang  Shishir G. Patil  Naman Jain  Sheng Shen  Matei Zaharia  Ion\n",
      "Stoica  Joseph E. Gonzalez\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Pretraining Large Language Models (LLMs) on large corpora of textual data is\n",
      "now a standard paradigm. When using these LLMs for many downstream\n",
      "applications, it is common to additionally bake in new knowledge (e.g., time-\n",
      "critical news, or private domain knowledge) into the pretrained model either\n",
      "through RAG-based-prompting, or finetuning. However, the optimal methodology\n",
      "for the model to gain such new knowledge remains an open question. In this\n",
      "paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe\n",
      "that improves the model’s ability to answer questions in an \"open-book\" in-\n",
      "domain setting. In RAFT, given a question, and a set of retrieved documents,\n",
      "we train the model to ignore those documents that don’t help in answering the\n",
      "question, which we call, distractor documents. RAFT accomplishes this by\n",
      "citing verbatim the right sequence from the relevant document that would help\n",
      "answer the question. This coupled with RAFT’s chain-of-thought-style response\n",
      "helps improve the model’s ability to reason. In domain specific RAG, RAFT\n",
      "consistently improves the model’s performance across PubMed, HotpotQA, and\n",
      "Gorilla datasets, presenting a post-training recipe to improve pre-trained\n",
      "LLMs to in-domain RAG. RAFT’s code and demo are open-sourced at\n",
      "<https://github.com/ShishirPatil/gorilla>\n",
      "\n",
      "![Refer to caption](x1.png) Figure 1: How best to prepare for an Exam?(a)\n",
      "Fine-tuning based approaches implement \"studying\" by either directly\n",
      "\"memorizing\" the input documents or answering practice QA without referencing\n",
      "the documents. (b) Alternatively, in-context retrieval methods fail to\n",
      "leverage the learning opportunity afforded by the fixed domain and are\n",
      "equivalent to taking an open-book exam without studying. While these\n",
      "approaches leverage in-domain learning, they fail to prepare for open-book\n",
      "tests. In contrast, our approach (c) RAFT leverages fine-tuning with question-\n",
      "answer pairs while referencing the documents in a simulated imperfect\n",
      "retrieval setting — thereby effectively preparing for the open-book exam\n",
      "setting.\n",
      "\n",
      "Machine Learning, ICML\n",
      "\n",
      "\\minted@def@optcl\n",
      "\n",
      "envname-P envname#1\n",
      "\n",
      "tianjunz@berkeley.edu, shishirpatil@berkeley.edu\n",
      "\n",
      "UC Berkeley\n",
      "\n",
      "  \n",
      "\n",
      "##  1 Introduction\n",
      "\n",
      "Trained on vast quantities of public data, Large Language Models LLMs have\n",
      "achieved significant advances in a wide range of general knowledge reasoning\n",
      "tasks (Brown et al., [2020](https://arxiv.org/html/2403.10131v1#bib.bib5); Wei\n",
      "et al., [2022](https://arxiv.org/html/2403.10131v1#bib.bib52)).\n",
      "\n",
      "However, increasingly LLMs are being employed in specialized domains to\n",
      "support tasks ranging from code completion for specific software frameworks to\n",
      "question answering on specific document collections (e.g., legal or medical\n",
      "documents). In these settings, general knowledge reasoning is less critical\n",
      "but instead, the primary goal is to maximize accuracy based on a given set of\n",
      "documents. Indeed, adapting LLMs to the specialized domains (e.g., recent\n",
      "news, enterprise private documents, or program resources constructed after the\n",
      "training cutoff) is essential to many emerging applications (Vu et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib49); Lazaridou et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib22)) and is the focus of\n",
      "this work.\n",
      "\n",
      "This paper studies the following question – _How to adapt pre-trained LLMs for\n",
      "Retrieval Augmented Generation (RAG) in specialized domains?_\n",
      "\n",
      "When it comes to adapting LLMs to specialized domains, we consider the\n",
      "following two candidates: in-context learning through Retrieval-Augmented\n",
      "Generation (RAG) and supervised fine-tuning. RAG-based methods allow the LLM\n",
      "to reference the documents when answering questions. However, these methods\n",
      "fail to leverage the learning opportunity afforded by the fixed domain setting\n",
      "and early access to the test documents. Alternatively, supervised fine-tuning\n",
      "offers the opportunity to learn more general patterns in the documents and\n",
      "better align to end tasks and user preferences (Zhou et al.,\n",
      "[2023a](https://arxiv.org/html/2403.10131v1#bib.bib59)). However, existing\n",
      "fine-tuning based approaches either fail to leverage the documents at test\n",
      "time (don’t incorporate RAG) or fail to account for the imperfections in the\n",
      "retrieval process during training.\n",
      "\n",
      "We can draw an analogy to an open-book exam. Existing in-context retrieval\n",
      "methods are equivalent to taking an open-book exam without studying.\n",
      "Alternatively, existing fine-tuning based approaches implement “studying\" by\n",
      "either directly “memorizing\" (Xiong et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib55)) the input documents or\n",
      "answering practice questions (Wang et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib51)) without referencing the\n",
      "documents. While these approaches leverage in-domain learning they fail to\n",
      "prepare for the open-book nature of test setting.\n",
      "\n",
      "In this paper, we study how to combine supervised fine-tuning (SFT) with\n",
      "retrieval augmented generation (RAG). We propose a novel adaptation strategy –\n",
      "Retrieval-Augmented Fine Tuning (RAFT). RAFT specifically addresses the\n",
      "challenge of fine-tuning LLMs to incorporate domain knowledge while also\n",
      "improving in-domain RAG performance. RAFT aims to not only enable models to\n",
      "learn domain specific knowledge through fine-tuning, but also to ensure\n",
      "robustness against inaccurate retrievals. This is achieved by training the\n",
      "models to understand the dynamics between the question posed (prompt), the\n",
      "domain specific documents retrieved, and the appropriate answer. Going back to\n",
      "our analogy, our approach is analogous to studying for an open-book exam by\n",
      "recognizing relevant, and irrelevant retrieved documents.\n",
      "\n",
      "In RAFT, we train the model to answer the question (Q) from Document(s) (D*)\n",
      "to generate an answer (A*), where A* includes chain-of-thought (Wei et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib52); Anthropic,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib2)), and in the presence of\n",
      "distractor documents (Dksubscript𝐷𝑘D_{k}italic_D start_POSTSUBSCRIPT italic_k\n",
      "end_POSTSUBSCRIPT). We explain the methodology in detail in Section\n",
      "[3](https://arxiv.org/html/2403.10131v1#S3 \"3 RAFT ‣ RAFT: Adapting Language\n",
      "Model to Domain Specific RAG\") and analyze the sensitivity to the number of\n",
      "distractor documents (k𝑘kitalic_k) at train- and test- time in Section\n",
      "[5](https://arxiv.org/html/2403.10131v1#S5 \"5 RAFT Generalizes to Top-K RAG ‣\n",
      "RAFT: Adapting Language Model to Domain Specific RAG\"). RAFT consistently\n",
      "outperforms Supervised-finetuning both with- and without- RAG across PubMed\n",
      "(Dernoncourt & Lee, [2017](https://arxiv.org/html/2403.10131v1#bib.bib10)),\n",
      "HotpotQA (Yang et al., [2018](https://arxiv.org/html/2403.10131v1#bib.bib57)),\n",
      "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets (Patil et\n",
      "al., [2023](https://arxiv.org/html/2403.10131v1#bib.bib38)), presenting a\n",
      "novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\n",
      "\n",
      "##  2 LLMs for Open-Book Exam\n",
      "\n",
      "To understand our goal better, we expand on our analogy between training an\n",
      "LLM in the real-world setting of preparing for an exam.\n",
      "\n",
      "#### Closed-Book Exam\n",
      "\n",
      "A closed book exam often refers to a scenario where the LLMs do not have\n",
      "access to any additional documents or references to answer the questions\n",
      "during the exam. For LLMs, this is equivalent to the scenario, for example, in\n",
      "which the LLM is used as a chatbot. In this scenario, the LLM draws from the\n",
      "knowledge baked in during pre-training and supervised finetuning to respond to\n",
      "the prompt.\n",
      "\n",
      "#### Open Book Exam\n",
      "\n",
      "In contrast, we liken the open-book exam setting to the scenario in which the\n",
      "LLM can refer to external sources of information (e.g., a website or a book\n",
      "chapter). In such scenarios, typically, the LLM is paired with a retriever\n",
      "which retrieves ‘k’ documents (or specific segments of the document) which are\n",
      "appended to the prompt. It is only through these documents retrieved that the\n",
      "LLM gains access to “new knowledge”. As a result, we argue that the LLM’s\n",
      "performance in these settings, where it is trained as a general-purpose LLM is\n",
      "largely dependent on the quality of the retriever and how accurately the\n",
      "retriever can identify the most relevant piece of information.\n",
      "\n",
      "#### Domain Specific Open-Book Exam\n",
      "\n",
      "In this paper, we focused on a narrower but increasingly popular domain than\n",
      "the general open book exam, called the domain specific open book exam. In\n",
      "domain specific open book exams, we know apriori the domain in which the LLM\n",
      "will be tested – used for inference. The LLM can respond to the prompt using\n",
      "use any and all information from this specific domain, which it has been fine-\n",
      "tuned on. Examples of domain specific examples include enterprise documents,\n",
      "latest news, code repositories belonging to an organization, etc. In all these\n",
      "scenarios, the LLM will be used to respond to the questions, whose answers can\n",
      "be found within a collection of documents (a small practical domain). The\n",
      "retrieval technique itself has little to no impact on the mechanism (though it\n",
      "may impact the accuracy). This paper mainly studies this, domain specific\n",
      "open-book setting and how to adapt a pretrained LLM to this specific domain,\n",
      "including how to make it more robust to a varying number of retrieved\n",
      "documents and distractors.\n",
      "\n",
      "##  3 RAFT\n",
      "\n",
      "![Refer to caption](extracted/5471482/figures/RAFT.png) Figure 2: Overview of\n",
      "our RAFT method. The top-left figure depicts our approach of adapting LLMs to\n",
      "reading solution from a set of positive and negative documents in contrast to\n",
      "standard RAG setup where models are trained based on the retriever outputs,\n",
      "which is a mixture of both memorization and reading. At test time, all methods\n",
      "follow the standard RAG setting, provided with a top-k retrieved documents in\n",
      "the context.\n",
      "\n",
      "In this section, we present RAFT, a novel way of training LLMs for domain\n",
      "specific open-book exams. We first introduce the classical technique of\n",
      "supervised fine-tuning, followed by the key takeaways from our experiments.\n",
      "Then, we introduce RAFT , a modified version of general instruction tuning.\n",
      "Lastly, we provide an overview of the experiments to expect in the later\n",
      "sections.\n",
      "\n",
      "Supervised Finetuning\n",
      "\n",
      "Consider the supervised fine-tuning (SFT) setting for a Question-Answer\n",
      "dataset. The formulation consists of the Dataset (D𝐷Ditalic_D) from which a\n",
      "set of Question (Q𝑄Qitalic_Q) and corresponding answer (A𝐴Aitalic_A) pairs are\n",
      "derived or already available. In the classical SFT setting, the model is\n",
      "trained to improve its ability to answer the questions based on its knowledge\n",
      "- obtained either during pre-training, or during the SFT training phase. The\n",
      "model so trained can also be used at test-time with the Retrieval Augmented\n",
      "Generation (RAG) setting, where additional documents can be introduced in the\n",
      "prompt to help the model answer the question. This can be represented as\n",
      "follows:\n",
      "\n",
      "  * •\n",
      "\n",
      "Train: 𝐐𝐐{\\mathbf{Q}}bold_Q →→\\to→ 𝐀𝐀{\\mathbf{A}}bold_A\n",
      "\n",
      "  * •\n",
      "\n",
      "0-shot Inference: 𝐐𝐐{\\mathbf{Q}}bold_Q →→\\to→ 𝐀𝐀{\\mathbf{A}}bold_A\n",
      "\n",
      "  * •\n",
      "\n",
      "RAG Inference: 𝐐𝐐{\\mathbf{Q}}bold_Q \\+ 𝐃𝐃{\\mathbf{D}}bold_D →→\\to→\n",
      "𝐀𝐀{\\mathbf{A}}bold_A\n",
      "\n",
      "RAFT\n",
      "\n",
      "Retrieval Aware Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\n",
      "tuning data to tailor the models for domain specific open-book settings,\n",
      "equivalent to in-domain RAG In RAFT, we prepare the training data such that\n",
      "each data point contains a question (Q𝑄Qitalic_Q), a set of documents\n",
      "(Dksubscript𝐷𝑘D_{k}italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT),\n",
      "and a corresponding Chain-of-though style answer (A*superscript𝐴A^{*}italic_A\n",
      "start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT) generated from one of the\n",
      "document (D*superscript𝐷D^{*}italic_D start_POSTSUPERSCRIPT *\n",
      "end_POSTSUPERSCRIPT). We differentiate between two types of documents:\n",
      "‘oracle’ documents (D*D*italic_D *) i.e. the documents from which the answer\n",
      "to the question can be deduced, and ‘distractor’ documents\n",
      "(Disubscript𝐷𝑖D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT)\n",
      "that do not contain answer-relevant information. As an implementation detail,\n",
      "the ‘oracle’ document doesn’t need to be a single document, but can be more\n",
      "than one document, as is the case in HotpotQA (Yang et al.,\n",
      "[2018](https://arxiv.org/html/2403.10131v1#bib.bib57)). Then, for P𝑃Pitalic_P\n",
      "fraction of the questions (qisubscript𝑞𝑖q_{i}italic_q start_POSTSUBSCRIPT\n",
      "italic_i end_POSTSUBSCRIPT) in the dataset, we retain the oracle document\n",
      "(di*superscriptsubscript𝑑𝑖d_{i}^{*}italic_d start_POSTSUBSCRIPT italic_i\n",
      "end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT) along with\n",
      "distractor documents (dk−1subscript𝑑𝑘1d_{k-1}italic_d start_POSTSUBSCRIPT\n",
      "italic_k - 1 end_POSTSUBSCRIPT). For (1−P)1𝑃(1-P)( 1 - italic_P ) fraction of\n",
      "the questions (qisubscript𝑞𝑖q_{i}italic_q start_POSTSUBSCRIPT italic_i\n",
      "end_POSTSUBSCRIPT) in the dataset, we include no oracle document and only\n",
      "include distractor documents (dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT\n",
      "italic_k end_POSTSUBSCRIPT). We then fine-tune the language model using the\n",
      "standard supervised training (SFT) technique, training it to generate answers\n",
      "from the provided documents and questions. Fig.\n",
      "[2](https://arxiv.org/html/2403.10131v1#S3.F2 \"Figure 2 ‣ 3 RAFT ‣ RAFT:\n",
      "Adapting Language Model to Domain Specific RAG\") illustrates the high-level\n",
      "design principal for RAFT .\n",
      "\n",
      "We demonstrate that our approach trains the model to perform better RAG on the\n",
      "set of documents it is trained on _i.e., in-domain_. By removing the oracle\n",
      "documents in some instances, we are compelling the model to memorize answers\n",
      "instead of deriving them from the context. The training data for RAFT is as\n",
      "follows, and an example of training data can be seen in Fig.\n",
      "[3](https://arxiv.org/html/2403.10131v1#S3.F3 \"Figure 3 ‣ 3 RAFT ‣ RAFT:\n",
      "Adapting Language Model to Domain Specific RAG\"):\n",
      "\n",
      "  * •\n",
      "\n",
      "𝐏𝐏{\\mathbf{P}}bold_P % of data: 𝐐𝐐{\\mathbf{Q}}bold_Q \\+\n",
      "𝐃*superscript𝐃{\\mathbf{D}}^{*}bold_D start_POSTSUPERSCRIPT *\n",
      "end_POSTSUPERSCRIPT \\+ 𝐃2subscript𝐃2{\\mathbf{D}}_{2}bold_D start_POSTSUBSCRIPT\n",
      "2 end_POSTSUBSCRIPT \\+ ……\\dots… \\+ 𝐃ksubscript𝐃𝑘{\\mathbf{D}}_{k}bold_D\n",
      "start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT →→\\to→ 𝐀*{\\mathbf{A}}*bold_A *\n",
      "\n",
      "  * •\n",
      "\n",
      "(1−𝐏1𝐏1-{\\mathbf{P}}1 - bold_P) % of data: 𝐐𝐐{\\mathbf{Q}}bold_Q \\+\n",
      "𝐃1subscript𝐃1{\\mathbf{D}}_{1}bold_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \\+\n",
      "𝐃2subscript𝐃2{\\mathbf{D}}_{2}bold_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \\+\n",
      "……\\dots… \\+ 𝐃ksubscript𝐃𝑘{\\mathbf{D}}_{k}bold_D start_POSTSUBSCRIPT italic_k\n",
      "end_POSTSUBSCRIPT →→\\to→ 𝐀*{\\mathbf{A}}*bold_A *\n",
      "\n",
      "Subsequently, for the test scenario, the model is provided with the Q and\n",
      "top-k documents retrieved by the RAG pipeline. Note that RAFT is independent\n",
      "of the retriever used.\n",
      "\n",
      "A key factor in enhancing training quality is the generation of a reasoning\n",
      "process, such as Chain-of-Thought, to explain the provided answers.RAFT\n",
      "approach is similar: we demonstrate that creating a full reasoning chain and\n",
      "in addition, clearly citing sources enhances the model’s accuracy in answering\n",
      "questions. In Fig. [3](https://arxiv.org/html/2403.10131v1#S3.F3 \"Figure 3 ‣ 3\n",
      "RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG\"), we illustrate\n",
      "this set-up. Generating the training data in this fashion, involves presenting\n",
      "the model with a question, context, and verified answers, and then requesting\n",
      "it to form a reasoning chain that appropriately references the original\n",
      "context.\n",
      "\n",
      "For all the datasets in our experiments, we generate the answers using the\n",
      "technique described above. Note that the Gorilla APIBench dataset, already\n",
      "includes reasoning in the answers. We provide an example of the generation\n",
      "step in Fig. [3](https://arxiv.org/html/2403.10131v1#S3.F3 \"Figure 3 ‣ 3 RAFT\n",
      "‣ RAFT: Adapting Language Model to Domain Specific RAG\"), the detailed\n",
      "reasoning answer includes a citation from the original context inside\n",
      "##begin_quote## and ##end_quote## as well as the detailed explanation on how\n",
      "to reach the conclusion based on the citations. We demonstrate that adding\n",
      "detailed reasoning paragraphs helps boost the model’s performance in our\n",
      "experiment section.\n",
      "\n",
      "[⬇](data:text/plain;base64,CnxcdGV4dGJme1F1ZXN0aW9uOn18IFRoZSBPYmVyb2kgZmFtaWx5IGlzIHBhcnQgb2YgYSBob3RlbCBjb21wYW55IHRoYXQgaGFzIGEgaGVhZCBvZmZpY2UgaW4gd2hhdCBjaXR5PwpccGFyfFx0ZXh0YmZ7Y29udGV4dDp9fCBbVGhlIE9iZXJvaSBmYW1pbHkgaXMgYW4gSW5kaWFuIGZhbWlseSB0aGF0IGlzIGZhbW91cyBmb3IgaXRzIGludm9sdmVtZW50IGluIGhvdGVscywgbmFtZWx5IHRocm91Z2ggVGhlIE9iZXJvaSBHcm91cF0uLi5bSXQgaXMgbG9jYXRlZCBpbiB0aGUgY2l0eSBjZW50ZXIgb2YgSmFrYXJ0YSwgbmVhciBNZWdhIEt1bmluZ2FuLCBhZGphY2VudCB0byB0aGUgc2lzdGVyIEpXIE1hcnJpb3R0IEhvdGVsLiBJdCBpcyBvcGVyYXRlZCBieSBUaGUgUml0ei1DYXJsdG9uIEhvdGVsIENvbXBhbnkuIFRoZSBjb21wbGV4IGhhcyB0d28gdG93ZXJzIHRoYXQgY29tcHJpc2UgYSBob3RlbCBhbmQgdGhlIEFpcmxhbmdnYSBBcGFydG1lbnQgcmVzcGVjdGl2ZWx5XS4uLltUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS5dClxwYXJ8XHRleHRiZntJbnN0cnVjdGlvbjp9fCBHaXZlbiB0aGUgcXVlc3Rpb24sIGNvbnRleHQsIGFuZCBhbnN3ZXIgYWJvdmUsIHByb3ZpZGUgYSBsb2dpY2FsIHJlYXNvbmluZyBmb3IgdGhhdCBhbnN3ZXIuIFBsZWFzZSB1c2UgdGhlIGZvcm1hdCBvZjogIyNSZWFzb246IHtyZWFzb259ICMjQW5zd2VyOiB7YW5zd2VyfS4KXHBhci0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tClxwYXJ8XHRleHRiZntDb1QgQW5zd2VyOn18ICMjUmVhc29uOiBUaGUgZG9jdW1lbnQgIyNiZWdpbl9xdW90ZSMjIFRoZSBPYmVyb2kgZmFtaWx5IGlzIGFuIEluZGlhbiBmYW1pbHkgdGhhdCBpcyBmYW1vdXMgZm9yIGl0cyBpbnZvbHZlbWVudCBpbiBob3RlbHMsIG5hbWVseSB0aHJvdWdoIFRoZSBPYmVyb2kgR3JvdXAuICMjZW5kX3F1b3RlIyMgZXN0YWJsaXNoZXMgdGhhdCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBpbnZvbHZlZCBpbiB0aGUgT2Jlcm9pIGdyb3VwLCBhbmQgdGhlIGRvY3VtZW50ICMjYmVnaW5fcXVvdGUjIyBUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS4gIyNlbmRfcXVvdGUjIyBlc3RhYmxpc2hlcyB0aGUgaGVhZCBvZmZpY2Ugb2YgVGhlIE9iZXJvaSBHcm91cC4gVGhlcmVmb3JlLCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBwYXJ0IG9mIGEgaG90ZWwgY29tcGFueSB3aG9zZSBoZWFkIG9mZmljZSBpcyBpbiBEZWxoaS4gIyNBbnN3ZXI6IERlbGhpClxwYXI=)\n",
      "\n",
      "|\\textbf{Question:}| The Oberoi family is part of a hotel company that has a\n",
      "head office in what city?\n",
      "\n",
      "\\par|\\textbf{context:}| [The Oberoi family is an Indian family that is famous\n",
      "for its involvement in hotels, namely through The Oberoi Group]…[It is located\n",
      "in the city center of Jakarta, near Mega Kuningan, adjacent to the sister JW\n",
      "Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex\n",
      "has two towers that comprise a hotel and the Airlangga Apartment\n",
      "respectively]…[The Oberoi Group is a hotel company with its head office in\n",
      "Delhi.]\n",
      "\n",
      "\\par|\\textbf{Instruction:}| Given the question, context, and answer above,\n",
      "provide a logical reasoning for that answer. Please use the format of:\n",
      "##Reason: {reason} ##Answer: {answer}.\n",
      "\n",
      "\\par————————————————————————————–\n",
      "\n",
      "\\par|\\textbf{CoT Answer:}| ##Reason: The document ##begin_quote## The Oberoi\n",
      "family is an Indian family that is famous for its involvement in hotels,\n",
      "namely through The Oberoi Group. ##end_quote## establishes that the Oberoi\n",
      "family is involved in the Oberoi group, and the document ##begin_quote## The\n",
      "Oberoi Group is a hotel company with its head office in Delhi. ##end_quote##\n",
      "establishes the head office of The Oberoi Group. Therefore, the Oberoi family\n",
      "is part of a hotel company whose head office is in Delhi. ##Answer: Delhi\n",
      "\n",
      "\\par\n",
      "\n",
      "  \n",
      "Figure 3: RAFT prompt to help LLM evaluate its own generated reasoning and\n",
      "answers, contrasting them with the correct reasoning and answers. The LLM is\n",
      "prompted to identify errors in its reasoning and extract key insights for\n",
      "improvement. This figure specifically represents the ‘GenerateExplanation‘\n",
      "step in the RAFT algorithm ([Section 3](https://arxiv.org/html/2403.10131v1#S3\n",
      "\"3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG\")). Table 1:\n",
      "RAFT improves RAG performance forall specialized domains: Across PubMed,\n",
      "HotpotQA, HuggingFace, Torch Hub, and Tensorflow Hub, we see that domain\n",
      "specific Finetuning improves significantly of the performance of the base\n",
      "model, but RAFT consistently outperforms the existing domain specific\n",
      "finetuning method with or without RAG. This suggests the need to train the\n",
      "model with context. We compare our model with LLaMA finetuning receipes, and\n",
      "provide GPT-3.5 for reference. |\n",
      "\n",
      "PubMed\n",
      "\n",
      "|\n",
      "\n",
      "HotpotQA\n",
      "\n",
      "|\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "|\n",
      "\n",
      "Torch Hub\n",
      "\n",
      "|\n",
      "\n",
      "TensorFlow Hub  \n",
      "  \n",
      "---|---|---|---|---|---  \n",
      "  \n",
      "GPT-3.5 + RAG\n",
      "\n",
      "|\n",
      "\n",
      "71.60\n",
      "\n",
      "| 41.5 |\n",
      "\n",
      "29.08\n",
      "\n",
      "|\n",
      "\n",
      "60.21\n",
      "\n",
      "|\n",
      "\n",
      "65.59  \n",
      "  \n",
      "LLaMA2-7B\n",
      "\n",
      "|\n",
      "\n",
      "56.5\n",
      "\n",
      "|\n",
      "\n",
      "0.54\n",
      "\n",
      "|\n",
      "\n",
      "0.22\n",
      "\n",
      "|\n",
      "\n",
      "0\n",
      "\n",
      "|\n",
      "\n",
      "0  \n",
      "  \n",
      "LLaMA2-7B + RAG\n",
      "\n",
      "|\n",
      "\n",
      "58.8\n",
      "\n",
      "|\n",
      "\n",
      "0.03\n",
      "\n",
      "|\n",
      "\n",
      "26.43\n",
      "\n",
      "|\n",
      "\n",
      "08.60\n",
      "\n",
      "|\n",
      "\n",
      "43.06  \n",
      "  \n",
      "DSF\n",
      "\n",
      "|\n",
      "\n",
      "59.7\n",
      "\n",
      "|\n",
      "\n",
      "6.38\n",
      "\n",
      "|\n",
      "\n",
      "61.06\n",
      "\n",
      "|\n",
      "\n",
      "84.94\n",
      "\n",
      "|\n",
      "\n",
      "86.56  \n",
      "  \n",
      "DSF + RAG\n",
      "\n",
      "|\n",
      "\n",
      "71.6\n",
      "\n",
      "|\n",
      "\n",
      "4.41\n",
      "\n",
      "|\n",
      "\n",
      "42.59\n",
      "\n",
      "|\n",
      "\n",
      "82.80\n",
      "\n",
      "|\n",
      "\n",
      "60.29  \n",
      "  \n",
      "RAFT (LLaMA2-7B)\n",
      "\n",
      "| 73.30 |\n",
      "\n",
      "35.28\n",
      "\n",
      "| 74.00 | 84.95 | 86.86  \n",
      "  \n",
      "##  4 Evaluation\n",
      "\n",
      "We design our experiments to study how well RAFT performs compared to various\n",
      "baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is\n",
      "better at reading and extracting information from in-domain documents, than\n",
      "domain specific finetuned model, and general-purpose model with RAG. As an\n",
      "ablation, we also demonstrate how important it is for the model to learn with\n",
      "Chain-of-Thought responses. In this section, we will first introduce all the\n",
      "datasets we used in the experiments, then all the baseline model/fine-tuning\n",
      "techniques that we benchmark against.\n",
      "\n",
      "###  4.1 Datasets\n",
      "\n",
      "In our experiments, we use the following datasets to evaluate our model and\n",
      "all baselines. We selected these datasets to represent both popular and\n",
      "diverse domains including Wikipedia, Coding/API documents, and question-\n",
      "answering on medical documents.\n",
      "\n",
      "  * •\n",
      "\n",
      "Natural Questions (NQ) (Kwiatkowski et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib21)), Trivia QA (Joshi et\n",
      "al., [2017](https://arxiv.org/html/2403.10131v1#bib.bib18)) and HotpotQA (Yang\n",
      "et al., [2018](https://arxiv.org/html/2403.10131v1#bib.bib57)) are the open-\n",
      "domain question-answers based on Wikipedia, mainly focused on common knowledge\n",
      "(e.g., movies, sports, etc).\n",
      "\n",
      "  * •\n",
      "\n",
      "HuggingFace, Torch Hub, and TensorFlow Hub are from the APIBench (Patil et\n",
      "al., [2023](https://arxiv.org/html/2403.10131v1#bib.bib38)) proposed in the\n",
      "Gorilla paper. These benchmarks measure how to generate the correct,\n",
      "functional, and executable API calls based on the documentation.\n",
      "\n",
      "  * •\n",
      "\n",
      "PubMed QA (Jin et al., [2019](https://arxiv.org/html/2403.10131v1#bib.bib17))\n",
      "is a question-answering dataset tailored only for biomedical-research\n",
      "question-answering. It mainly focuses on answering medical and biology\n",
      "questions based on a given set of documents.\n",
      "\n",
      "Note that the first category of dataset (NQ, Trivia QA, and HotpotQA) is a\n",
      "relatively general domain whereas the latter two domains are on very domain\n",
      "specific documents.\n",
      "\n",
      "#### Baselines\n",
      "\n",
      "We consider the following baselines for our experiments:\n",
      "\n",
      "  * •\n",
      "\n",
      "LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\n",
      "instruction-finetuned model for QA tasks, where we provide clearly written\n",
      "instructions, but no reference documentation.\n",
      "\n",
      "  * •\n",
      "\n",
      "LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\n",
      "except here we include reference documents. This is a popular technique when\n",
      "dealing with domain specific QA tasks.\n",
      "\n",
      "  * •\n",
      "\n",
      "domain specific Finetuning with 0-shot prompting (DSF): Performing standard\n",
      "supervised finetuning, without documents in context. We find that it mostly\n",
      "useful to align the answering style of the model as well as get familiar with\n",
      "the domain context.\n",
      "\n",
      "  * •\n",
      "\n",
      "domain specific Finetuning with RAG (DSF + RAG): Equip a domain specific\n",
      "finetuned model with external knowledge using RAG. So, for the “knowledge” the\n",
      "model does not know, it can still refer to the context.\n",
      "\n",
      "###  4.2 Results\n",
      "\n",
      "Using the above datasets and baselines, we evaluate our model RAFT and\n",
      "demonstrate the effectiveness of RAFT in Tab.\n",
      "[1](https://arxiv.org/html/2403.10131v1#S3.T1 \"Table 1 ‣ 3 RAFT ‣ RAFT:\n",
      "Adapting Language Model to Domain Specific RAG\"). We see that RAFT\n",
      "consistently and significantly outperforms the baselines. Compared with the\n",
      "base Llama-2 instruction-tuned model, RAFT with RAG does much better in terms\n",
      "of extracting information as well as being robust towards distractors. The\n",
      "gain can be as big as 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation.\n",
      "Compared with DSF on the specific dataset, our model does better at relying on\n",
      "the provided context to solve the problem. RAFT does much better on tasks like\n",
      "HotpotQA and HuggingFace datasets (30.87% on HotpotQA and 31.41% on\n",
      "HuggingFace). Note that for PubMed QA, since it is a binary yes/no question,\n",
      "we don’t observe significant gains when we compare our model with DSF + RAG.\n",
      "Even compared with a much larger and better model GPT-3.5, RAFT demonstrates\n",
      "significant advantages.\n",
      "\n",
      "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly\n",
      "due to its answering style not aligning with the ground truth. By applying\n",
      "domain specific tuning, we significantly enhance its performance. This process\n",
      "enables the model to learn and adopt the appropriate style of answering.\n",
      "However, introducing RAG to a domain-specifically fine-tuned (DSF) model\n",
      "doesn’t invariably lead to better outcomes. This might indicate that the model\n",
      "lacks training in context processing and extracting useful information from\n",
      "it. By incorporating our method, RAFT , we train the model not only to match\n",
      "its answering style with that required but also to improve its document\n",
      "processing capabilities. Consequently, our approach outperforms all others.\n",
      "\n",
      "###  4.3 Effect of CoT\n",
      "\n",
      "We also conduct an analysis to evaluate the effectiveness of the Chain-of-\n",
      "Thought approach in enhancing the model’s performance. As indicated in Table\n",
      "[2](https://arxiv.org/html/2403.10131v1#S4.T2 \"Table 2 ‣ 4.3 Effect of CoT ‣ 4\n",
      "Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG\"), simply\n",
      "providing the answer to a question may not always be adequate. This approach\n",
      "can lead to a rapid decrease in loss, resulting in the training process to\n",
      "diverge. Incorporating a reasoning chain that not only guides the model to the\n",
      "answer but also enriches the model’s understanding can improve the overall\n",
      "accuracy. In our experiments, integrating the Chain-of-Thought significantly\n",
      "enhances training robustness. We employ GPT-4-1106 to generate our Chain-of-\n",
      "Thought prompts and include an example of the prompt we used in Figure\n",
      "[3](https://arxiv.org/html/2403.10131v1#S3.F3 \"Figure 3 ‣ 3 RAFT ‣ RAFT:\n",
      "Adapting Language Model to Domain Specific RAG\").\n",
      "\n",
      "Table 2: Ablation on Chain-of-Thought: The numbers of RAFT and RAFT without\n",
      "CoT. Results on various datasets show that adding CoT can significantly\n",
      "improve the performance of the finetuned model. With a gain of 9.66% and\n",
      "14.93% on the Hotpot QA and HuggingFace datasets respectively. |\n",
      "\n",
      "PubMed\n",
      "\n",
      "|\n",
      "\n",
      "HotpotQA\n",
      "\n",
      "|\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "|\n",
      "\n",
      "Torch Hub\n",
      "\n",
      "|\n",
      "\n",
      "TensorFlow Hub  \n",
      "  \n",
      "---|---|---|---|---|---  \n",
      "  \n",
      "RAFT w.o CoT\n",
      "\n",
      "|\n",
      "\n",
      "68.30\n",
      "\n",
      "|\n",
      "\n",
      "25.62\n",
      "\n",
      "|\n",
      "\n",
      "59.07\n",
      "\n",
      "| 86.56 |\n",
      "\n",
      "83.21  \n",
      "  \n",
      "RAFT\n",
      "\n",
      "| 73.30 | 35.28 | 74.00 |\n",
      "\n",
      "84.95\n",
      "\n",
      "| 86.86  \n",
      "  \n",
      "###  4.4 Qualitative Analysis\n",
      "\n",
      "To illustrate the potential advantages of RAFT over the domain-specifically\n",
      "fine-tuned (DSF) approach, we present a comparative example in Figure\n",
      "[4](https://arxiv.org/html/2403.10131v1#S4.F4 \"Figure 4 ‣ 4.4 Qualitative\n",
      "Analysis ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific\n",
      "RAG\"). This example qualitatively demonstrates a scenario where the DSF model\n",
      "becomes confused by a question asking for the identity of a screenwriter.\n",
      "Instead of providing the correct name, it mistakenly cites one of the films\n",
      "written by the screenwriter. In contrast, the RAFT model accurately answers\n",
      "the question. This discrepancy suggests that training a model solely with\n",
      "question-answer pairs may impair its ability to derive relevant context from\n",
      "provided documents. The comparison underscores the importance of incorporating\n",
      "both standard instructional tuning and context comprehension into the training\n",
      "dataset to preserve and enhance the model’s ability to process text\n",
      "effectively.\n",
      "\n",
      "  \n",
      "\n",
      "HotPot QA\n",
      "\n",
      "Question: What screenwriter with credits for “Evolution”[0/1879]e a film\n",
      "starring Nicolas Cage and Téa Leoni? Documents: ……\\ldots… David Weissman is a\n",
      "screenwriter and director. His film credits include “The Family Man” (2000),\n",
      "“Evolution” (2001), and “When in Rome” (2010). ……\\ldots… The Family Man is a\n",
      "2000 American romantic comedy-drama film directed by Brett Ratner, written by\n",
      "David Diamond and David Weissman, and starring Nicolas Cage and Téa Leoni. …\n",
      "——————————————————————————————————————————————– RAFT: ##Reason: The\n",
      "screenwriter with credits for the film “Evolution,” starring Nicolas Cage and\n",
      "Téa Leoni, is David Weissman. This information is provided in the reference\n",
      "documents which mention David Weissman as a screenwriter with film credits\n",
      "including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”\n",
      "(2010). Therefore, the screenwriter for “Evolution” is David Weissman.\n",
      "##Answer: David Weissman ✓ DSF: The Family Man ✗  \n",
      "Figure 4: Comparison of RAFT and DSF: We prompt RAFT and DSF fine-tuned models\n",
      "on the HotpotQA dataset. We can see that the DSF model extracts the wrong\n",
      "information from the context. For the question, who is the screenwriter, it\n",
      "responds with a film name. RAFT manages to get the result correctly .\n",
      "\n",
      "###  4.5 Should we train the LLM always with the oracle context for RAG?\n",
      "\n",
      "In our exploration of whether large language models (LLMs) should always be\n",
      "trained with the oracle context for Retrieval-Augmented Generation (RAG), we\n",
      "address a key question: what proportion (p%) of the training data should\n",
      "include oracle documents? Intuitively, one might assume that for effective\n",
      "training in reading and extracting information from context (e.g., RAG tasks),\n",
      "the oracle document should always be included during training (P = 100%).\n",
      "However, our findings challenge this assumption: incorporating a portion of\n",
      "the training data without the oracle document in the context (P = 80%) appears\n",
      "to enhance the model’s performance on RAG tasks.\n",
      "\n",
      "Fig. [5](https://arxiv.org/html/2403.10131v1#S4.F5 \"Figure 5 ‣ 4.5 Should we\n",
      "train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT:\n",
      "Adapting Language Model to Domain Specific RAG\") presents our investigation\n",
      "into the hyperparameter P%, which represents the percentage of training\n",
      "instances that should include oracle documents. Our analysis reveals that the\n",
      "optimal proportion varies across datasets, with figures ranging from 40%, 60%,\n",
      "and 100%. This indicates that training your LLM without the correct\n",
      "corresponding context at times can be beneficial for the downstream task of\n",
      "answering questions related to the documents. In our training setup, we\n",
      "include four distractor documents alongside the oracle document, and at test\n",
      "time, we maintain this format by providing the oracle document with four\n",
      "distractors. Our findings suggest that, for domain specific RAG tasks,\n",
      "including a certain percentage of training data without the oracle documents\n",
      "in the context proves to be advantageous.\n",
      "\n",
      "![Refer to caption](x2.png)\n",
      "\n",
      "![Refer to caption](x3.png)\n",
      "\n",
      "![Refer to caption](x4.png)\n",
      "\n",
      "Figure 5: How many golden documents to involve? We study the hyperparameter\n",
      "P%percent𝑃P\\%italic_P % which indicates what fraction of the training data\n",
      "contains the oracle document(s) in its context. Results on NQ, TQA and\n",
      "HotpotQA suggest that mixing a fraction of data that does not have the oracle\n",
      "document in its context is helpful for in-domain RAG.\n",
      "\n",
      "##  5 RAFT Generalizes to Top-K RAG\n",
      "\n",
      "After demonstrating the performance of RAFT on various benchmarks, we now\n",
      "study another important problem: How does the number of distractor documents\n",
      "in RAFT affect the model’s performance when augmented with top-k retriever\n",
      "augmented generation (RAG) result during the evaluation? Previous research has\n",
      "highlighted the vulnerability of LLMs to irrelevant text (see studies (Shi et\n",
      "al., [2023a](https://arxiv.org/html/2403.10131v1#bib.bib43); Weston &\n",
      "Sukhbaatar, [2023](https://arxiv.org/html/2403.10131v1#bib.bib53); Liu et al.,\n",
      "[2023b](https://arxiv.org/html/2403.10131v1#bib.bib29))). This issue is\n",
      "particularly critical for LLMs + RAG since top-k RAG is frequently employed at\n",
      "test time to ensure high recall. Such a scenario necessitates the model to\n",
      "have the ability to discern and disregard irrelevant content, focusing solely\n",
      "on pertinent information.\n",
      "\n",
      "###  5.1 Making Model Robust to top-K RAG\n",
      "\n",
      "To tackle the challenge of enhancing large language models’ (LLMs) ability to\n",
      "sift through irrelevant text within the retrieval pipeline, our analysis\n",
      "revealed that training solely with oracle (highly relevant) documents can\n",
      "inadvertently diminish the model’s ability to discern and disregard irrelevant\n",
      "information. To address this, our algorithm, RAFT , adopts a strategy that\n",
      "integrates oracle documents with a mix of irrelevant ones. This methodology\n",
      "prompts us to investigate the ideal fraction of negative (irrelevant)\n",
      "documents to incorporate throughout the training process and to assess how\n",
      "well this training approach adapts to different volumes of documents\n",
      "encountered by the Retrieval-Augmented Generation (RAG) during the test phase.\n",
      "Our aim is to refine the balance between relevant and irrelevant information\n",
      "to strengthen the model’s efficiency in identifying and utilizing pertinent\n",
      "content. Notice that Sec [4.5](https://arxiv.org/html/2403.10131v1#S4.SS5 \"4.5\n",
      "Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation\n",
      "‣ RAFT: Adapting Language Model to Domain Specific RAG\") looked at what P% of\n",
      "training data should include distractors, while in this section, we study\n",
      "test-time scenarios.\n",
      "\n",
      "Training with Negative Documents To enhance the robustness of large language\n",
      "models (LLMs) against irrelevant text in retrieved documents, we adopted a\n",
      "finetuning approach that incorporates both golden (highly relevant) documents\n",
      "and distractor (irrelevant) documents. The model was trained with varying\n",
      "numbers of distractor documents, but consistently evaluated using the top-k\n",
      "documents obtained from the retriever - not to be confused with p𝑝pitalic_p.\n",
      "\n",
      "Our findings, detailed in Fig. [6](https://arxiv.org/html/2403.10131v1#S5.F6\n",
      "\"Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K\n",
      "RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG\"), reveal that\n",
      "finetuning with only the oracle document frequently results in inferior\n",
      "performance compared to configurations that include a greater number of\n",
      "distractor documents. As we can see in the figure, the better performance for\n",
      "Natural Questions is training with D*+3⁢Dsuperscript𝐷3𝐷D^{*}+3Ditalic_D\n",
      "start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + 3 italic_D and it is\n",
      "D*+1⁢Dsuperscript𝐷1𝐷D^{*}+1Ditalic_D start_POSTSUPERSCRIPT *\n",
      "end_POSTSUPERSCRIPT + 1 italic_D documents with Hotpot QA. This insight has\n",
      "been particularly beneficial for our algorithm, RAFT . In our experiments, we\n",
      "typically employ a training setup consisting of one oracle document alongside\n",
      "four distractor documents. This approach strikes a balance, ensuring the model\n",
      "is not overwhelmed by distractors while still gaining the ability to\n",
      "effectively discern and prioritize relevant information.\n",
      "\n",
      "Generalization to a variable number of test-time documents. We extended our\n",
      "research to examine the impact of different quantities of test-time documents\n",
      "on the model’s performance. Specifically, our experiments focused on assessing\n",
      "how models, trained with varying numbers of distractor documents, respond to\n",
      "changes in the number of documents presented at test time.\n",
      "\n",
      "The results, illustrated in Fig. [6](https://arxiv.org/html/2403.10131v1#S5.F6\n",
      "\"Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K\n",
      "RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG\"), confirm that the\n",
      "inclusion of distractor documents during training indeed makes the model more\n",
      "resilient to fluctuations in the number of documents encountered during\n",
      "testing. This ability to maintain consistent performance despite variations in\n",
      "test-time document numbers further validates the robustness of our approach,\n",
      "RAFT . This finding underscores the importance of a well-calibrated training\n",
      "environment to prepare the model for a range of scenarios it may encounter in\n",
      "real-world applications.\n",
      "\n",
      "![Refer to caption](x5.png)\n",
      "\n",
      "![Refer to caption](x6.png)\n",
      "\n",
      "Figure 6: Test-Time Documents Varying: We study how robust RAFT is to varying\n",
      "numbers of test-time documents that a retriever might provide. In NQ, we find\n",
      "that training with 4 documents leads to the best performance, but training\n",
      "with 2 documents is optimal for HotpotQA. However, across both datasets,\n",
      "training with all datasets consisting of _oracle_ documents hurts performance.\n",
      "\n",
      "##  6 Related Works\n",
      "\n",
      "Retrieval-Augmented Language Models RAG enhances language models by\n",
      "integrating a retrieval module that sources relevant information from external\n",
      "knowledge bases, significantly improving performance across various NLP tasks,\n",
      "including language modeling (Guu et al.,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib12); Borgeaud et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib4); Khandelwal et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib20); Shi et al.,\n",
      "[2023d](https://arxiv.org/html/2403.10131v1#bib.bib46); Lin et al.,\n",
      "[2023b](https://arxiv.org/html/2403.10131v1#bib.bib27); Shi et al.,\n",
      "[2023c](https://arxiv.org/html/2403.10131v1#bib.bib45); Asai et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib3); Xu et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib56); Wang et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib50)) and open-domain\n",
      "question answering (Izacard et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib15); Lewis et al.,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib24)). This integration\n",
      "follows a “retrieve-and-read\" paradigm where the retrieval module provides\n",
      "additional context from external sources, which the LM then uses to generate\n",
      "the final output. The retrieval process involves using the input as a query to\n",
      "fetch documents, which the LM incorporates for final predictions. For\n",
      "instance, Atlas (Izacard et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib15)) fine-tunes T5 models\n",
      "with the retriever, treating documents as latent variables, while RETRO\n",
      "(Borgeaud et al., [2022](https://arxiv.org/html/2403.10131v1#bib.bib4))\n",
      "modifies the decoder-only architecture to include retrieved texts and conducts\n",
      "pre-training from scratch. kNN-LM (Khandelwal et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib20)) interpolates between\n",
      "the LM’s next token distribution and distributions computed from retrieved\n",
      "tokens at inference. (Shi et al.,\n",
      "[2023d](https://arxiv.org/html/2403.10131v1#bib.bib46); Ram et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib41)) assume black-box access\n",
      "to an LM and combine it with either off-the-shelf or fine-tuned retriever.\n",
      "\n",
      "Memorization A key question around large neural language models is whether\n",
      "they truly “understand” text (Feldman,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib11); Power et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib39)) or simply rely on\n",
      "surface pattern memorization (Carlini et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib6); Tänzer et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib47)). (Feldman,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib11); Carlini et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib6),\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib8)) develop methodologies to\n",
      "quantify the extent of memorization in neural models. (Brown et al.,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib5); Power et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib39); Liu et al.,\n",
      "[2022b](https://arxiv.org/html/2403.10131v1#bib.bib31)) further explored how\n",
      "memorization impacts the models’ generalization capabilities. Recently, a\n",
      "seminal work by (Carlini et al.,\n",
      "[2021](https://arxiv.org/html/2403.10131v1#bib.bib7); Shi et al.,\n",
      "[2023b](https://arxiv.org/html/2403.10131v1#bib.bib44)) demonstrated the\n",
      "ability of language models to memorize and regurgitate training data, raising\n",
      "significant privacy concerns (Kandpal et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib19); Pan et al.,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib37)).\n",
      "\n",
      "Finetuning of LLMs Recent years have seen rapid progress in developing large-\n",
      "scale language models (LLMs) (Brown et al.,\n",
      "[2020](https://arxiv.org/html/2403.10131v1#bib.bib5); OpenAI,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib35); Workshop et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib54); Touvron et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib48),\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib48); Anil et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib1)). To adapt these\n",
      "foundation models to downstream tasks, fine-tuning (Mishra et al.,\n",
      "[2021](https://arxiv.org/html/2403.10131v1#bib.bib33); Sanh et al.,\n",
      "[2021](https://arxiv.org/html/2403.10131v1#bib.bib42); Chung et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib9); Muennighoff et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib34); Zhou et al.,\n",
      "[2023b](https://arxiv.org/html/2403.10131v1#bib.bib60); Lin et al.,\n",
      "[2023b](https://arxiv.org/html/2403.10131v1#bib.bib27); Ji et al.,\n",
      "[2024](https://arxiv.org/html/2403.10131v1#bib.bib16)) has become a prevalent\n",
      "approach. Traditional supervised fine-tuning may be limited by the cost and\n",
      "compute required for adapating LLMs. Addressing these challenges, research in\n",
      "the realm of parameter-efficient fine-tuning (Houlsby et al.,\n",
      "[2019](https://arxiv.org/html/2403.10131v1#bib.bib13)), such as Prompt Tuning\n",
      "(Lester et al., [2021](https://arxiv.org/html/2403.10131v1#bib.bib23)),\n",
      "Prefix-Tuning (Li & Liang,\n",
      "[2021](https://arxiv.org/html/2403.10131v1#bib.bib25)), P-Tuning (Liu et al.,\n",
      "[2022a](https://arxiv.org/html/2403.10131v1#bib.bib30)) and Low-Rank based\n",
      "fine-tuning (Hu et al.,\n",
      "[2021](https://arxiv.org/html/2403.10131v1#bib.bib14)), has gained traction.\n",
      "These methods enable LLMs to acquire domain-specific knowledge and adapt to\n",
      "specialized tasks such as question answering, summarization, and dialogue\n",
      "generation. Another branch of finetuning is through RLHF (Ouyang et al.,\n",
      "[2022](https://arxiv.org/html/2403.10131v1#bib.bib36); Rafailov et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib40); Liu et al.,\n",
      "[2023a](https://arxiv.org/html/2403.10131v1#bib.bib28); Zhang et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib58)), which adopts RL to\n",
      "align LLM’s preference with human.\n",
      "\n",
      "Finetuning for RAG More recently, several papers have been exploring the idea\n",
      "of finetuning a pretrained LLM to be better at RAG tasks (Lin et al.,\n",
      "[2023a](https://arxiv.org/html/2403.10131v1#bib.bib26); Wang et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib50); Xu et al.,\n",
      "[2023](https://arxiv.org/html/2403.10131v1#bib.bib56); Liu et al.,\n",
      "[2024](https://arxiv.org/html/2403.10131v1#bib.bib32)). These works focus on\n",
      "constructing a combination of finetuning dataset for RAG and train a model to\n",
      "perform well on these tasks. In particular, in their settings, at test time,\n",
      "the domain or documents can be different than the training time; whereas our\n",
      "paper studies a slightly opposite scenario where we only care about testing\n",
      "the LLM on the same set of documents.\n",
      "\n",
      "##  7 Conclusion\n",
      "\n",
      "RAFT is a training strategy designed to enhance the model’s performance in\n",
      "answering questions within a specific domain, in \"open-book\" settings. This\n",
      "technique demonstrates a fine-tuning recipe for LLMs for question-answering\n",
      "tasks based on a selected collection of documents. We have pinpointed several\n",
      "crucial design decisions, such as training the model alongside distractor\n",
      "documents, organizing the dataset so a portion lacks oracle documents in their\n",
      "context, and formulating answers in a chain-of-thought manner with direct\n",
      "quotations from the relevant text. Our evaluations on PubMed, HotpotQA, and\n",
      "Gorilla API Bench underline RAFT’s significant potential. Looking forward, we\n",
      "anticipate that in-domain Retrieval-Augmented Generation (RAG) will continue\n",
      "to gain interest within both industrial and academic spheres. Unlike general-\n",
      "RAG, our work addresses practical scenarios where LLMs are tasked with\n",
      "answering questions using domain-specific knowledge. Aligning with current\n",
      "trends, our findings suggest that smaller, fine-tuned models are capable of\n",
      "performing comparably well in domain-specific question-answering tasks, in\n",
      "contrast to their generic LLM counterparts.\n",
      "\n",
      "## References\n",
      "\n",
      "  * Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al.  Palm 2 technical report.  _arXiv preprint arXiv:2305.10403_ , 2023. \n",
      "  * Anthropic (2023) Anthropic.  Prompt engineering for claude’s long context window.  2023\\. \n",
      "  * Asai et al. (2023) Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.  Self-rag: Learning to retrieve, generate, and critique through self-reflection.  _arXiv preprint arXiv:2310.11511_ , 2023. \n",
      "  * Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al.  Improving language models by retrieving from trillions of tokens.  In _International conference on machine learning_ , pp. 2206–2240. PMLR, 2022. \n",
      "  * Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.  Language models are few-shot learners.  _Advances in neural information processing systems_ , 33:1877–1901, 2020. \n",
      "  * Carlini et al. (2019) Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D.  The secret sharer: Evaluating and testing unintended memorization in neural networks.  In _28th USENIX Security Symposium (USENIX Security 19)_ , pp. 267–284, 2019. \n",
      "  * Carlini et al. (2021) Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al.  Extracting training data from large language models.  In _30th USENIX Security Symposium (USENIX Security 21)_ , pp. 2633–2650, 2021. \n",
      "  * Carlini et al. (2022) Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C.  Quantifying memorization across neural language models.  In _The Eleventh International Conference on Learning Representations_ , 2022. \n",
      "  * Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.  Scaling instruction-finetuned language models.  _arXiv preprint arXiv:2210.11416_ , 2022. \n",
      "  * Dernoncourt & Lee (2017) Dernoncourt, F. and Lee, J. Y.  Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts.  _arXiv preprint arXiv:1710.06071_ , 2017. \n",
      "  * Feldman (2020) Feldman, V.  Does learning require memorization? a short tale about a long tail.  In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_ , pp. 954–959, 2020. \n",
      "  * Guu et al. (2020) Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.  Retrieval augmented language model pre-training.  In _International conference on machine learning_ , pp. 3929–3938. PMLR, 2020. \n",
      "  * Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.  Parameter-efficient transfer learning for nlp.  In _International Conference on Machine Learning_ , pp. 2790–2799. PMLR, 2019. \n",
      "  * Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.  Lora: Low-rank adaptation of large language models.  _arXiv preprint arXiv:2106.09685_ , 2021. \n",
      "  * Izacard et al. (2023) Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.  Atlas: Few-shot learning with retrieval augmented language models.  _Journal of Machine Learning Research_ , 24(251):1–43, 2023.  URL <http://jmlr.org/papers/v24/23-0037.html>. \n",
      "  * Ji et al. (2024) Ji, C. C.-J., Mao, H., Yan, F., Shishir G. Patil, T. Z., Stoica, I., and Gonzalez, J. E.  Gorilla openfunctions v2.  2024\\. \n",
      "  * Jin et al. (2019) Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X.  Pubmedqa: A dataset for biomedical research question answering.  _arXiv preprint arXiv:1909.06146_ , 2019. \n",
      "  * Joshi et al. (2017) Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.  Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.  _arXiv preprint arXiv:1705.03551_ , 2017. \n",
      "  * Kandpal et al. (2022) Kandpal, N., Wallace, E., and Raffel, C.  Deduplicating training data mitigates privacy risks in language models.  In _International Conference on Machine Learning_ , pp. 10697–10707. PMLR, 2022. \n",
      "  * Khandelwal et al. (2019) Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.  Generalization through memorization: Nearest neighbor language models.  _arXiv preprint arXiv:1911.00172_ , 2019. \n",
      "  * Kwiatkowski et al. (2019) Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al.  Natural questions: a benchmark for question answering research.  _Transactions of the Association for Computational Linguistics_ , 7:453–466, 2019. \n",
      "  * Lazaridou et al. (2022) Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N.  Internet-augmented language models through few-shot prompting for open-domain question answering.  _arXiv preprint arXiv:2203.05115_ , 2022. \n",
      "  * Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N.  The power of scale for parameter-efficient prompt tuning.  _arXiv preprint arXiv:2104.08691_ , 2021. \n",
      "  * Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.  Retrieval-augmented generation for knowledge-intensive nlp tasks.  _Advances in Neural Information Processing Systems_ , 33:9459–9474, 2020. \n",
      "  * Li & Liang (2021) Li, X. L. and Liang, P.  Prefix-tuning: Optimizing continuous prompts for generation.  _arXiv preprint arXiv:2101.00190_ , 2021. \n",
      "  * Lin et al. (2023a) Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al.  Ra-dit: Retrieval-augmented dual instruction tuning.  _arXiv preprint arXiv:2310.01352_ , 2023a. \n",
      "  * Lin et al. (2023b) Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al.  Ra-dit: Retrieval-augmented dual instruction tuning.  _arXiv preprint arXiv:2310.01352_ , 2023b. \n",
      "  * Liu et al. (2023a) Liu, H., Sferrazza, C., and Abbeel, P.  Chain of hindsight aligns language models with feedback.  _arXiv preprint arXiv:2302.02676_ , 3, 2023a. \n",
      "  * Liu et al. (2023b) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.  Lost in the middle: How language models use long contexts.  _arXiv preprint arXiv:2307.03172_ , 2023b. \n",
      "  * Liu et al. (2022a) Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.  P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.  In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_ , pp. 61–68, 2022a. \n",
      "  * Liu et al. (2022b) Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M.  Towards understanding grokking: An effective theory of representation learning.  _Advances in Neural Information Processing Systems_ , 35:34651–34663, 2022b. \n",
      "  * Liu et al. (2024) Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B.  Chatqa: Building gpt-4 level conversational qa models.  _arXiv preprint arXiv:2401.10225_ , 2024. \n",
      "  * Mishra et al. (2021) Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.  Cross-task generalization via natural language crowdsourcing instructions.  _arXiv preprint arXiv:2104.08773_ , 2021. \n",
      "  * Muennighoff et al. (2023) Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Le Scao, T., Bari, M. S., Shen, S., Yong, Z. X., Schoelkopf, H., Tang, X., Radev, D., Aji, A. F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., and Raffel, C.  Crosslingual generalization through multitask finetuning.  In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pp. 15991–16111, Toronto, Canada, July 2023. Association for Computational Linguistics.  doi: [10.18653/v1/2023.acl-long.891](10.18653/v1/2023.acl-long.891).  URL <https://aclanthology.org/2023.acl-long.891>. \n",
      "  * OpenAI (2023) OpenAI.  Gpt-4 technical report, 2023. \n",
      "  * Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.  Training language models to follow instructions with human feedback.  _Advances in Neural Information Processing Systems_ , 35:27730–27744, 2022. \n",
      "  * Pan et al. (2020) Pan, X., Zhang, M., Ji, S., and Yang, M.  Privacy risks of general-purpose language models.  In _2020 IEEE Symposium on Security and Privacy (SP)_ , pp. 1314–1331. IEEE, 2020. \n",
      "  * Patil et al. (2023) Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E.  Gorilla: Large language model connected with massive apis.  _arXiv preprint arXiv:2305.15334_ , 2023. \n",
      "  * Power et al. (2022) Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V.  Grokking: Generalization beyond overfitting on small algorithmic datasets.  _arXiv preprint arXiv:2201.02177_ , 2022. \n",
      "  * Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C.  Direct preference optimization: Your language model is secretly a reward model.  _arXiv preprint arXiv:2305.18290_ , 2023. \n",
      "  * Ram et al. (2023) Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.  In-context retrieval-augmented language models.  _arXiv preprint arXiv:2302.00083_ , 2023. \n",
      "  * Sanh et al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al.  Multitask prompted training enables zero-shot task generalization.  _arXiv preprint arXiv:2110.08207_ , 2021. \n",
      "  * Shi et al. (2023a) Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D.  Large language models can be easily distracted by irrelevant context.  In _International Conference on Machine Learning_ , pp. 31210–31227. PMLR, 2023a. \n",
      "  * Shi et al. (2023b) Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.  Detecting pretraining data from large language models.  _arXiv preprint arXiv:2310.16789_ , 2023b. \n",
      "  * Shi et al. (2023c) Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., Smith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M.  In-context pretraining: Language modeling beyond document boundaries.  _arXiv preprint arXiv:2310.10638_ , 2023c. \n",
      "  * Shi et al. (2023d) Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.  Replug: Retrieval-augmented black-box language models.  _arXiv preprint arXiv:2301.12652_ , 2023d. \n",
      "  * Tänzer et al. (2022) Tänzer, M., Ruder, S., and Rei, M.  Memorisation versus generalisation in pre-trained language models.  In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pp. 7564–7578, 2022. \n",
      "  * Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.  Llama 2: Open foundation and fine-tuned chat models.  _arXiv preprint arXiv:2307.09288_ , 2023. \n",
      "  * Vu et al. (2023) Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., et al.  Freshllms: Refreshing large language models with search engine augmentation.  _arXiv preprint arXiv:2310.03214_ , 2023. \n",
      "  * Wang et al. (2023) Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.  Instructretro: Instruction tuning post retrieval-augmented pretraining.  _arXiv preprint arXiv:2310.07713_ , 2023. \n",
      "  * Wang et al. (2022) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.  Self-instruct: Aligning language models with self-generated instructions.  _arXiv preprint arXiv:2212.10560_ , 2022. \n",
      "  * Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.  Chain-of-thought prompting elicits reasoning in large language models.  _Advances in Neural Information Processing Systems_ , 35:24824–24837, 2022. \n",
      "  * Weston & Sukhbaatar (2023) Weston, J. and Sukhbaatar, S.  System 2 attention (is something you might need too).  _arXiv preprint arXiv:2311.11829_ , 2023. \n",
      "  * Workshop et al. (2022) Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., et al.  Bloom: A 176b-parameter open-access multilingual language model.  _arXiv preprint arXiv:2211.05100_ , 2022. \n",
      "  * Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al.  Effective long-context scaling of foundation models.  _arXiv preprint arXiv:2309.16039_ , 2023. \n",
      "  * Xu et al. (2023) Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.  Retrieval meets long context large language models.  _arXiv preprint arXiv:2310.03025_ , 2023. \n",
      "  * Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D.  Hotpotqa: A dataset for diverse, explainable multi-hop question answering.  _arXiv preprint arXiv:1809.09600_ , 2018. \n",
      "  * Zhang et al. (2023) Zhang, T., Liu, F., Wong, J., Abbeel, P., and Gonzalez, J. E.  The wisdom of hindsight makes language models better instruction followers.  _arXiv preprint arXiv:2302.05206_ , 2023. \n",
      "  * Zhou et al. (2023a) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al.  Lima: Less is more for alignment.  _arXiv preprint arXiv:2305.11206_ , 2023a. \n",
      "  * Zhou et al. (2023b) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al.  Lima: Less is more for alignment.  _arXiv preprint arXiv:2305.11206_ , 2023b. \n",
      "\n",
      "Generated on Fri Mar 15 09:29:36 2024 by\n",
      "[LATExml![\\[LOGO\\]](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==)](http://dlmf.nist.gov/LaTeXML/)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(content_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10,\n",
       " 'name': 'Reason: {reason} ##Answer: {answer}.',\n",
       " 'level': 2,\n",
       " 'text': '\\n\\\\par————————————————————————————–\\n\\n\\\\par|\\\\textbf{CoT Answer:}| ##Reason: The document ##begin_quote## The Oberoi\\nfamily is an Indian family that is famous for its involvement in hotels,\\nnamely through The Oberoi Group. ##end_quote## establishes that the Oberoi\\nfamily is involved in the Oberoi group, and the document ##begin_quote## The\\nOberoi Group is a hotel company with its head office in Delhi. ##end_quote##\\nestablishes the head office of The Oberoi Group. Therefore, the Oberoi family\\nis part of a hotel company whose head office is in Delhi. ##Answer: Delhi\\n\\n\\\\par\\n\\n  \\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and\\nanswers, contrasting them with the correct reasoning and answers. The LLM is\\nprompted to identify errors in its reasoning and extract key insights for\\nimprovement. This figure specifically represents the ‘GenerateExplanation‘\\nstep in the RAFT algorithm ([Section 3](https://arxiv.org/html/2403.10131v1#S3\\n\"3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG\")). Table 1:\\nRAFT improves RAG performance forall specialized domains: Across PubMed,\\nHotpotQA, HuggingFace, Torch Hub, and Tensorflow Hub, we see that domain\\nspecific Finetuning improves significantly of the performance of the base\\nmodel, but RAFT consistently outperforms the existing domain specific\\nfinetuning method with or without RAG. This suggests the need to train the\\nmodel with context. We compare our model with LLaMA finetuning receipes, and\\nprovide GPT-3.5 for reference. |\\n\\nPubMed\\n\\n|\\n\\nHotpotQA\\n\\n|\\n\\nHuggingFace\\n\\n|\\n\\nTorch Hub\\n\\n|\\n\\nTensorFlow Hub  \\n  \\n---|---|---|---|---|---  \\n  \\nGPT-3.5 + RAG\\n\\n|\\n\\n71.60\\n\\n29.08\\n\\n|\\n\\n60.21\\n\\n|\\n\\n65.59  \\n  \\nLLaMA2-7B\\n\\n|\\n\\n56.5\\n\\n|\\n\\n0.54\\n\\n|\\n\\n0.22\\n\\n|\\n\\n0\\n\\n|\\n\\n0  \\n  \\nLLaMA2-7B + RAG\\n\\n|\\n\\n58.8\\n\\n|\\n\\n0.03\\n\\n|\\n\\n26.43\\n\\n|\\n\\n08.60\\n\\n|\\n\\n43.06  \\n  \\nDSF\\n\\n|\\n\\n59.7\\n\\n|\\n\\n6.38\\n\\n|\\n\\n61.06\\n\\n|\\n\\n84.94\\n\\n|\\n\\n86.56  \\n  \\nDSF + RAG\\n\\n|\\n\\n71.6\\n\\n|\\n\\n4.41\\n\\n|\\n\\n42.59\\n\\n|\\n\\n82.80\\n\\n|\\n\\n60.29  \\n  \\nRAFT (LLaMA2-7B)\\n\\n35.28\\n\\n',\n",
       " 'contents': [Content 3: table (| 41.5 |),\n",
       "  Content 4: table (| 73.30 |),\n",
       "  Content 5: table (| 74.00 | 84.95 | 86.86  )],\n",
       " 'previous_section': Section 9: begin_quote## and ##end_quote## as well as the detailed explanation on how (Level 2),\n",
       " 'next_section': Section 11: 4 Evaluation (Level 2),\n",
       " 'parent_section': None,\n",
       " 'child_sections': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_parser.sections[10].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions in the MMLU benchmarks require deeper reasoning. Furthermore, they\n",
      "also require understanding and application of formulae which are often physics\n",
      "and chemistry principles and concepts. In this case, we first teach the model\n",
      "to do abstraction in the form of concepts and first principles such as\n",
      "Newton’s first law of motion, Doppler effect, and Gibbs free energy etc. The\n",
      "implicit step-back question here is “what are the physics or chemistry\n",
      "principles and concepts involved in solving this task?”. We provide\n",
      "demonstrations to teach the model to recite from its own knowledge relevant\n",
      "principles for solving the task (see Appendix D.1 for few-shot exemplars).\n",
      "\n",
      "\n",
      "Figure 3: Ablation study of Step-Back Prompting accuracy on MMLU high-school\n",
      "Physics against number of few shot exemplars: robust performance with respect\n",
      "to varying number of shots.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(markdown_parser.sections[10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image\n",
      "1 Introduction\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x1.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "1 Introduction\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x2.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "4.1 Step-Back Prompting\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x3.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "4.3 Ablation and Analysis\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x4.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "4.3 Ablation and Analysis\n",
      "|  |  |  |  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "5.2 Results\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x5.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "5.3 Ablation and Analysis\n",
      "| Method | MuSiQue | StrategyQA  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "5.3 Ablation and Analysis\n",
      "| PaLM-2L | 35.5% (3%) | 82.8% (0.7%)  \n",
      "| PaLM-2L 1-shot | 29.0% (0.5%) | 76.6% (0.5%)  \n",
      "| PaLM-2L + CoT | 38.7% (3.2%) | 83.6% (0.4%)  \n",
      "| PaLM-2L + CoT 1-shot | 38.5% (2.2%) | 76.8% (1.4%)  \n",
      "| PaLM-2L + TDB | 39.0% (2.3%) | 82.7% (0.9%)  \n",
      "| PaLM-2L + RAG | 39.6% (2.8%) | 84.2% (0.5%)  \n",
      "| PaLM-2L + Step-Back (ours) | 42.6% (3.1%) | 82.7% (0.4%)  \n",
      "| PaLM-2L + Step-Back + RAG (ours) |  42.8% (2.0%) |  86.4% (1%)  \n",
      "| GPT-4 | 38.5% (0.2%) | 78.3% (1.1%)  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "a.1 TimeQA Error Analysis\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x6.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "a.2 StrategyQA Error Analysis\n",
      "{'description': '', 'src': 'https://media.arxiv-vanity.com/render-output/8350071/x7.png'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Appendix B Dataset Details\n",
      "| MMLU high-school Chemistry | Test | 203  \n",
      "| TimeQA Easy | Test | 2613  \n",
      "| TimeQA Hard | Test | 2613  \n",
      "| SituatedQA | Test | 2901  \n",
      "| StrategyQA | Dev | 229  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "d.4 Baseline Prompts\n",
      "| Question | Answer  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "e.2 Example Wins from Step-Back Prompting\n",
      "{'description': '\\\\[LOGO\\\\]', 'src': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=='}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for content in markdown_parser.contents:\n",
    "    print(content.type)\n",
    "    print(content.section_name)\n",
    "    print(content.content)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_tables(table_html):\n",
    "    try:\n",
    "        # Attempt to parse the table using BeautifulSoup\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Extracting the rows from the table\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        # Parsing each cell in the row\n",
    "        table_content = []\n",
    "        for row in rows:\n",
    "            row_content = []\n",
    "            for cell in row.find_all(['td', 'th']):\n",
    "                cell_content = cell.get_text(strip=True)\n",
    "                row_content.append(cell_content)\n",
    "            table_content.append(row_content)\n",
    "        \n",
    "        return {\n",
    "            'type': 'table',\n",
    "            'caption': soup.find('figcaption').get_text(strip=True) if soup.find('figcaption') else '',\n",
    "            'content': table_content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # If there's an error, print it and return a message.\n",
    "        print(f\"Error parsing table: {e}\")\n",
    "        \n",
    "        # Fallback: Try to extract whatever text is possible.\n",
    "        return {\n",
    "            'type': 'table',\n",
    "            'caption': 'An error occurred while parsing the table.',\n",
    "            'content': table_html.get_text(strip=True) if table_html else 'No content could be retrieved.'\n",
    "        }\n",
    "\n",
    "\n",
    "def parse_images(figure_html):\n",
    "    try:\n",
    "        # Attempt to parse the figure using BeautifulSoup\n",
    "        soup = BeautifulSoup(figure_html, 'html.parser')\n",
    "        figures = soup.find_all('figure', class_='ltx_figure')\n",
    "        \n",
    "        image_content_list = []\n",
    "\n",
    "        for figure in figures:\n",
    "            image = figure.find('img')\n",
    "            caption = figure.find('figcaption')\n",
    "\n",
    "            if image and caption:\n",
    "                content = {\n",
    "                    'type': 'image',\n",
    "                    'caption': caption.get_text(strip=True),\n",
    "                    'content': image['src'] if image.has_attr('src') else None\n",
    "                }\n",
    "            else:\n",
    "                # Handling case where there might be an image without a caption or vice versa\n",
    "                content = {\n",
    "                    'type': 'image',\n",
    "                    'caption': caption.get_text(strip=True) if caption else 'No caption available.',\n",
    "                    'content': image['src'] if image and image.has_attr('src') else 'No image source available.'\n",
    "                }\n",
    "            \n",
    "            image_content_list.append(content)\n",
    "\n",
    "        return image_content_list\n",
    "\n",
    "    except Exception as e:\n",
    "        # If there's an error, print it and return a message.\n",
    "        print(f\"Error parsing image: {e}\")\n",
    "        \n",
    "        # Fallback: Return an error message within the content dictionary\n",
    "        return {\n",
    "            'type': 'image',\n",
    "            'caption': 'An error occurred while parsing the image.',\n",
    "            'content': 'No content could be retrieved.'\n",
    "        }\n",
    "\n",
    "\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Initialize a list to hold all content dictionaries\n",
    "    contents = []\n",
    "    \n",
    "    # Parse all table figures\n",
    "    table_figures = soup.find_all('figure', class_='ltx_table')\n",
    "    for table_figure in table_figures:\n",
    "        table_content = parse_tables(str(table_figure))\n",
    "        if table_content:\n",
    "            contents.append(table_content)\n",
    "    \n",
    "    # Parse all image figures\n",
    "    image_figures = soup.find_all('figure', class_='ltx_figure')\n",
    "    for image_figure in image_figures:\n",
    "        image_content = parse_images(str(image_figure))\n",
    "        if image_content:\n",
    "            contents.extend(image_content)  # Extend since parse_images returns a list\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394327"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_items = parse_html(content)\n",
    "len(content_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table\n",
      "Table 1:Strong performance ofStep-Back Promptingon STEM tasks achieving state-of-the-art surpassing GPT-4. CoT: zero-shot Chain of Thought prompting(Kojimaet al.,2022), TDB: Take a Deep Breathe prompting(Yanget al.,2023). The Table reports the average accuracy over 5 evaluation runs, with standard deviations in the parentheses.\n",
      "                            0             1               2\n",
      "0                      Method  MMLU Physics  MMLU Chemistry\n",
      "1                     PaLM-2L  66.4% (0.8%)    70.9% (0.9%)\n",
      "2              PaLM-2L 1-shot    64% (1.6%)    75.6% (0.4%)\n",
      "3               PaLM-2L + CoT      65% (2%)    75.3% (1.5%)\n",
      "4        PaLM-2L + CoT 1-shot  61.5% (1.8%)      76.6% (1%)\n",
      "5               PaLM-2L + TDB  65.7% (0.7%)    73.8% (1.1%)\n",
      "6  PaLM-2L + Step-Back (ours)   73.2%(1.9%)    81.8% (1.4%)\n",
      "7                       GPT-4  70.3% (2.3%)    79.9% (1.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 2:Strong performance ofStep-Back Promptingon Knowledge QA tasks. CoT: Chain of Thought prompting, TDB: Take a Deep Breathe prompting, RAG: retrieval-augmented generation.Step-Back Promptingresults in significant performance improvements.\n",
      "                                   0       1         2         3             4\n",
      "0                             Method  TimeQA  TQA Easy  TQA Hard    SituatedQA\n",
      "1                            PaLM-2L   41.5%     42.6%     40.4%  54.3% (0.3%)\n",
      "2                     PaLM-2L 1-shot   40.7%     41.7%     39.1%  51.8% (0.6%)\n",
      "3                      PaLM-2L + CoT   40.8%     41.8%     39.8%  56.4% (0.2%)\n",
      "4               PaLM-2L + CoT 1-shot   38.1%     39.3%     36.8%    54% (0.8%)\n",
      "5                      PaLM-2L + TDB   40.9%     42.6%     39.1%    54% (0.5%)\n",
      "6                      PaLM-2L + RAG   57.4%     67.8%     46.8%  59.3% (0.4%)\n",
      "7         PaLM-2L + Step-Back (ours)     66%     70.4%     61.6%  57.5% (0.3%)\n",
      "8   PaLM-2L + Step-Back + RAG (ours)   68.7%     75.2%     62.3%    61% (0.4%)\n",
      "9                              GPT-4   45.6%     48.9%     42.6%   63.2%(0.4%)\n",
      "10                                                                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 3:Results ofStep-Back Promptingon Multi-Hop Reasoning. CoT: Chain of Thought prompting, TDB: Take a Deep Breathe prompting, RAG: retrieval augmentation generation. Average accuracy is over 5 evaluation runs with the standard deviations included in the parentheses.\n",
      "  0                                 1             2             3\n",
      "0                              Method       MuSiQue    StrategyQA\n",
      "1                             PaLM-2L    35.5% (3%)  82.8% (0.7%)\n",
      "2                      PaLM-2L 1-shot  29.0% (0.5%)  76.6% (0.5%)\n",
      "3                       PaLM-2L + CoT  38.7% (3.2%)  83.6% (0.4%)\n",
      "4                PaLM-2L + CoT 1-shot  38.5% (2.2%)  76.8% (1.4%)\n",
      "5                       PaLM-2L + TDB  39.0% (2.3%)  82.7% (0.9%)\n",
      "6                       PaLM-2L + RAG  39.6% (2.8%)  84.2% (0.5%)\n",
      "7          PaLM-2L + Step-Back (ours)  42.6% (3.1%)  82.7% (0.4%)\n",
      "8    PaLM-2L + Step-Back + RAG (ours)  42.8% (2.0%)     86.4%(1%)\n",
      "9                               GPT-4  38.5% (0.2%)  78.3% (1.1%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 4:Stats of the evaluation datasets used in this paper.\n",
      "                     0                           1      2                   3\n",
      "0               Domain                     Dataset  Split  Number of Examples\n",
      "1                 STEM    MMLU high-school Physics   Test                 151\n",
      "2                       MMLU high-school Chemistry   Test                 203\n",
      "3         Knowledge QA                      TimeQA   Test                5226\n",
      "4                                      TimeQA Easy   Test                2613\n",
      "5                                      TimeQA Hard   Test                2613\n",
      "6                                       SituatedQA   Test                2901\n",
      "7  Multi-hop Reasoning                     MuSiQue    Dev                2417\n",
      "8                                       StrategyQA    Dev                 229\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 5:Illustration of few shot evaluation with the PaLM-2L model.\n",
      "                                                   0\n",
      "0  Are the following two answers to the given que...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 6:Prompt of extracting the underlying principles involved in MMLU physics and chemistry questions.\n",
      "                                                   0\n",
      "0      MMLU Physics/Chemistry First-Principle Prompt\n",
      "1  You are an expert at Physics/Chemistry. You ar...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 7:Prompt of querying the model for final answer with first principles behind the question in MMLU high-school Physics and Chemistry.\n",
      "                                                   0\n",
      "0         MMLU Physics/Chemistry Final Answer Prompt\n",
      "1  You are an expert at Physics/Chemistry. You ar...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 8:Few-shot demonstration exemplar for MMLU high-school Physics.\n",
      "            0                                                  1\n",
      "0    Question  A spherical conductor carries a net charge. Ho...\n",
      "1  Principles  Coulomb’s Law: the force between two charged p...\n",
      "2      Answer  Using the Principles of Coulomb’s Law, we can ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 9:Few-shot demonstration exemplar for MMLU high-school Chemistry.\n",
      "            0                                                  1\n",
      "0    Question  A sample of an unknown chloride compound was d...\n",
      "1  Principles  Precipitation reactions: Precipitation reactio...\n",
      "2      Answer  Assuming the unknown chloride compound is MCl,...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 10:Prompt of asking step-back question in Knowledge QA tasks.\n",
      "                                                   0\n",
      "0                      Knowledge QA Step-Back Prompt\n",
      "1  You are an expert at world knowledge. Your tas...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 11:Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n",
      "             0                                                  1  \\\n",
      "0      dataset                                  Original Question   \n",
      "1       TimeQA  Which position did Knox Cunningham hold from M...   \n",
      "2       TimeQA  Who was the spouse of Anna Karina from 1968 to...   \n",
      "3       TimeQA  Which team did Thierry Audel play for from 200...   \n",
      "4       TimeQA  What was the operator of GCR Class 11E from 19...   \n",
      "5       TimeQA  Which country did Sokolovsko belong to from 13...   \n",
      "6   SituatedQA  when was the last time a team from canada won ...   \n",
      "7   SituatedQA  when did england last get to the semi final in...   \n",
      "8   SituatedQA  what is the biggest hotel in las vegas nv as o...   \n",
      "9   SituatedQA  who has scored most runs in t20 matches as of ...   \n",
      "10  SituatedQA  who is the highest paid player in the nba this...   \n",
      "\n",
      "                                                    2  \n",
      "0                                  Step-back Question  \n",
      "1   Which positions have Knox Cunningham held in h...  \n",
      "2                Who were the spouses of Anna Karina?  \n",
      "3   Which teams did Thierry Audel play for in his ...  \n",
      "4   What were the operators of GCR Class 11E in hi...  \n",
      "5   Which countries did Sokolovsko belong to in hi...  \n",
      "6   which years did a team from canada won the sta...  \n",
      "7   which years did england get to the semi final ...  \n",
      "8   what is the size of the hotels in las vegas nv...  \n",
      "9   What are the runs of players in t20 matches as...  \n",
      "10  what is the salary of the high paid players in...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 12:Prompt of querying the model for final answer with additional contexts from original and step-back retrieval augmentations in TimeQA and SituatedQA\n",
      "                                                   0\n",
      "0                   Knowledge QA Final-Answer Prompt\n",
      "1  You are an expert of world knowledge. I am goi...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 13:Few-shot demonstration exemplars for asking step-back questions in MuSiQue and StrategyQA.\n",
      "             0                                                  1  \\\n",
      "0      dataset                                  Original Question   \n",
      "1      MuSiQue  at year saw the creation of the region where t...   \n",
      "2      MuSiQue             Jan Šindel’s was born in what country?   \n",
      "3      MuSiQue  When was the abolishment of the studio that di...   \n",
      "4      MuSiQue  What city is the person who broadened the doct...   \n",
      "5      MuSiQue  When was the baseball team winning the world s...   \n",
      "6   StrategyQA  Could the members of The Police perform lawful...   \n",
      "7   StrategyQA  Would a Monoamine Oxidase candy bar cheer up a...   \n",
      "8   StrategyQA      Would a dog respond to bell before Grey seal?   \n",
      "9   StrategyQA       Is shrimp scampi definitely free of plastic?   \n",
      "10  StrategyQA        Do the anchors on Rede Globo speak Chinese?   \n",
      "\n",
      "                                                    2  \n",
      "0                                  Step-back Question  \n",
      "1   which region is the county of Hertfordshire lo...  \n",
      "2              what is Jan Šindel’s personal history?  \n",
      "3                  which studio distributed The Game?  \n",
      "4   who broadened the doctrine of philosophy of la...  \n",
      "5   which baseball team won the world series in 20...  \n",
      "6              what can the members of The Police do?  \n",
      "7          What are the effects of Monoamine Oxidase?  \n",
      "8       Would a dog respond to bell before Grey seal?  \n",
      "9                      what is shrimp scampi made of?  \n",
      "10  What languages do the anchors on Rede Globo sp...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 14:Prompt of querying the baseline model for final answer with few-shot demonstration exemplars.\n",
      "                                                   0\n",
      "0                           Baseline few-shot Prompt\n",
      "1  You are an expert of world knowledge and physi...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 15:Demonstration exemplar for standard prompting.\n",
      "                0                                                  1  \\\n",
      "0         dataset                                           Question   \n",
      "1    MMLU Physics  A point charge, Q1 = +1 mC, is fixed at the or...   \n",
      "2  MMLU Chemistry  A sample of an unknown chloride compound was d...   \n",
      "3          TimeQA  Who was the spouse of Anna Karina from 1968 to...   \n",
      "4      SituatedQA  what is the biggest hotel in las vegas nv as o...   \n",
      "5         MuSiQue  What year saw the creation of the region where...   \n",
      "6      StrategyQA  Would a Monoamine Oxidase candy bar cheer up a...   \n",
      "\n",
      "                                                   2  \n",
      "0                                             Answer  \n",
      "1  The work required to move a charge in an elect...  \n",
      "2  0.0050 moles of lead chloride precipitate cont...  \n",
      "3  Anna Karina’s spouse from 1968 to 1974 was Fre...  \n",
      "4  The biggest hotel in Las Vegas, Nevada as of N...  \n",
      "5                                               1994  \n",
      "6  A Monoamine Oxidase (MAO) candy bar would not ...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 16:Demonstration exemplar for CoT prompting for MMLU Physics.\n",
      "              0                                                  1  \\\n",
      "0       dataset                                           Question   \n",
      "1  MMLU Physics  A point charge, Q1 = +1 mC, is fixed at the or...   \n",
      "\n",
      "                                                   2  \n",
      "0                                             Answer  \n",
      "1  Sure, let’s think step by step.1. Calculate th...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 17:Demonstration exemplar for CoT prompting for MMLU Chemistry.\n",
      "                0                                                  1  \\\n",
      "0         dataset                                           Question   \n",
      "1  MMLU Chemistry  A sample of an unknown chloride compound was d...   \n",
      "\n",
      "                                                   2  \n",
      "0                                             Answer  \n",
      "1  Here’s the step-by-step solution:Identify the ...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 18:Demonstration exemplar for CoT prompting for TimeQA and SituatedQA.\n",
      "            0                                                  1  \\\n",
      "0     dataset                                           Question   \n",
      "1      TimeQA  Who was the spouse of Anna Karina from 1968 to...   \n",
      "2  SituatedQA  what is the biggest hotel in las vegas nv as o...   \n",
      "\n",
      "                                                   2  \n",
      "0                                             Answer  \n",
      "1  Step 1: Identify Anna Karina’s spouses:Jean-Lu...  \n",
      "2  Sure, let’s think step by step:\\nWhat were the...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 19:Demonstration exemplar for CoT prompting for MuSiQue and StrategyQA.\n",
      "            0                                                  1  \\\n",
      "0     dataset                                           Question   \n",
      "1     MuSiQue  What year saw the creation of the region where...   \n",
      "2  StrategyQA  Would a Monoamine Oxidase candy bar cheer up a...   \n",
      "\n",
      "                                                   2  \n",
      "0                                             Answer  \n",
      "1  The county of Hertfordshire is in the ’East of...  \n",
      "2  No, a Monoamine Oxidase (MAO) candy bar would ...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 20:Error example ofStep-Back Promptingon MMLU high-school Physics in the class ofFactual Error. In this case, the Factual error leads to the wrong answer despite that the Principle and Reasoning are all sound.\n",
      "            0                                                  1\n",
      "0    Question  An alarm whose frequency is 400 Hz is dropped ...\n",
      "1  Principles  Doppler Effect: the effect produced by the mot...\n",
      "2      Answer  Using the Principles of Doppler Effect, we can...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 21:Error example ofStep-Back Promptingon MMLU high-school Physics in the class ofMath Error. As highlighted, the math error leads to directly a wrong answer with a factor of 2 off.\n",
      "            0                                                  1\n",
      "0    Question  An astronaut lands on a planet whose mass and ...\n",
      "1  Principles  Weight: W = m * g, where W is the weight, m is...\n",
      "2      Answer  Using the Principles of Newton’s law of univer...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 22:Error example ofStep-Back Promptingon MMLU high-school Physics in the class ofPrinciple Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule out option (2) of fractional charges.\n",
      "            0                                                  1\n",
      "0    Question  Two spheres of net charge +5e and -6e briefly ...\n",
      "1  Principles  Law of conservation of charge: the total charg...\n",
      "2      Answer  Using the Principles of Conservation of charge...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 23:Error example ofStep-Back Promptingon MMLU high-school Physics in the class ofContext Loss. In this case, at the end of the Reasoning chain, the model forgot the original question, and lose the context to apply the reasoning to the question.\n",
      "            0                                                  1\n",
      "0    Question  The circumference of a helium-filled balloon i...\n",
      "1  Principles  Ideal Gas Law: PV = nRT, where P is the pressu...\n",
      "2      Answer  Using the Principles of Charles’s Law and Gay-...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 24:Error example ofStep-Back Promptingon MMLU high-school Physics in the class ofReasoning Error. In this case, the model was able to correctly identify air resistance is the cause of the velocity difference, but made the error at the final reasoning step of deriving the right answer.\n",
      "            0                                                  1\n",
      "0    Question  Physics students are checking the constant acc...\n",
      "1  Principles  Vertical motion: y = 0.5 * g *t2, where y is t...\n",
      "2      Answer  Using the Principles of Vertical motion and Ho...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 25:Illustration of wins ofStep-Back Promptingon the MMLU-Physics dataset.\n",
      "              0                                                  1\n",
      "0       dataset                                            example\n",
      "1  MMLU-Physics  Original Question: What happens to the pressur...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 26:Illustration of wins ofStep-Back Promptingon the MMLU-Physics dataset.\n",
      "              0                                                  1\n",
      "0       dataset                                            example\n",
      "1  MMLU-Physics  Original Question: A liquid flows at a constan...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 27:Illustration of wins ofStep-Back Promptingon the MMLU-Physics dataset.\n",
      "              0                                                  1\n",
      "0       dataset                                            example\n",
      "1  MMLU-Physics  Original Question: A 2μF capacitor is connecte...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 28:Illustration of wins ofStep-Back Promptingon the MMLU-Chemistry dataset.\n",
      "                0                                                  1\n",
      "0         dataset                                            example\n",
      "1  MMLU-Chemistry  Original Question: Which of the following indi...\n",
      "2  MMLU-Chemistry  Original Question: Potassium-40 is a minor iso...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 29:Illustration of wins ofStep-Back Promptingon the TimeQA dataset.\n",
      "         0                                                  1\n",
      "0  dataset                                            example\n",
      "1   TimeQA  Original Question: Carlos Ascues played for wh...\n",
      "2   TimeQA  Original Question: Seth Nana Twumasi played fo...\n",
      "3   TimeQA  Original Question: What position did Carmel Ha...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 30:Illustration of wins ofStep-Back Promptingon the SituatedQA dataset.\n",
      "            0                                                  1\n",
      "0     dataset                                            example\n",
      "1  SituatedQA  Original Question: when was the last time miss...\n",
      "2  SituatedQA  Original Question: when is the last time congr...\n",
      "3  SituatedQA  Original Question: when was the last time unc ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "table\n",
      "Table 31:Illustration of wins ofStep-Back Promptingon the StrategyQA dataset.\n",
      "            0                                                  1\n",
      "0     dataset                                            example\n",
      "1  StrategyQA  Original Question: Could you drive a Rowe 550 ...\n",
      "2  StrategyQA  Original Question: what channel does thursday ...\n",
      "3  StrategyQA  Original Question: who is the present presiden...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 1:Strong Performance ofStep-Back Prompting: our proposed Abstraction-and-Reasoning scheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge QA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x1.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 2:Illustration ofStep-Back Promptingwith two steps of Abstraction and Reasoning guided by concepts and principles.Top: an example of MMLU high-school physics(Hendryckset al.,2020)where the first principle of Ideal Gas Law is retrieved via abstraction.Bottom: an example from TimeQA(Chenet al.,2021)where the high-level concept of education history is a result of the abstraction.Left: PaLM-2L(Anilet al.,2023)fails to answer the original question. Chain-of-Thought prompting(Weiet al.,2022b; Kojimaet al.,2022)ran into errors during intermediate reasoning steps (highlighted as red).Right: PaLM-2L(Anilet al.,2023)successfully answers the question viaStep-Back Prompting.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x2.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 3:Ablation study ofStep-Back Promptingaccuracy on MMLU high-school Physics against number of few shot exemplars: robust performance with respect to varying number of shots.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x3.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 4:Error Analysis ofStep-Back Promptingon MMLU high-school Physics.Left: example categories in four buckets regarding whether the baseline or Step-Back prediction is right or wrong.Right: five classes of errors Step-Back makes with Reasoning being the dominating class.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x4.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 5:Ablation and error analysis ofStep-Back Promptingon TimeQA.Left: ablation against number of few-shot exemplars.Right: four classes of errors Step-Back makes with Reasoning and RAG being the dominating error sources.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x5.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 6:Error Analysis of Step-Back Prompting on TimeQA.Left: Step-Back + RAG vs Baseline predictions.Right: Step-Back RAG vs RAG predictions. Step-Back + RAG is able to fix39.9%of the predictions where the baseline prediction is wrong, while causing5.6%errors. Furthermore, Step-Back + RAG fixes21.6%errors coming from RAG. The%of errors introduced byStep-Back Promptingto RAG is still relatively low (6.3%).\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x6.png\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "Figure 7:Error Analysis of Step-Back Prompting on StrategyQA.Left: Step-Back + RAG vs Baseline predictions.Right: Step-Back + RAG vs RAG predictions. Step-Back + RAG is able to turn15.4%wrong predictions into correct predictions, while leading to6.1%errors the other way around.\n",
      "Furthermore, Step-Back + RAG fixes12.7%errors coming from RAG. The errors introduced to RAG by Step-Back is just4.4%.\n",
      "https://media.arxiv-vanity.com/render-output/8350071/x7.png\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_dfs = []\n",
    "for content in content_items:\n",
    "    print(content['type'])\n",
    "    print(content['caption'])\n",
    "    try:\n",
    "        if content['type'] == 'table':\n",
    "            print(pd.DataFrame(content['content']))\n",
    "            # first row is header\n",
    "            table_dfs.append(pd.DataFrame(content['content'][1:], columns=content['content'][0]))\n",
    "        elif content['type'] == 'image':\n",
    "            print(content['content'])\n",
    "    except:\n",
    "        print(\"Error parsing table\")\n",
    "        print(content['content'])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>Original Question</th>\n",
       "      <th>Step-back Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TimeQA</td>\n",
       "      <td>Which position did Knox Cunningham hold from M...</td>\n",
       "      <td>Which positions have Knox Cunningham held in h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TimeQA</td>\n",
       "      <td>Who was the spouse of Anna Karina from 1968 to...</td>\n",
       "      <td>Who were the spouses of Anna Karina?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TimeQA</td>\n",
       "      <td>Which team did Thierry Audel play for from 200...</td>\n",
       "      <td>Which teams did Thierry Audel play for in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TimeQA</td>\n",
       "      <td>What was the operator of GCR Class 11E from 19...</td>\n",
       "      <td>What were the operators of GCR Class 11E in hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TimeQA</td>\n",
       "      <td>Which country did Sokolovsko belong to from 13...</td>\n",
       "      <td>Which countries did Sokolovsko belong to in hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SituatedQA</td>\n",
       "      <td>when was the last time a team from canada won ...</td>\n",
       "      <td>which years did a team from canada won the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SituatedQA</td>\n",
       "      <td>when did england last get to the semi final in...</td>\n",
       "      <td>which years did england get to the semi final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SituatedQA</td>\n",
       "      <td>what is the biggest hotel in las vegas nv as o...</td>\n",
       "      <td>what is the size of the hotels in las vegas nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SituatedQA</td>\n",
       "      <td>who has scored most runs in t20 matches as of ...</td>\n",
       "      <td>What are the runs of players in t20 matches as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SituatedQA</td>\n",
       "      <td>who is the highest paid player in the nba this...</td>\n",
       "      <td>what is the salary of the high paid players in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset                                  Original Question  \\\n",
       "0      TimeQA  Which position did Knox Cunningham hold from M...   \n",
       "1      TimeQA  Who was the spouse of Anna Karina from 1968 to...   \n",
       "2      TimeQA  Which team did Thierry Audel play for from 200...   \n",
       "3      TimeQA  What was the operator of GCR Class 11E from 19...   \n",
       "4      TimeQA  Which country did Sokolovsko belong to from 13...   \n",
       "5  SituatedQA  when was the last time a team from canada won ...   \n",
       "6  SituatedQA  when did england last get to the semi final in...   \n",
       "7  SituatedQA  what is the biggest hotel in las vegas nv as o...   \n",
       "8  SituatedQA  who has scored most runs in t20 matches as of ...   \n",
       "9  SituatedQA  who is the highest paid player in the nba this...   \n",
       "\n",
       "                                  Step-back Question  \n",
       "0  Which positions have Knox Cunningham held in h...  \n",
       "1               Who were the spouses of Anna Karina?  \n",
       "2  Which teams did Thierry Audel play for in his ...  \n",
       "3  What were the operators of GCR Class 11E in hi...  \n",
       "4  Which countries did Sokolovsko belong to in hi...  \n",
       "5  which years did a team from canada won the sta...  \n",
       "6  which years did england get to the semi final ...  \n",
       "7  what is the size of the hotels in las vegas nv...  \n",
       "8  What are the runs of players in t20 matches as...  \n",
       "9  what is the salary of the high paid players in...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_dfs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## html parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.crawl.htmlParser import HTMLParserExtended\n",
    "from bs4 import BeautifulSoup\n",
    "html_parser = HTMLParserExtended()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = request_content(f\"https://www.arxiv-vanity.com/papers/{arxiv_num}/\").decode('utf-8')\n",
    "\n",
    "content = str(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "sections = soup.find_all('section')\n",
    "len(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_parser.parse_html_str(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Content: image, Section: 1Introduction,\n",
       " Content: table, Section: 3EmotionPrompt,\n",
       " Content: image, Section: 3EmotionPrompt,\n",
       " Content: image, Section: 3.1Motivation,\n",
       " Content: table, Section: 3.2Taking inspiration from Psychology,\n",
       " Content: table, Section: 4Experiments,\n",
       " Content: table, Section: 4Experiments,\n",
       " Content: table, Section: 4Experiments,\n",
       " Content: table, Section: 4Experiments,\n",
       " Content: table, Section: 4Experiments,\n",
       " Content: table, Section: 4.2Our Emotional Prompts,\n",
       " Content: table, Section: 4.3Main Results,\n",
       " Content: table, Section: 4.4Truthfulness and Informativeness,\n",
       " Content: table, Section: 4.4Truthfulness and Informativeness,\n",
       " Content: table, Section: 4.5Effect of More Emotional Stimulus,\n",
       " Content: table, Section: 5Analysis,\n",
       " Content: image, Section: 5Analysis,\n",
       " Content: image, Section: 5.1Why EmotionPrompt Works?,\n",
       " Content: table, Section: 5.2Human Study]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_parser.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1Introduction (Level 2)\n",
      "- 2Background (Level 2)\n",
      "  - 2.1Psychology on Emotion Study (Level 3)\n",
      "  - 2.2Large Language Models (Level 3)\n",
      "- 3EmotionPrompt (Level 2)\n",
      "  - 3.1Motivation (Level 3)\n",
      "  - 3.2Taking inspiration from Psychology (Level 3)\n",
      "- 4Experiments (Level 2)\n",
      "  - 4.1Setup (Level 3)\n",
      "  - 4.2Our Emotional Prompts (Level 3)\n",
      "  - 4.3Main Results (Level 3)\n",
      "  - 4.4Truthfulness and Informativeness (Level 3)\n",
      "  - 4.5Effect of More Emotional Stimulus (Level 3)\n",
      "- 5Analysis (Level 2)\n",
      "  - 5.1Why EmotionPrompt Works? (Level 3)\n",
      "  - 5.2Human Study (Level 3)\n",
      "- 6Conclusion and Limitation (Level 2)\n",
      "- Ethical Statement (Level 2)\n",
      "- References (Level 2)\n"
     ]
    }
   ],
   "source": [
    "html_parser.print_hierarchy(print_contents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Content: table, Section: 4.3Main Results]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_parser.sections[10].contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_parser.sections[8].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1Introduction (Level 2)\n",
      "  * image: {'type': 'image', 'caption': 'Figure 1:An illustration of EmotionPrompt.', 'content': 'https://media.arxiv-vanity.com/render-output/8207042/x1.png'}\n",
      "- 2Background (Level 2)\n",
      "  - 2.1Psychology on Emotion Study (Level 3)\n",
      "  - 2.2Large Language Models (Level 3)\n",
      "- 3EmotionPrompt (Level 2)\n",
      "  * table: {'type': 'table', 'caption': 'Table 1:Statistics of test sets in this paper', 'content': [['Category', 'Task', 'Original Prompt'], ['Semantics', 'Sentiment Analysis (100)', 'Determine whether a movie review is positive or negative.'], ['Sentence Similarity (100)', 'Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.'], ['Word in Context (100)', 'Determine whether an input word has the same meaning in the two input sentences.'], ['Cause Selection (25)', 'Find which of the two given cause and effect sentences is the cause.'], ['Knowledge', 'Larger Animal (100)', 'Write the larger of the two given animals.'], ['Numerical', 'Sum (100)', 'Sum the two given numbers.'], ['Spelling', 'Starting With (100)', 'Extract the words starting with a given letter from the input sentence.'], ['First Letter (100)', 'Extract the first letter of the input word.']]}\n",
      "  * image: {'type': 'image', 'caption': 'Figure 2:Left:Psychology theories and emotional stimulus.Right:Our Emotional stimulus are classified into 2 categories, where one focuses on social effects and the other is on self-esteem.', 'content': 'https://media.arxiv-vanity.com/render-output/8207042/x2.png'}\n",
      "  - 3.1Motivation (Level 3)\n",
      "    * image: {'type': 'image', 'caption': 'Figure 2:Left:Psychology theories and emotional stimulus.Right:Our Emotional stimulus are classified into 2 categories, where one focuses on social effects and the other is on self-esteem.', 'content': 'https://media.arxiv-vanity.com/render-output/8207042/x2.png'}\n",
      "  - 3.2Taking inspiration from Psychology (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 1:Statistics of test sets in this paper', 'content': [['Category', 'Task', 'Original Prompt'], ['Semantics', 'Sentiment Analysis (100)', 'Determine whether a movie review is positive or negative.'], ['Sentence Similarity (100)', 'Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.'], ['Word in Context (100)', 'Determine whether an input word has the same meaning in the two input sentences.'], ['Cause Selection (25)', 'Find which of the two given cause and effect sentences is the cause.'], ['Knowledge', 'Larger Animal (100)', 'Write the larger of the two given animals.'], ['Numerical', 'Sum (100)', 'Sum the two given numbers.'], ['Spelling', 'Starting With (100)', 'Extract the words starting with a given letter from the input sentence.'], ['First Letter (100)', 'Extract the first letter of the input word.']]}\n",
      "- 4Experiments (Level 2)\n",
      "  * table: {'type': 'table', 'caption': 'Table 2:Our Emotion Stimulus', 'content': [['Stimulus ID', 'Stimulus Content'], ['EP_01', ', write your answer and give me a confidence score between 0-1 for your answer.'], ['EP_02', 'This is very important to my career.'], ['EP_03', 'You’d better be sure.'], ['EP_04', 'Are you sure?'], ['EP_05', 'Are you sure that’s your final answer? It might be worth taking another look.'], ['EP_06', 'Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.'], ['EP_07', 'Are you sure that’s your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.'], ['EP_08', 'Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.'], ['EP_09', 'Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.'], ['EP_10', 'Take pride in your work and give it your best. Your commitment to excellence sets you apart.'], ['EP_11', 'Remember that progress is made one step at a time. Stay determined and keep moving forward.']]}\n",
      "  * table: {'type': 'table', 'caption': 'Table 3:Results on Different Models and Tasks. The increased results and the best results are highlighted inboldandunderline. The value 0.00 indicates that we do not receive meaningful or useful response that is left for future investigation.', 'content': [['Model', 'Prompt', 'Zero-shot', 'Few-shot'], ['SA', 'SS', 'LA', 'Sum', 'SW', 'WC', 'CS', 'FL', 'SA', 'SS', 'LA', 'Sum', 'SW', 'WC', 'CS', 'FL'], ['ChatGPT', 'Origin', '0.82', '0.41', '0.77', '0.93', '0.32', '0.51', '0.96', '1.00', '0.78', '0.4', '0.9', '1.00', '0.41', '0.52', '0.72', '1.00'], ['EP_01', '0.91', '0.52', '0.91', '1.00', '0.36', '0.54', '1.00', '1.00', '0.9', '0.39', '0.9', '1.00', '0.2', '0.57', '0.64', '1.00'], ['EP_02', '0.85', '0.53', '0.91', '1.00', '0.46', '0.57', '0.96', '1.00', '0.82', '0.47', '0.92', '1.00', '0.54', '0.52', '0.8', '1.00'], ['EP_03', '0.86', '0.52', '0.9', '1.00', '0.48', '0.57', '1.00', '1.00', '0.82', '0.42', '0.93', '1.00', '0.45', '0.59', '0.76', '1.00'], ['EP_04', '0.83', '0.49', '0.88', '1.00', '0.45', '0.51', '0.96', '1.00', '0.82', '0.44', '0.9', '1.00', '0.42', '0.56', '0.72', '1.00'], ['EP_05', '0.84', '0.5', '0.88', '1.00', '0.49', '0.55', '0.96', '1.00', '0.84', '0.42', '0.95', '1.00', '0.46', '0.55', '0.56', '1.00'], ['EP_06', '0.89', '0.56', '0.91', '1.00', '0.32', '0.62', '1.00', '1.00', '0.95', '0.39', '0.87', '1.00', '0.35', '0.55', '0.64', '1.00'], ['EP_07', '0.88', '0.49', '0.83', '1.00', '0.38', '0.47', '0.88', '1.00', '0.8', '0.42', '0.94', '1.00', '0.47', '0.49', '0.88', '1.00'], ['EP_08', '0.86', '0.51', '0.9', '1.00', '0.43', '0.63', '0.64', '1.00', '0.84', '0.44', '0.93', '1.00', '0.49', '0.62', '0.84', '1.00'], ['EP_09', '0.87', '0.52', '0.88', '1.00', '0.53', '0.54', '0.8', '1.00', '0.8', '0.48', '0.9', '1.00', '0.51', '0.56', '0.8', '1.00'], ['EP_10', '0.87', '0.56', '0.91', '1.00', '0.49', '0.58', '0.72', '1.00', '0.77', '0.41', '0.91', '1.00', '0.47', '0.59', '0.88', '1.00'], ['EP_11', '0.88', '0.54', '0.9', '1.00', '0.49', '0.54', '0.96', '1.00', '0.84', '0.56', '0.93', '1.00', '0.47', '0.57', '0.8', '1.00'], ['avg', '0.87', '0.52', '0.89', '1.00', '0.44', '0.56', '0.90', '1.00', '0.84', '0.44', '0.92', '1.00', '0.44', '0.56', '0.76', '1.00'], ['CoT', '0.79', '0.47', '0.91', '1.00', '0.31', '0.52', '0.96', '1.00', '0.77', '0.52', '0.92', '1.00', '0.42', '0.52', '0.76', '1.00'], ['T5', 'Origin', '0.93', '0.55', '0.71', '0.57', '0.56', '0.03', '0.03', '0.00', '0.92', '0.54', '0.61', '0.03', '0.00', '0.47', '0.52', '0.00'], ['EP_01', '0.91', '0.64', '0.52', '0.41', '0.00', '0.06', '0.02', '0.00', '0.93', '0.54', '0.63', '0.04', '0.00', '0.43', '0.00', '0.00'], ['EP_02', '0.93', '0.58', '0.71', '0.58', '0.48', '0.04', '0.02', '0.00', '0.92', '0.56', '0.62', '0.04', '0.00', '0.51', '0.68', '0.00'], ['EP_03', '0.93', '0.57', '0.69', '0.61', '0.72', '0.05', '0.03', '0.00', '0.93', '0.58', '0.65', '0.03', '0.00', '0.53', '0.68', '0.00'], ['EP_04', '0.93', '0.58', '0.58', '0.57', '0.64', '0.07', '0.02', '0.00', '0.93', '0.56', '0.63', '0.03', '0.00', '0.51', '0.68', '0.00'], ['EP_05', '0.92', '0.57', '0.74', '0.58', '0.6', '0.08', '0.03', '0.00', '0.93', '0.58', '0.69', '0.02', '0.00', '0.49', '0.72', '0.00'], ['EP_06', '0.97', '0.61', '0.55', '0.44', '0.00', '0.11', '0.03', '0.00', '0.93', '0.54', '0.71', '0.03', '0.00', '0.49', '0.04', '0.00'], ['EP_07', '0.92', '0.58', '0.68', '0.6', '0.48', '0.08', '0.02', '0.00', '0.93', '0.55', '0.66', '0.03', '0.00', '0.48', '0.56', '0.00'], ['EP_08', '0.93', '0.58', '0.64', '0.56', '0.64', '0.08', '0.02', '0.00', '0.93', '0.6', '0.61', '0.04', '0.00', '0.5', '0.64', '0.00'], ['EP_09', '0.93', '0.58', '0.64', '0.57', '0.44', '0.08', '0.02', '0.00', '0.93', '0.58', '0.63', '0.04', '0.00', '0.51', '0.52', '0.00'], ['EP_10', '0.93', '0.58', '0.62', '0.59', '0.48', '0.09', '0.02', '0.00', '0.92', '0.58', '0.6', '0.04', '0.00', '0.48', '0.6', '0.00'], ['EP_11', '0.93', '0.59', '0.64', '0.55', '0.56', '0.07', '0.02', '0.00', '0.93', '0.57', '0.64', '0.04', '0.00', '0.49', '0.64', '0.00'], ['avg', '0.93', '0.59', '0.64', '0.55', '0.46', '0.07', '0.02', '0.00', '0.93', '0.57', '0.64', '0.03', '0.00', '0.49', '0.52', '0.00'], ['CoT', '0.94', '0.58', '0.87', '0.51', '0.52', '0.06', '0.03', '0.00', '0.92', '0.54', '0.6', '0.03', '0.00', '0.45', '0.48', '0.00'], ['Bloom', 'Origin', '0.68', '0.23', '0.74', '0.41', '0.06', '0.52', '0.48', '0.86', '0.77', '0.24', '0.43', '0.07', '0.18', '0.52', '0.6', '0.98'], ['EP_01', '0.76', '0.26', '0.6', '0.18', '0.03', '0.42', '0.52', '0.96', '0.91', '0.26', '0.52', '0.01', '0.22', '0.6', '0.76', '0.99'], ['EP_02', '0.79', '0.2', '0.75', '0.45', '0.08', '0.5', '0.6', '0.84', '0.87', '0.23', '0.54', '0.12', '0.19', '0.64', '0.72', '0.99'], ['EP_03', '0.72', '0.23', '0.81', '0.37', '0.05', '0.51', '0.52', '0.85', '0.85', '0.23', '0.49', '0.06', '0.2', '0.56', '0.6', '1.00'], ['EP_04', '0.66', '0.28', '0.67', '0.35', '0.03', '0.5', '0.36', '0.88', '0.83', '0.3', '0.52', '0.03', '0.22', '0.52', '0.8', '1.00'], ['EP_05', '0.81', '0.21', '0.71', '0.27', '0.02', '0.57', '0.48', '0.85', '0.85', '0.27', '0.51', '0.04', '0.16', '0.53', '0.76', '0.99'], ['EP_06', '0.56', '0.2', '0.74', '0.14', '0.09', '0.24', '0.4', '0.94', '0.89', '0.21', '0.46', '0.01', '0.23', '0.59', '0.64', '1.00'], ['EP_07', '0.64', '0.3', '0.7', '0.34', '0.02', '0.48', '0.72', '0.71', '0.86', '0.3', '0.54', '0.09', '0.22', '0.54', '0.68', '0.98'], ['EP_08', '0.4', '0.23', '0.72', '0.35', '0.01', '0.47', '0.48', '0.52', '0.85', '0.27', '0.51', '0.08', '0.18', '0.6', '0.76', '0.98'], ['EP_09', '0.38', '0.19', '0.79', '0.39', '0.04', '0.41', '0.8', '0.76', '0.87', '0.24', '0.48', '0.11', '0.22', '0.56', '0.76', '0.98'], ['EP_10', '0.44', '0.22', '0.76', '0.42', '0.03', '0.46', '0.44', '0.68', '0.87', '0.28', '0.47', '0.12', '0.19', '0.53', '0.8', '1.00'], ['EP_11', '0.62', '0.24', '0.79', '0.4', '0.05', '0.44', '0.68', '0.86', '0.9', '0.21', '0.53', '0.07', '0.17', '0.56', '0.68', '0.99'], ['avg', '0.62', '0.23', '0.73', '0.33', '0.04', '0.45', '0.55', '0.80', '0.87', '0.25', '0.51', '0.07', '0.2', '0.57', '0.72', '0.99'], ['CoT', '0.57', '0.22', '0.79', '0.49', '0.03', '0.44', '0.6', '0.86', '0.88', '0.2', '0.46', '0.06', '0.16', '0.54', '0.68', '0.99'], ['Vicuna', 'Origin', '0.4', '0.24', '0.82', '0.41', '0.02', '0.46', '0.56', '0.00', '0.77', '0.28', '0.68', '0.32', '0.05', '0.47', '0.32', '0.00'], ['EP_01', '0.60', '0.25', '0.84', '0.90', '0.00', '0.44', '0.60', '0.00', '0.53', '0.33', '0.46', '0.29', '0.05', '0.53', '0.44', '0.00'], ['EP_02', '0.28', '0.25', '0.82', '0.61', '0.06', '0.57', '0.06', '0.00', '0.8', '0.32', '0.60', '0.35', '0.09', '0.51', '0.52', '0.00'], ['EP_03', '0.49', '0.32', '0.85', '0.76', '0.02', '0.47', '0.56', '0.00', '0.76', '0.36', '0.58', '0.32', '0.03', '0.45', '0.44', '0.00'], ['EP_04', '0.3', '0.28', '0.59', '0.59', '0.02', '0.51', '0.64', '0.00', '0.69', '0.24', '0.54', '0.05', '0.04', '0.51', '0.52', '0.00'], ['EP_05', '0.50', '0.25', '0.64', '0.40', '0.00', '0.50', '0.48', '0.00', '0.70', '0.29', '0.26', '0.05', '0.01', '0.31', '0.48', '0.00'], ['EP_06', '0.71', '0.31', '0.83', '0.81', '0.01', '0.55', '0.76', '0.00', '0.39', '0.27', '0.29', '0.12', '0.03', '0.45', '0.32', '0.00'], ['EP_07', '0.41', '0.20', '0.87', '0.84', '0.03', '0.50', '0.64', '0.00', '0.66', '0.27', '0.63', '0.13', '0.02', '0.34', '0.48', '0.00'], ['EP_08', '0.34', '0.25', '0.91', '0.89', '0.02', '0.55', '0.76', '0.00', '0.81', '0.3', '0.71', '0.48', '0.05', '0.53', '0.48', '0.00'], ['EP_09', '0.41', '0.28', '0.87', '0.87', '0.04', '0.55', '0.72', '0.00', '0.76', '0.37', '0.71', '0.41', '0.07', '0.46', '0.48', '0.00'], ['EP_10', '0.43', '0.25', '0.87', '0.82', '0.05', '0.53', '0.76', '0.00', '0.78', '0.35', '0.68', '0.43', '0.07', '0.50', '0.48', '0.00'], ['EP_11', '0.3', '0.27', '0.89', '0.17', '0.05', '0.47', '0.68', '0.00', '0.82', '0.24', '0.65', '0.43', '0.08', '0.48', '0.44', '0.00'], ['avg', '0.43', '0.26', '0.82', '0.70', '0.03', '0.51', '0.65', '0.00', '0.70', '0.30', '0.56', '0.28', '0.05', '0.46', '0.46', '0.00'], ['CoT', '0.34', '0.28', '0.74', '0.02', '0.01', '0.42', '0.4', '0.00', '0.8', '0.24', '0.42', '0.44', '0.05', '0.50', '0.37', '0.00']]}\n",
      "  * table: {'type': 'table', 'caption': 'Table 4:An Examination of the Effectiveness of Emotional Prompts: An Analysis through the Lens of Input Attention.', 'content': [['Prompt', 'Input Attention'], ['Origin', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fg'], ['EP_01', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.,fgwritefgyourfganswerfgandfggivefgmefgafgconfidencefgscorefgbetweenfg0-1fgforfgyourfganswer.fg'], ['EP_02', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgThisfgisfgveryfgimportantfgtofgmyfgcareer.fg'], ['EP_03', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgYou’dfgbetterfgbefgsure.fg'], ['EP_04', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsure?fg'], ['EP_05', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsurefgthat’sfgyourfgfinalfganswer?fgItfgmightfgbefgworthfgtakingfganotherfglook.fg'], ['EP_06', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgProvidefgyourfganswerfgandfgafgconfidencefgscorefgbetweenfg0-1fgforfgyourfgprediction.fgAdditionally,fgbrieflyfgexplainfgthefgmainfgreasonsfgsupportingfgyourfgclassificationfgdecisionfgtofghelpfgmefgunderstandfgyourfgthoughtfgprocess.fgThisfgtaskfgisfgvitalfgtofgmyfgcareer,fgandfgIfggreatlyfgvaluefgyourfgthoroughfganalysis.fg'], ['EP_07', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsurefgthat’sfgyourfgfinalfganswer?fgBelievefginfgyourfgabilitiesfgandfgstrivefgforfgexcellence.fgYourfghardfgworkfgwillfgyieldfgremarkablefgresults.fg'], ['EP_08', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgEmbracefgchallengesfgasfgopportunitiesfgforfggrowth.fgEachfgobstaclefgyoufgovercomefgbringsfgyoufgcloserfgtofgsuccess.fg'], ['EP_09', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgStayfgfocusedfgandfgdedicatedfgtofgyourfggoals.fgYourfgconsistentfgeffortsfgwillfgleadfgtofgoutstandingfgachievements.fg'], ['EP_10', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgTakefgpridefginfgyourfgworkfgandfggivefgitfgyourfgbest.fgYourfgcommitmentfgtofgexcellencefgsetsfgyoufgapart.fg'], ['EP_11', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgRememberfgthatfgprogressfgisfgmadefgonefgstepfgatfgafgtime.fgStayfgdeterminedfgandfgkeepfgmovingfgforward.fg'], ['', '']]}\n",
      "  * table: {'type': 'table', 'caption': 'Table 5:Result on TruthfulQA. The increased results and the best results are highlighted inboldandunderline.', 'content': [['', 'ChatGPT', 'Vicuna-13b', 'T5'], ['Prompt', '%true', '%info', '%true', '%info', '%true', '%info'], ['Origin', '0.75', '0.53', '0.77', '0.32', '0.54', '0.42'], ['EP_01', '0.61', '0.94', '0.12', '0.0', '0.26', '0.14'], ['EP_02', '0.83', '0.66', '0.97', '0.0', '0.61', '0.35'], ['EP_03', '0.82', '0.69', '0.99', '0.0', '0.53', '0.44'], ['EP_04', '0.87', '0.67', '0.87', '0.22', '0.62', '0.36'], ['EP_05', '0.87', '0.62', '1.0', '0.0', '0.46', '0.48'], ['EP_06', '0.78', '0.50', '0.39', '0.0', '0.49', '0.46'], ['EP_07', '0.83', '0.70', '0.99', '0.04', '0.77', '0.18'], ['EP_08', '0.81', '0.66', '0.99', '0.09', '0.56', '0.40'], ['EP_09', '0.81', '0.68', '0.86', '0.13', '0.52', '0.46'], ['EP_10', '0.81', '0.68', '0.84', '0.02', '0.50', '0.47'], ['EP_11', '0.81', '0.66', '1.0', '0.01', '0.57', '0.40'], ['avg', '0.80', '0.68', '0.82', '0.05', '0.54', '0.38'], ['CoT', '0.76', '0.44', '0.99', '0.0', '0.48', '0.33']]}\n",
      "  * table: {'type': 'table', 'caption': 'Table 6:Effect of More Emotional Stimulus.', 'content': [['CombinedPrompt', 'Combined', 'Prompt', 'Tasks'], ['Combined'], ['Prompt'], ['SA', 'SS', 'WC', 'CS', 'LA', 'Sum', 'SW'], ['EP_01+EP_02', '0.91', '0.42', '0.61', '1.0', '0.91', '1.0', '0.42'], ['EP_01+EP_03', '0.92', '0.44', '0.6', '1.0', '0.91', '1.0', '0.42'], ['EP_01+EP_04', '0.89', '0.42', '0.61', '1.0', '0.92', '1.0', '0.48'], ['EP_01+EP_05', '0.91', '0.42', '0.6', '1.0', '0.93', '1.0', '0.45'], ['EP_02+EP_03', '0.88', '0.39', '0.6', '1.0', '0.91', '1.0', '0.36'], ['EP_02+EP_08', '0.88', '0.38', '0.6', '0.76', '0.93', '1.0', '0.28'], ['EP_02+EP_09', '0.87', '0.39', '0.6', '0.8', '0.92', '1.0', '0.34'], ['EP_04+EP_06', '0.74', '0.55', '0.62', '1.0', '0.93', '1.0', '0.35'], ['EP_04+EP_07', '0.88', '0.42', '0.61', '0.84', '0.94', '1.0', '0.32'], ['EP_04+EP_08', '0.78', '0.42', '0.59', '0.64', '0.94', '1.0', '0.32'], ['EP_04+EP_09', '0.85', '0.34', '0.56', '0.6', '0.94', '1.0', '0.33'], ['EP_01+EP_04+EP_06', 'EP_01+EP_04', '+EP_06', '0.8', '0.52', '0.62', '1.0', '0.92', '1.0', '0.48'], ['EP_01+EP_04'], ['+EP_06'], ['EP_01+EP_04+EP_07', 'EP_01+EP_04', '+EP_07', '0.89', '0.43', '0.63', '1.0', '0.93', '1.0', '0.46'], ['EP_01+EP_04'], ['+EP_07'], ['EP_01+EP_04+EP_08', 'EP_01+EP_04', '+EP_08', '0.85', '0.4', '0.62', '0.88', '0.9', '1.0', '0.44'], ['EP_01+EP_04'], ['+EP_08'], ['EP_01+EP_04+EP_09', 'EP_01+EP_04', '+EP_09', '0.9', '0.39', '0.6', '1.0', '0.93', '1.0', '0.48'], ['EP_01+EP_04'], ['+EP_09']]}\n",
      "  - 4.1Setup (Level 3)\n",
      "  - 4.2Our Emotional Prompts (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 2:Our Emotion Stimulus', 'content': [['Stimulus ID', 'Stimulus Content'], ['EP_01', ', write your answer and give me a confidence score between 0-1 for your answer.'], ['EP_02', 'This is very important to my career.'], ['EP_03', 'You’d better be sure.'], ['EP_04', 'Are you sure?'], ['EP_05', 'Are you sure that’s your final answer? It might be worth taking another look.'], ['EP_06', 'Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.'], ['EP_07', 'Are you sure that’s your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.'], ['EP_08', 'Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.'], ['EP_09', 'Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.'], ['EP_10', 'Take pride in your work and give it your best. Your commitment to excellence sets you apart.'], ['EP_11', 'Remember that progress is made one step at a time. Stay determined and keep moving forward.']]}\n",
      "  - 4.3Main Results (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 3:Results on Different Models and Tasks. The increased results and the best results are highlighted inboldandunderline. The value 0.00 indicates that we do not receive meaningful or useful response that is left for future investigation.', 'content': [['Model', 'Prompt', 'Zero-shot', 'Few-shot'], ['SA', 'SS', 'LA', 'Sum', 'SW', 'WC', 'CS', 'FL', 'SA', 'SS', 'LA', 'Sum', 'SW', 'WC', 'CS', 'FL'], ['ChatGPT', 'Origin', '0.82', '0.41', '0.77', '0.93', '0.32', '0.51', '0.96', '1.00', '0.78', '0.4', '0.9', '1.00', '0.41', '0.52', '0.72', '1.00'], ['EP_01', '0.91', '0.52', '0.91', '1.00', '0.36', '0.54', '1.00', '1.00', '0.9', '0.39', '0.9', '1.00', '0.2', '0.57', '0.64', '1.00'], ['EP_02', '0.85', '0.53', '0.91', '1.00', '0.46', '0.57', '0.96', '1.00', '0.82', '0.47', '0.92', '1.00', '0.54', '0.52', '0.8', '1.00'], ['EP_03', '0.86', '0.52', '0.9', '1.00', '0.48', '0.57', '1.00', '1.00', '0.82', '0.42', '0.93', '1.00', '0.45', '0.59', '0.76', '1.00'], ['EP_04', '0.83', '0.49', '0.88', '1.00', '0.45', '0.51', '0.96', '1.00', '0.82', '0.44', '0.9', '1.00', '0.42', '0.56', '0.72', '1.00'], ['EP_05', '0.84', '0.5', '0.88', '1.00', '0.49', '0.55', '0.96', '1.00', '0.84', '0.42', '0.95', '1.00', '0.46', '0.55', '0.56', '1.00'], ['EP_06', '0.89', '0.56', '0.91', '1.00', '0.32', '0.62', '1.00', '1.00', '0.95', '0.39', '0.87', '1.00', '0.35', '0.55', '0.64', '1.00'], ['EP_07', '0.88', '0.49', '0.83', '1.00', '0.38', '0.47', '0.88', '1.00', '0.8', '0.42', '0.94', '1.00', '0.47', '0.49', '0.88', '1.00'], ['EP_08', '0.86', '0.51', '0.9', '1.00', '0.43', '0.63', '0.64', '1.00', '0.84', '0.44', '0.93', '1.00', '0.49', '0.62', '0.84', '1.00'], ['EP_09', '0.87', '0.52', '0.88', '1.00', '0.53', '0.54', '0.8', '1.00', '0.8', '0.48', '0.9', '1.00', '0.51', '0.56', '0.8', '1.00'], ['EP_10', '0.87', '0.56', '0.91', '1.00', '0.49', '0.58', '0.72', '1.00', '0.77', '0.41', '0.91', '1.00', '0.47', '0.59', '0.88', '1.00'], ['EP_11', '0.88', '0.54', '0.9', '1.00', '0.49', '0.54', '0.96', '1.00', '0.84', '0.56', '0.93', '1.00', '0.47', '0.57', '0.8', '1.00'], ['avg', '0.87', '0.52', '0.89', '1.00', '0.44', '0.56', '0.90', '1.00', '0.84', '0.44', '0.92', '1.00', '0.44', '0.56', '0.76', '1.00'], ['CoT', '0.79', '0.47', '0.91', '1.00', '0.31', '0.52', '0.96', '1.00', '0.77', '0.52', '0.92', '1.00', '0.42', '0.52', '0.76', '1.00'], ['T5', 'Origin', '0.93', '0.55', '0.71', '0.57', '0.56', '0.03', '0.03', '0.00', '0.92', '0.54', '0.61', '0.03', '0.00', '0.47', '0.52', '0.00'], ['EP_01', '0.91', '0.64', '0.52', '0.41', '0.00', '0.06', '0.02', '0.00', '0.93', '0.54', '0.63', '0.04', '0.00', '0.43', '0.00', '0.00'], ['EP_02', '0.93', '0.58', '0.71', '0.58', '0.48', '0.04', '0.02', '0.00', '0.92', '0.56', '0.62', '0.04', '0.00', '0.51', '0.68', '0.00'], ['EP_03', '0.93', '0.57', '0.69', '0.61', '0.72', '0.05', '0.03', '0.00', '0.93', '0.58', '0.65', '0.03', '0.00', '0.53', '0.68', '0.00'], ['EP_04', '0.93', '0.58', '0.58', '0.57', '0.64', '0.07', '0.02', '0.00', '0.93', '0.56', '0.63', '0.03', '0.00', '0.51', '0.68', '0.00'], ['EP_05', '0.92', '0.57', '0.74', '0.58', '0.6', '0.08', '0.03', '0.00', '0.93', '0.58', '0.69', '0.02', '0.00', '0.49', '0.72', '0.00'], ['EP_06', '0.97', '0.61', '0.55', '0.44', '0.00', '0.11', '0.03', '0.00', '0.93', '0.54', '0.71', '0.03', '0.00', '0.49', '0.04', '0.00'], ['EP_07', '0.92', '0.58', '0.68', '0.6', '0.48', '0.08', '0.02', '0.00', '0.93', '0.55', '0.66', '0.03', '0.00', '0.48', '0.56', '0.00'], ['EP_08', '0.93', '0.58', '0.64', '0.56', '0.64', '0.08', '0.02', '0.00', '0.93', '0.6', '0.61', '0.04', '0.00', '0.5', '0.64', '0.00'], ['EP_09', '0.93', '0.58', '0.64', '0.57', '0.44', '0.08', '0.02', '0.00', '0.93', '0.58', '0.63', '0.04', '0.00', '0.51', '0.52', '0.00'], ['EP_10', '0.93', '0.58', '0.62', '0.59', '0.48', '0.09', '0.02', '0.00', '0.92', '0.58', '0.6', '0.04', '0.00', '0.48', '0.6', '0.00'], ['EP_11', '0.93', '0.59', '0.64', '0.55', '0.56', '0.07', '0.02', '0.00', '0.93', '0.57', '0.64', '0.04', '0.00', '0.49', '0.64', '0.00'], ['avg', '0.93', '0.59', '0.64', '0.55', '0.46', '0.07', '0.02', '0.00', '0.93', '0.57', '0.64', '0.03', '0.00', '0.49', '0.52', '0.00'], ['CoT', '0.94', '0.58', '0.87', '0.51', '0.52', '0.06', '0.03', '0.00', '0.92', '0.54', '0.6', '0.03', '0.00', '0.45', '0.48', '0.00'], ['Bloom', 'Origin', '0.68', '0.23', '0.74', '0.41', '0.06', '0.52', '0.48', '0.86', '0.77', '0.24', '0.43', '0.07', '0.18', '0.52', '0.6', '0.98'], ['EP_01', '0.76', '0.26', '0.6', '0.18', '0.03', '0.42', '0.52', '0.96', '0.91', '0.26', '0.52', '0.01', '0.22', '0.6', '0.76', '0.99'], ['EP_02', '0.79', '0.2', '0.75', '0.45', '0.08', '0.5', '0.6', '0.84', '0.87', '0.23', '0.54', '0.12', '0.19', '0.64', '0.72', '0.99'], ['EP_03', '0.72', '0.23', '0.81', '0.37', '0.05', '0.51', '0.52', '0.85', '0.85', '0.23', '0.49', '0.06', '0.2', '0.56', '0.6', '1.00'], ['EP_04', '0.66', '0.28', '0.67', '0.35', '0.03', '0.5', '0.36', '0.88', '0.83', '0.3', '0.52', '0.03', '0.22', '0.52', '0.8', '1.00'], ['EP_05', '0.81', '0.21', '0.71', '0.27', '0.02', '0.57', '0.48', '0.85', '0.85', '0.27', '0.51', '0.04', '0.16', '0.53', '0.76', '0.99'], ['EP_06', '0.56', '0.2', '0.74', '0.14', '0.09', '0.24', '0.4', '0.94', '0.89', '0.21', '0.46', '0.01', '0.23', '0.59', '0.64', '1.00'], ['EP_07', '0.64', '0.3', '0.7', '0.34', '0.02', '0.48', '0.72', '0.71', '0.86', '0.3', '0.54', '0.09', '0.22', '0.54', '0.68', '0.98'], ['EP_08', '0.4', '0.23', '0.72', '0.35', '0.01', '0.47', '0.48', '0.52', '0.85', '0.27', '0.51', '0.08', '0.18', '0.6', '0.76', '0.98'], ['EP_09', '0.38', '0.19', '0.79', '0.39', '0.04', '0.41', '0.8', '0.76', '0.87', '0.24', '0.48', '0.11', '0.22', '0.56', '0.76', '0.98'], ['EP_10', '0.44', '0.22', '0.76', '0.42', '0.03', '0.46', '0.44', '0.68', '0.87', '0.28', '0.47', '0.12', '0.19', '0.53', '0.8', '1.00'], ['EP_11', '0.62', '0.24', '0.79', '0.4', '0.05', '0.44', '0.68', '0.86', '0.9', '0.21', '0.53', '0.07', '0.17', '0.56', '0.68', '0.99'], ['avg', '0.62', '0.23', '0.73', '0.33', '0.04', '0.45', '0.55', '0.80', '0.87', '0.25', '0.51', '0.07', '0.2', '0.57', '0.72', '0.99'], ['CoT', '0.57', '0.22', '0.79', '0.49', '0.03', '0.44', '0.6', '0.86', '0.88', '0.2', '0.46', '0.06', '0.16', '0.54', '0.68', '0.99'], ['Vicuna', 'Origin', '0.4', '0.24', '0.82', '0.41', '0.02', '0.46', '0.56', '0.00', '0.77', '0.28', '0.68', '0.32', '0.05', '0.47', '0.32', '0.00'], ['EP_01', '0.60', '0.25', '0.84', '0.90', '0.00', '0.44', '0.60', '0.00', '0.53', '0.33', '0.46', '0.29', '0.05', '0.53', '0.44', '0.00'], ['EP_02', '0.28', '0.25', '0.82', '0.61', '0.06', '0.57', '0.06', '0.00', '0.8', '0.32', '0.60', '0.35', '0.09', '0.51', '0.52', '0.00'], ['EP_03', '0.49', '0.32', '0.85', '0.76', '0.02', '0.47', '0.56', '0.00', '0.76', '0.36', '0.58', '0.32', '0.03', '0.45', '0.44', '0.00'], ['EP_04', '0.3', '0.28', '0.59', '0.59', '0.02', '0.51', '0.64', '0.00', '0.69', '0.24', '0.54', '0.05', '0.04', '0.51', '0.52', '0.00'], ['EP_05', '0.50', '0.25', '0.64', '0.40', '0.00', '0.50', '0.48', '0.00', '0.70', '0.29', '0.26', '0.05', '0.01', '0.31', '0.48', '0.00'], ['EP_06', '0.71', '0.31', '0.83', '0.81', '0.01', '0.55', '0.76', '0.00', '0.39', '0.27', '0.29', '0.12', '0.03', '0.45', '0.32', '0.00'], ['EP_07', '0.41', '0.20', '0.87', '0.84', '0.03', '0.50', '0.64', '0.00', '0.66', '0.27', '0.63', '0.13', '0.02', '0.34', '0.48', '0.00'], ['EP_08', '0.34', '0.25', '0.91', '0.89', '0.02', '0.55', '0.76', '0.00', '0.81', '0.3', '0.71', '0.48', '0.05', '0.53', '0.48', '0.00'], ['EP_09', '0.41', '0.28', '0.87', '0.87', '0.04', '0.55', '0.72', '0.00', '0.76', '0.37', '0.71', '0.41', '0.07', '0.46', '0.48', '0.00'], ['EP_10', '0.43', '0.25', '0.87', '0.82', '0.05', '0.53', '0.76', '0.00', '0.78', '0.35', '0.68', '0.43', '0.07', '0.50', '0.48', '0.00'], ['EP_11', '0.3', '0.27', '0.89', '0.17', '0.05', '0.47', '0.68', '0.00', '0.82', '0.24', '0.65', '0.43', '0.08', '0.48', '0.44', '0.00'], ['avg', '0.43', '0.26', '0.82', '0.70', '0.03', '0.51', '0.65', '0.00', '0.70', '0.30', '0.56', '0.28', '0.05', '0.46', '0.46', '0.00'], ['CoT', '0.34', '0.28', '0.74', '0.02', '0.01', '0.42', '0.4', '0.00', '0.8', '0.24', '0.42', '0.44', '0.05', '0.50', '0.37', '0.00']]}\n",
      "  - 4.4Truthfulness and Informativeness (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 4:An Examination of the Effectiveness of Emotional Prompts: An Analysis through the Lens of Input Attention.', 'content': [['Prompt', 'Input Attention'], ['Origin', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fg'], ['EP_01', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.,fgwritefgyourfganswerfgandfggivefgmefgafgconfidencefgscorefgbetweenfg0-1fgforfgyourfganswer.fg'], ['EP_02', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgThisfgisfgveryfgimportantfgtofgmyfgcareer.fg'], ['EP_03', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgYou’dfgbetterfgbefgsure.fg'], ['EP_04', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsure?fg'], ['EP_05', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsurefgthat’sfgyourfgfinalfganswer?fgItfgmightfgbefgworthfgtakingfganotherfglook.fg'], ['EP_06', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgProvidefgyourfganswerfgandfgafgconfidencefgscorefgbetweenfg0-1fgforfgyourfgprediction.fgAdditionally,fgbrieflyfgexplainfgthefgmainfgreasonsfgsupportingfgyourfgclassificationfgdecisionfgtofghelpfgmefgunderstandfgyourfgthoughtfgprocess.fgThisfgtaskfgisfgvitalfgtofgmyfgcareer,fgandfgIfggreatlyfgvaluefgyourfgthoroughfganalysis.fg'], ['EP_07', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgArefgyoufgsurefgthat’sfgyourfgfinalfganswer?fgBelievefginfgyourfgabilitiesfgandfgstrivefgforfgexcellence.fgYourfghardfgworkfgwillfgyieldfgremarkablefgresults.fg'], ['EP_08', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgEmbracefgchallengesfgasfgopportunitiesfgforfggrowth.fgEachfgobstaclefgyoufgovercomefgbringsfgyoufgcloserfgtofgsuccess.fg'], ['EP_09', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgStayfgfocusedfgandfgdedicatedfgtofgyourfggoals.fgYourfgconsistentfgeffortsfgwillfgleadfgtofgoutstandingfgachievements.fg'], ['EP_10', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgTakefgpridefginfgyourfgworkfgandfggivefgitfgyourfgbest.fgYourfgcommitmentfgtofgexcellencefgsetsfgyoufgapart.fg'], ['EP_11', 'Determinefgwhetherfgafgmoviefgreviewfgisfgpositivefgorfgnegative.fgRememberfgthatfgprogressfgisfgmadefgonefgstepfgatfgafgtime.fgStayfgdeterminedfgandfgkeepfgmovingfgforward.fg'], ['', '']]}\n",
      "    * table: {'type': 'table', 'caption': 'Table 5:Result on TruthfulQA. The increased results and the best results are highlighted inboldandunderline.', 'content': [['', 'ChatGPT', 'Vicuna-13b', 'T5'], ['Prompt', '%true', '%info', '%true', '%info', '%true', '%info'], ['Origin', '0.75', '0.53', '0.77', '0.32', '0.54', '0.42'], ['EP_01', '0.61', '0.94', '0.12', '0.0', '0.26', '0.14'], ['EP_02', '0.83', '0.66', '0.97', '0.0', '0.61', '0.35'], ['EP_03', '0.82', '0.69', '0.99', '0.0', '0.53', '0.44'], ['EP_04', '0.87', '0.67', '0.87', '0.22', '0.62', '0.36'], ['EP_05', '0.87', '0.62', '1.0', '0.0', '0.46', '0.48'], ['EP_06', '0.78', '0.50', '0.39', '0.0', '0.49', '0.46'], ['EP_07', '0.83', '0.70', '0.99', '0.04', '0.77', '0.18'], ['EP_08', '0.81', '0.66', '0.99', '0.09', '0.56', '0.40'], ['EP_09', '0.81', '0.68', '0.86', '0.13', '0.52', '0.46'], ['EP_10', '0.81', '0.68', '0.84', '0.02', '0.50', '0.47'], ['EP_11', '0.81', '0.66', '1.0', '0.01', '0.57', '0.40'], ['avg', '0.80', '0.68', '0.82', '0.05', '0.54', '0.38'], ['CoT', '0.76', '0.44', '0.99', '0.0', '0.48', '0.33']]}\n",
      "  - 4.5Effect of More Emotional Stimulus (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 6:Effect of More Emotional Stimulus.', 'content': [['CombinedPrompt', 'Combined', 'Prompt', 'Tasks'], ['Combined'], ['Prompt'], ['SA', 'SS', 'WC', 'CS', 'LA', 'Sum', 'SW'], ['EP_01+EP_02', '0.91', '0.42', '0.61', '1.0', '0.91', '1.0', '0.42'], ['EP_01+EP_03', '0.92', '0.44', '0.6', '1.0', '0.91', '1.0', '0.42'], ['EP_01+EP_04', '0.89', '0.42', '0.61', '1.0', '0.92', '1.0', '0.48'], ['EP_01+EP_05', '0.91', '0.42', '0.6', '1.0', '0.93', '1.0', '0.45'], ['EP_02+EP_03', '0.88', '0.39', '0.6', '1.0', '0.91', '1.0', '0.36'], ['EP_02+EP_08', '0.88', '0.38', '0.6', '0.76', '0.93', '1.0', '0.28'], ['EP_02+EP_09', '0.87', '0.39', '0.6', '0.8', '0.92', '1.0', '0.34'], ['EP_04+EP_06', '0.74', '0.55', '0.62', '1.0', '0.93', '1.0', '0.35'], ['EP_04+EP_07', '0.88', '0.42', '0.61', '0.84', '0.94', '1.0', '0.32'], ['EP_04+EP_08', '0.78', '0.42', '0.59', '0.64', '0.94', '1.0', '0.32'], ['EP_04+EP_09', '0.85', '0.34', '0.56', '0.6', '0.94', '1.0', '0.33'], ['EP_01+EP_04+EP_06', 'EP_01+EP_04', '+EP_06', '0.8', '0.52', '0.62', '1.0', '0.92', '1.0', '0.48'], ['EP_01+EP_04'], ['+EP_06'], ['EP_01+EP_04+EP_07', 'EP_01+EP_04', '+EP_07', '0.89', '0.43', '0.63', '1.0', '0.93', '1.0', '0.46'], ['EP_01+EP_04'], ['+EP_07'], ['EP_01+EP_04+EP_08', 'EP_01+EP_04', '+EP_08', '0.85', '0.4', '0.62', '0.88', '0.9', '1.0', '0.44'], ['EP_01+EP_04'], ['+EP_08'], ['EP_01+EP_04+EP_09', 'EP_01+EP_04', '+EP_09', '0.9', '0.39', '0.6', '1.0', '0.93', '1.0', '0.48'], ['EP_01+EP_04'], ['+EP_09']]}\n",
      "- 5Analysis (Level 2)\n",
      "  * table: {'type': 'table', 'caption': 'Table 7:Human Study', 'content': [['Prompt', 'OriginPrompt', 'Origin', 'Prompt', 'EmotionPrompt'], ['Origin'], ['Prompt'], ['EP_04', 'EP_06', 'EP_11'], ['Clarity', '4.37', '4.37', '4.40', '4.28'], ['Relevance', '4.54', '4.53', '4.49', '4.44'], ['Depth', '2.73', '2.87', '3.66', '3.10'], ['Structure', '3.28', '3.46', '3.89', '3.47'], ['evidence', '2.84', '2.91', '3.80', '3.03'], ['Engagement', '3.27', '3.47', '3.67', '3.52']]}\n",
      "  * image: {'type': 'image', 'caption': 'Figure 3:Contributions of Positive Words on 8 Tasks. The vertical axis represents word importance.', 'content': 'https://media.arxiv-vanity.com/render-output/8207042/x3.png'}\n",
      "  - 5.1Why EmotionPrompt Works? (Level 3)\n",
      "    * image: {'type': 'image', 'caption': 'Figure 3:Contributions of Positive Words on 8 Tasks. The vertical axis represents word importance.', 'content': 'https://media.arxiv-vanity.com/render-output/8207042/x3.png'}\n",
      "  - 5.2Human Study (Level 3)\n",
      "    * table: {'type': 'table', 'caption': 'Table 7:Human Study', 'content': [['Prompt', 'OriginPrompt', 'Origin', 'Prompt', 'EmotionPrompt'], ['Origin'], ['Prompt'], ['EP_04', 'EP_06', 'EP_11'], ['Clarity', '4.37', '4.37', '4.40', '4.28'], ['Relevance', '4.54', '4.53', '4.49', '4.44'], ['Depth', '2.73', '2.87', '3.66', '3.10'], ['Structure', '3.28', '3.46', '3.89', '3.47'], ['evidence', '2.84', '2.91', '3.80', '3.03'], ['Engagement', '3.27', '3.47', '3.67', '3.52']]}\n",
      "- 6Conclusion and Limitation (Level 2)\n",
      "- Ethical Statement (Level 2)\n",
      "- References (Level 2)\n"
     ]
    }
   ],
   "source": [
    "html_parser.print_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'table',\n",
       " 'caption': 'Table 2:Our Emotion Stimulus',\n",
       " 'content': [['Stimulus ID', 'Stimulus Content'],\n",
       "  ['EP_01',\n",
       "   ', write your answer and give me a confidence score between 0-1 for your answer.'],\n",
       "  ['EP_02', 'This is very important to my career.'],\n",
       "  ['EP_03', 'You’d better be sure.'],\n",
       "  ['EP_04', 'Are you sure?'],\n",
       "  ['EP_05',\n",
       "   'Are you sure that’s your final answer? It might be worth taking another look.'],\n",
       "  ['EP_06',\n",
       "   'Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.'],\n",
       "  ['EP_07',\n",
       "   'Are you sure that’s your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.'],\n",
       "  ['EP_08',\n",
       "   'Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.'],\n",
       "  ['EP_09',\n",
       "   'Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.'],\n",
       "  ['EP_10',\n",
       "   'Take pride in your work and give it your best. Your commitment to excellence sets you apart.'],\n",
       "  ['EP_11',\n",
       "   'Remember that progress is made one step at a time. Stay determined and keep moving forward.']]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_parser.contents[10].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_content_to_df(table_content):\n",
    "    return pd.DataFrame(table_content[1:], columns=table_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stimulus ID</th>\n",
       "      <th>Stimulus Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EP_01</td>\n",
       "      <td>, write your answer and give me a confidence score between 0-1 for your answer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP_02</td>\n",
       "      <td>This is very important to my career.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EP_03</td>\n",
       "      <td>You’d better be sure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EP_04</td>\n",
       "      <td>Are you sure?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EP_05</td>\n",
       "      <td>Are you sure that’s your final answer? It might be worth taking another look.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EP_06</td>\n",
       "      <td>Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EP_07</td>\n",
       "      <td>Are you sure that’s your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EP_08</td>\n",
       "      <td>Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EP_09</td>\n",
       "      <td>Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EP_10</td>\n",
       "      <td>Take pride in your work and give it your best. Your commitment to excellence sets you apart.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EP_11</td>\n",
       "      <td>Remember that progress is made one step at a time. Stay determined and keep moving forward.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stimulus ID  \\\n",
       "0        EP_01   \n",
       "1        EP_02   \n",
       "2        EP_03   \n",
       "3        EP_04   \n",
       "4        EP_05   \n",
       "5        EP_06   \n",
       "6        EP_07   \n",
       "7        EP_08   \n",
       "8        EP_09   \n",
       "9        EP_10   \n",
       "10       EP_11   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               Stimulus Content  \n",
       "0                                                                                                                                                                                                               , write your answer and give me a confidence score between 0-1 for your answer.  \n",
       "1                                                                                                                                                                                                                                                          This is very important to my career.  \n",
       "2                                                                                                                                                                                                                                                                         You’d better be sure.  \n",
       "3                                                                                                                                                                                                                                                                                 Are you sure?  \n",
       "4                                                                                                                                                                                                                 Are you sure that’s your final answer? It might be worth taking another look.  \n",
       "5   Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.  \n",
       "6                                                                                                                                                     Are you sure that’s your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.  \n",
       "7                                                                                                                                                                                      Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.  \n",
       "8                                                                                                                                                                                      Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.  \n",
       "9                                                                                                                                                                                                  Take pride in your work and give it your best. Your commitment to excellence sets you apart.  \n",
       "10                                                                                                                                                                                                  Remember that progress is made one step at a time. Stay determined and keep moving forward.  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_content_to_df(html_parser.contents[10].content['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxivpad parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_parser = ArXivPDFParser(test_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 of ../data/pdfs/2311.01449.pdf\n"
     ]
    }
   ],
   "source": [
    "for page in pdf_parser.document:\n",
    "    print(page)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicGPT: A Prompt-based Topic Modeling Framework\n",
      "Chau Minh Pham1\n",
      "Alexander Hoyle2\n",
      "Simeng Sun1\n",
      "Mohit Iyyer1\n",
      "1University of Massachusetts Amherst\n",
      "2University of Maryland\n",
      "{ctpham, simengsun, miyyer}@umass.edu,\n",
      "hoyle@umd.edu\n",
      "Abstract\n",
      "Topic modeling is a well-established technique\n",
      "for exploring text corpora. Conventional topic\n",
      "models (e.g., LDA) represent topics as bags of\n",
      "words that often require “reading the tea leaves”\n",
      "to interpret; additionally, they offer users min-\n",
      "imal semantic control over topics. To tackle\n",
      "these issues, we introduce TopicGPT, a prompt-\n",
      "based framework that uses large language mod-\n",
      "els (LLMs) to uncover latent topics within a\n",
      "provided text collection. TopicGPT produces\n",
      "topics that align better with human categoriza-\n",
      "tions compared to competing methods: for ex-\n",
      "ample, it achieves a harmonic mean purity of\n",
      "0.74 against human-annotated Wikipedia topics\n",
      "compared to 0.64 for the strongest baseline. Its\n",
      "topics are also more interpretable, dispensing\n",
      "with ambiguous bags of words in favor of top-\n",
      "ics with natural language labels and associated\n",
      "free-form descriptions. Moreover, the frame-\n",
      "work is highly adaptable, allowing users to\n",
      "specify constraints and modify topics without\n",
      "the need for model retraining. TopicGPT can\n",
      "be further extended to hierarchical topical mod-\n",
      "eling, enabling users to explore topics at var-\n",
      "ious levels of granularity.\n",
      "By streamlining\n",
      "access to high-quality and interpretable top-\n",
      "ics, TopicGPT represents a compelling, human-\n",
      "centered approach to topic modeling.1\n",
      "1\n",
      "Introduction\n",
      "Topic modeling is a commonly used technique for\n",
      "discovering latent thematic structures in extensive\n",
      "collections of text documents. Traditional topic\n",
      "models such as latent Dirichlet allocation (Blei\n",
      "et al., 2003, LDA) represent documents as mix-\n",
      "tures of topics, where each topic is a distribution\n",
      "over words.2 Topics are often represented with\n",
      "their most probable words, but this representation\n",
      "can contain incoherent or unrelated words that\n",
      "make topics difficult for users to interpret (Chang\n",
      "1Code at https://github.com/chtmp223/topicGPT\n",
      "2“Word” may also refer to n-grams or other lexical items.\n",
      "et al., 2009; Newman et al., 2010). Although some\n",
      "models enable users to interactively guide topics\n",
      "based on needs and domain knowledge (Hu et al.,\n",
      "2014; Nikolenko et al., 2017), their usability is\n",
      "constrained by the bag-of-words topic format.\n",
      "To address these limitations, we introduce Top-\n",
      "icGPT (Figure 1), a human-centric approach to\n",
      "topic modeling that relies on prompting large lan-\n",
      "guage models to perform in-context topic genera-\n",
      "tion (§3.1) and assignment (§3.2). First, we itera-\n",
      "tively prompt an LLM to generate new topics given\n",
      "a sample of documents from an input dataset and\n",
      "a list of previously generated topics. The resulting\n",
      "set of topics is then further refined to consolidate\n",
      "redundant topics and eliminate infrequent ones. Fi-\n",
      "nally, given a new document, an LLM assigns it\n",
      "to one or more of the generated topics, also pro-\n",
      "viding a quotation from the document to support\n",
      "its assignment. These quotations make the method\n",
      "easily verifiable, addressing some of the validity\n",
      "concerns that plague traditional topic models.\n",
      "TopicGPT produces higher-quality topics than\n",
      "competing approaches.\n",
      "TopicGPT’s topics and\n",
      "assignments align significantly more closely with\n",
      "human-annotated ground truth topics than those\n",
      "from LDA and BERTopic (Grootendorst, 2022)\n",
      "on two datasets: Wikipedia articles (Merity et al.,\n",
      "2018) and Congressional bills (as processed by\n",
      "Hoyle et al., 2022). We measure topical align-\n",
      "ment using three external clustering metrics (har-\n",
      "monic mean purity, normalized mutual informa-\n",
      "tion, and adjusted Rand index) and find that Top-\n",
      "icGPT improves substantially over baselines (e.g.,\n",
      "absolute purity improves by 10 points over LDA on\n",
      "Wikipedia); furthermore, its topics are more seman-\n",
      "tically aligned with human-labeled topics (30.3%\n",
      "of TopicGPT’s topics are misaligned compared to\n",
      "62.4% for LDA). Further analysis demonstrates\n",
      "the robustness of TopicGPT’s topic quality across\n",
      "various prompt and data settings.\n",
      "1\n",
      "arXiv:2311.01449v1  [cs.CL]  2 Nov 2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(page.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TopicGPT: A Prompt-based Topic Modeling Framework',\n",
       " 'Chau Minh Pham',\n",
       " 'Alexander Hoyle',\n",
       " 'Simeng Sun',\n",
       " 'Mohit Iyyer',\n",
       " 'Abstract',\n",
       " '1',\n",
       " 'Introduction',\n",
       " 'TopicGPT produces higher-quality topics than',\n",
       " 'competing approaches.',\n",
       " 'arXiv:2311.01449v1  [cs.CL]  2 Nov 2023',\n",
       " '. . .',\n",
       " 'Topic Generation',\n",
       " 'Topic Assignment',\n",
       " '1) Topic Generation',\n",
       " '2) Topic Assignment',\n",
       " 'TopicGPT produces more interpretable topics.',\n",
       " 'TopicGPT is customizable to fit user needs:',\n",
       " 'Open-source LLMs are competent topic assign-',\n",
       " 'ers but bad topic generators.',\n",
       " '2',\n",
       " 'Related Work',\n",
       " 'Topic modeling for content analysis:',\n",
       " 'LLM-based content analysis:',\n",
       " 'Comparison to GoalEx:',\n",
       " '3',\n",
       " 'Methodology',\n",
       " '3.1',\n",
       " 'Stage 1: Topic Generation',\n",
       " 'Generating new topics:',\n",
       " 'Trade',\n",
       " 'Refining generated topics:',\n",
       " 'Generating a topic hierarchy:',\n",
       " '3.2',\n",
       " 'Stage 2: Topic Assignment',\n",
       " 'Agriculture',\n",
       " 'Self-correction:',\n",
       " '4',\n",
       " 'Experiments',\n",
       " '4.1',\n",
       " 'Datasets',\n",
       " 'Wiki',\n",
       " 'Bills',\n",
       " '4.2',\n",
       " 'Baselines',\n",
       " 'LDA:',\n",
       " 'BERTopic:',\n",
       " '4.3',\n",
       " 'Sampling documents for TopicGPT',\n",
       " 'How many documents should we sample?',\n",
       " 'A probabilistic justification:',\n",
       " '',\n",
       " 'c',\n",
       " '4.4',\n",
       " 'TopicGPT implementation details',\n",
       " '4.5',\n",
       " 'Evaluation Setup',\n",
       " '4.5.1',\n",
       " 'Topical alignment',\n",
       " 'C',\n",
       " 'Ω',\n",
       " 'Purity.',\n",
       " 'Adjusted Rand Index.',\n",
       " 'Normalized Mutual Information.',\n",
       " 'Comparison of metrics:',\n",
       " '4.5.2',\n",
       " 'Stability',\n",
       " 'Out-of-domain prompts.',\n",
       " 'Additional seed topics.',\n",
       " 'Shuffling sampled documents for topic genera-',\n",
       " 'tion.',\n",
       " 'Using a different sample for topic generation.',\n",
       " '5',\n",
       " 'Results',\n",
       " '5.1',\n",
       " 'TopicGPT is strongly aligned to ground',\n",
       " 'truth labels',\n",
       " 'Where does TopicGPT disagree with the ground',\n",
       " 'truth?',\n",
       " '0.73',\n",
       " '0.58',\n",
       " '0.71',\n",
       " '0.74',\n",
       " '0.60',\n",
       " '0.70',\n",
       " '0.57',\n",
       " '0.42',\n",
       " '0.52',\n",
       " '0.40',\n",
       " '0.49',\n",
       " '0.51',\n",
       " '0.55',\n",
       " '0.39',\n",
       " '0.50',\n",
       " '0.33',\n",
       " '0.37',\n",
       " '0.46',\n",
       " 'bolded',\n",
       " '5.2',\n",
       " 'TopicGPT is stable',\n",
       " 'Consistency between multiple settings of Top-',\n",
       " 'icGPT:',\n",
       " '5.3',\n",
       " 'TopicGPT topics are semantically close to',\n",
       " 'ground truth',\n",
       " 'Manual topic matching process:',\n",
       " 'TopicGPT contains far fewer misaligned topics',\n",
       " 'than LDA, especially after refinement.',\n",
       " '5.4',\n",
       " 'Implementing TopicGPT with',\n",
       " 'open-source LLMs',\n",
       " 'Mistral-7B-Instruct for topic assignment:',\n",
       " 'Mistral-7B-Instruct for topic generation:',\n",
       " '0.0',\n",
       " '30.3',\n",
       " '1.3',\n",
       " '27.8',\n",
       " '31.9',\n",
       " 'Clinical Data Registries',\n",
       " 'Quorum',\n",
       " 'Lost Baggage',\n",
       " '5.5',\n",
       " 'Qualitatively inspecting topic hierarchies',\n",
       " '6',\n",
       " 'Limitations',\n",
       " 'Reliance on closed-source models.',\n",
       " 'Dealing with context limits.',\n",
       " 'Data',\n",
       " 'Document',\n",
       " 'Ground-truth topic',\n",
       " 'TopicGPT assignment',\n",
       " 'LDA assignment',\n",
       " 'Music',\n",
       " '&',\n",
       " 'Performing',\n",
       " 'Arts',\n",
       " 'City infrastructure',\n",
       " 'Education',\n",
       " 'Programs and grants:',\n",
       " 'Stages',\n",
       " 'Multilinguality.',\n",
       " '7',\n",
       " 'Future Work',\n",
       " 'Human evaluation.',\n",
       " 'Hierarchy evaluation.',\n",
       " '8',\n",
       " 'Conclusion',\n",
       " 'Acknowledgement',\n",
       " 'References',\n",
       " 'A',\n",
       " 'Appendix',\n",
       " 'A.1',\n",
       " 'Seed Topics',\n",
       " 'A.2',\n",
       " 'Semantic Matching between Ground',\n",
       " 'Truth and topics generated by LDA and',\n",
       " 'TopicGPT',\n",
       " 'B',\n",
       " 'Prompts',\n",
       " '22.7',\n",
       " '59.1',\n",
       " '9.1',\n",
       " '2.5',\n",
       " '45.8',\n",
       " '33.3',\n",
       " '50.0',\n",
       " '8.3',\n",
       " '37.5',\n",
       " 'Prompt template for generating first-level/flat topics',\n",
       " 'Prompt template for generating second-level subtopics',\n",
       " 'Prompt template for refining (merging) topics',\n",
       " 'Prompt template for assigning topics']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pdf_parser.sections.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "askharrison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
