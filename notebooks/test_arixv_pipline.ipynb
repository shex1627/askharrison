{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.arxiv_search import expand_arxiv_query, run_multi_arixv_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"\"\"RAG(retrieval augmented generation) sometimes the sources retrieved are relevant but not enough to answer the question user ask. How do research approach this case\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"\"\"reduce hallucination in RAG(retrieval augmented generation), make queries tailored for arxiv search, and then use the retrieved papers to answer the question\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = expand_arxiv_query(problem_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reducing hallucination in retrieval augmented generation models',\n",
       " 'Techniques for minimizing hallucination in RAG models',\n",
       " 'Enhancing the effectiveness of Retrieval Augmented Generation',\n",
       " 'Studies on improving the accuracy of Retrieval Augmented Generation',\n",
       " 'Methods to reduce artifact generation in RAG models',\n",
       " 'Optimization techniques in retrieval augmented generation models',\n",
       " 'Novel methods for reducing hallucination in RAG',\n",
       " 'Approaches for improving reliability in retrieval augmented generation models',\n",
       " 'Research on limiting hallucination in RAG',\n",
       " 'Advanced strategies for retrieval augmented generation optimization']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "arxiv_query_results = run_multi_arixv_queries(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_query_results.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten arxiv_query_results\n",
    "all_results = []\n",
    "for query in arxiv_query_results:\n",
    "    for result in arxiv_query_results[query]:\n",
    "        all_results.append(result)\n",
    "\n",
    "# make arxiv query results a dataframe and create a new dataframe with only unique entry_id\n",
    "import pandas as pd\n",
    "\n",
    "arixv_result_df = pd.DataFrame(all_results)\n",
    "unique_arixv_result_df = arixv_result_df.drop_duplicates(subset='entry_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 13), (159, 13))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.shape, unique_arixv_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>links</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.18251v1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.22353v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.15391v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.17783v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.05856v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.12248v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2406.19150v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2408.15533v2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.11414v2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2411.12759v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   authors  categories  comment  doi  \\\n",
       "entry_id                                                               \n",
       "http://arxiv.org/abs/2410.18251v1        4           4        4    0   \n",
       "http://arxiv.org/abs/2410.22353v1        3           3        0    0   \n",
       "http://arxiv.org/abs/2401.15391v1        3           3        3    0   \n",
       "http://arxiv.org/abs/2410.17783v1        3           3        3    0   \n",
       "http://arxiv.org/abs/2401.05856v1        3           3        0    0   \n",
       "http://arxiv.org/abs/2410.12248v1        3           3        0    0   \n",
       "http://arxiv.org/abs/2406.19150v1        3           3        0    0   \n",
       "http://arxiv.org/abs/2408.15533v2        3           3        0    0   \n",
       "http://arxiv.org/abs/2410.11414v2        3           3        3    0   \n",
       "http://arxiv.org/abs/2411.12759v1        3           3        0    0   \n",
       "\n",
       "                                   journal_ref  links  pdf_url  \\\n",
       "entry_id                                                         \n",
       "http://arxiv.org/abs/2410.18251v1            0      4        4   \n",
       "http://arxiv.org/abs/2410.22353v1            0      3        3   \n",
       "http://arxiv.org/abs/2401.15391v1            0      3        3   \n",
       "http://arxiv.org/abs/2410.17783v1            0      3        3   \n",
       "http://arxiv.org/abs/2401.05856v1            0      3        3   \n",
       "http://arxiv.org/abs/2410.12248v1            0      3        3   \n",
       "http://arxiv.org/abs/2406.19150v1            0      3        3   \n",
       "http://arxiv.org/abs/2408.15533v2            0      3        3   \n",
       "http://arxiv.org/abs/2410.11414v2            0      3        3   \n",
       "http://arxiv.org/abs/2411.12759v1            0      3        3   \n",
       "\n",
       "                                   primary_category  published  summary  \\\n",
       "entry_id                                                                  \n",
       "http://arxiv.org/abs/2410.18251v1                 4          4        4   \n",
       "http://arxiv.org/abs/2410.22353v1                 3          3        3   \n",
       "http://arxiv.org/abs/2401.15391v1                 3          3        3   \n",
       "http://arxiv.org/abs/2410.17783v1                 3          3        3   \n",
       "http://arxiv.org/abs/2401.05856v1                 3          3        3   \n",
       "http://arxiv.org/abs/2410.12248v1                 3          3        3   \n",
       "http://arxiv.org/abs/2406.19150v1                 3          3        3   \n",
       "http://arxiv.org/abs/2408.15533v2                 3          3        3   \n",
       "http://arxiv.org/abs/2410.11414v2                 3          3        3   \n",
       "http://arxiv.org/abs/2411.12759v1                 3          3        3   \n",
       "\n",
       "                                   title  updated  \n",
       "entry_id                                           \n",
       "http://arxiv.org/abs/2410.18251v1      4        4  \n",
       "http://arxiv.org/abs/2410.22353v1      3        3  \n",
       "http://arxiv.org/abs/2401.15391v1      3        3  \n",
       "http://arxiv.org/abs/2410.17783v1      3        3  \n",
       "http://arxiv.org/abs/2401.05856v1      3        3  \n",
       "http://arxiv.org/abs/2410.12248v1      3        3  \n",
       "http://arxiv.org/abs/2406.19150v1      3        3  \n",
       "http://arxiv.org/abs/2408.15533v2      3        3  \n",
       "http://arxiv.org/abs/2410.11414v2      3        3  \n",
       "http://arxiv.org/abs/2411.12759v1      3        3  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.groupby('entry_id').count().sort_values('title', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.prompts.content_curation import create_arxiv_filtering_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_arxiv_filtering_prompt in module askharrison.prompts.content_curation:\n",
      "\n",
      "create_arxiv_filtering_prompt(problem_statement: str, doc_abstract: str)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(create_arxiv_filtering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt for each arxiv entry\n",
    "arxiv_reranking_prompts = [create_arxiv_filtering_prompt(problem_statement, \n",
    "                                         record['title']+\"\\n\"+record['summary']) for record in unique_arixv_result_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.llm_models import parallel_llm_processor, process_question, safe_eval, extract_python_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 159/159 [01:11<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "reranking_llm_response = parallel_llm_processor(arxiv_reranking_prompts, llm_function=process_question, \n",
    "                                                max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159, 13), 159)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_responses_results = [safe_eval(extract_python_code(response)) for response in reranking_llm_response]\n",
    "# filter out empty responses\n",
    "\n",
    "unique_arixv_result_df.shape, len(llm_responses_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
     ]
    }
   ],
   "source": [
    "# extract reasoning, is_direct, 'is_relevant' from llm_responses_results if it is not empty, and add to unique_arixv_result_df\n",
    "unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase max column width in pandas\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arixv_result_df.query('is_direct == True').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Context-Augmented Code Generation Using Programming Knowledge Graphs</td>\n",
       "      <td>http://arxiv.org/abs/2410.18251v1</td>\n",
       "      <td>Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\\nimproved code generation, but, they frequently face difficulties when dealing\\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\\naddresses this issue by retrieving and integrating external knowledge at the\\ninference time. However, retrieval models often fail to find most relevant\\ncontext, and generation models, with limited context capacity, can hallucinate\\nwhen given irrelevant data. We present a novel framework that leverages a\\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\\nThis approach enables fine-grained code retrieval by focusing on the most\\nrelevant segments while reducing irrelevant context through a tree-pruning\\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\\nhallucinations by selectively integrating non-RAG solutions. We propose two\\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\\ntree pruning to enhance retrieval precision, a re-ranking method for robust\\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\\ncode augmentation with relevant comments and docstrings.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2410.15116v1</td>\n",
       "      <td>Generation of plausible but incorrect factual information, often termed\\nhallucination, has attracted significant research interest. Retrieval-augmented\\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\\nemerges as a promising method to reduce hallucination. However, existing RALMs\\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\\naddress this challenge, we propose COFT, a novel\\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\\nknowledge graph to extract potential key entities in a given context. Second,\\n\\textit{scorer} measures the importance of each entity by calculating its\\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\\nentities with a dynamic threshold algorithm and highlights the corresponding\\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\\nexperiments on the knowledge hallucination benchmark demonstrate the\\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\\nscore metric. Moreover, COFT also exhibits remarkable versatility across\\nvarious long-form tasks, such as reading comprehension and question answering.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Effects of Hallucinations in Synthetic Training Data for Relation Extraction</td>\n",
       "      <td>http://arxiv.org/abs/2410.08393v1</td>\n",
       "      <td>Relation extraction is crucial for constructing knowledge graphs, with large\\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\\nevaluating models. Generative data augmentation (GDA) is a common approach to\\nexpand such datasets. However, this approach often introduces hallucinations,\\nsuch as spurious facts, whose impact on relation extraction remains\\nunderexplored. In this paper, we examine the effects of hallucinations on the\\nperformance of relation extraction on the document and sentence levels. Our\\nempirical study reveals that hallucinations considerably compromise the ability\\nof models to extract relations from text, with recall reductions between 19.1%\\nand 39.2%. We identify that relevant hallucinations impair the model's\\nperformance, while irrelevant hallucinations have a minimal impact.\\nAdditionally, we develop methods for the detection of hallucinations to improve\\ndata quality and model performance. Our approaches successfully classify texts\\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\\n92.2%. These methods not only assist in removing hallucinations but also help\\nin estimating their prevalence within datasets, which is crucial for selecting\\nhigh-quality data. Overall, our work confirms the profound impact of relevant\\nhallucinations on the effectiveness of relation extraction models.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2407.18044v1</td>\n",
       "      <td>Digital health chatbots powered by Large Language Models (LLMs) have the\\npotential to significantly improve personal health management for chronic\\nconditions by providing accessible and on-demand health coaching and\\nquestion-answering. However, these chatbots risk providing unverified and\\ninaccurate information because LLMs generate responses based on patterns\\nlearned from diverse internet data. Retrieval Augmented Generation (RAG) can\\nhelp mitigate hallucinations and inaccuracies in LLM responses by grounding it\\non reliable content. However, efficiently and accurately retrieving most\\nrelevant set of content for real-time user questions remains a challenge. In\\nthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a\\nnovel approach that pre-computes a database of potential queries from a content\\nbase using LLMs. For an incoming patient question, QB-RAG efficiently matches\\nit against this pre-generated query database using vector search, improving\\nalignment between user questions and the content. We establish a theoretical\\nfoundation for QB-RAG and provide a comparative analysis of existing retrieval\\nenhancement techniques for RAG systems. Finally, our empirical evaluation\\ndemonstrates that QB-RAG significantly improves the accuracy of healthcare\\nquestion answering, paving the way for robust and trustworthy LLM applications\\nin digital health.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots</td>\n",
       "      <td>http://arxiv.org/abs/2403.01193v3</td>\n",
       "      <td>Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\\nof artificial intelligence. However, their tendency to hallucinate -- generate\\nplausible but false information -- poses a significant challenge. This issue is\\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\\nGeneration (RAG) can counter hallucinations by integrating external knowledge\\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\\nin some cases, but can still be misled when prompts directly contradict the\\nmodel's pre-trained understanding. These findings highlight the complex nature\\nof hallucinations and the need for more robust solutions to ensure LLM\\nreliability in real-world applications. We offer practical recommendations for\\nRAG deployment and discuss implications for the development of more trustworthy\\nLLMs.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources</td>\n",
       "      <td>http://arxiv.org/abs/2406.07136v1</td>\n",
       "      <td>Query expansion has been employed for a long time to improve the accuracy of\\nquery retrievers. Earlier works relied on pseudo-relevance feedback (PRF)\\ntechniques, which augment a query with terms extracted from documents retrieved\\nin a first stage. However, the documents may be noisy hindering the\\neffectiveness of the ranking. To avoid this, recent studies have instead used\\nLarge Language Models (LLMs) to generate additional content to expand a query.\\nThese techniques are prone to hallucination and also focus on the LLM usage\\ncost. However, the cost may be dominated by the retrieval in several important\\npractical scenarios, where the corpus is only available via APIs which charge a\\nfee per retrieved document. We propose combining classic PRF techniques with\\nLLMs and create a progressive query expansion algorithm ProQE that iteratively\\nexpands the query as it retrieves more documents. ProQE is compatible with both\\nsparse and dense retrieval systems. Our experimental results on four retrieval\\ndatasets show that ProQE outperforms state-of-the-art baselines by 37% and is\\nthe most cost-effective.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Seven Failure Points When Engineering a Retrieval Augmented Generation System</td>\n",
       "      <td>http://arxiv.org/abs/2401.05856v1</td>\n",
       "      <td>Software engineers are increasingly adding semantic search capabilities to\\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\\nRAG system involves finding documents that semantically match a query and then\\npassing the documents to a large language model (LLM) such as ChatGPT to\\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\\nproblem of hallucinated responses from LLMs, b) link sources/references to\\ngenerated responses, and c) remove the need for annotating documents with\\nmeta-data. However, RAG systems suffer from limitations inherent to information\\nretrieval systems and from reliance on LLMs. In this paper, we present an\\nexperience report on the failure points of RAG systems from three case studies\\nfrom separate domains: research, education, and biomedical. We share the\\nlessons learned and present 7 failure points to consider when designing a RAG\\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\\nsystem is only feasible during operation, and 2) the robustness of a RAG system\\nevolves rather than designed in at the start. We conclude with a list of\\npotential research directions on RAG systems for the software engineering\\ncommunity.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2411.07820v2</td>\n",
       "      <td>We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel\\napproach designed to bridge the pre-retrieval information gap in\\nRetrieval-Augmented Generation (RAG) systems through query optimization\\ntailored to meet the specific knowledge requirements of Large Language Models\\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\\nframework begins by extracting parametric knowledge from LLMs, followed by\\nusing a specialized query optimizer for refining these queries. This process\\nensures the retrieval of only the most pertinent information essential for\\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\\ncomputational costs, we propose a trainable scheme for our pipeline that\\nutilizes a smaller, tunable model as the query optimizer, which is refined\\nthrough knowledge distillation from a larger teacher model. Our evaluations on\\nvarious question-answering (QA) datasets and with different retrieval systems\\nshow that ERRR consistently outperforms existing baselines, proving to be a\\nversatile and cost-effective module for improving the utility and accuracy of\\nRAG systems.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Retrieve Anything To Augment Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2310.07554v2</td>\n",
       "      <td>Large language models (LLMs) face significant challenges stemming from their\\ninherent limitations in knowledge, memory, alignment, and action. These\\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\\nfrom the external world, such as knowledge base, memory store, demonstration\\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\\nbridging the gap between LLMs and the external assistance. However,\\nconventional methods encounter two pressing issues. On the one hand, the\\ngeneral-purpose retrievers are not properly optimized for the retrieval\\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\\nrequired versatility, hindering their performance across the diverse retrieval\\naugmentation scenarios.\\n  In this work, we present a novel approach, the LLM-Embedder, which\\ncomprehensively supports the diverse retrieval augmentation needs of LLMs with\\none unified embedding model. Training such a unified model is non-trivial, as\\nvarious retrieval tasks aim to capture distinct semantic relationships, often\\nsubject to mutual interference. To address this challenge, we systematically\\noptimize our training methodology. This includes reward formulation based on\\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\\nfine-tuning with explicit instructions, and homogeneous in-batch negative\\nsampling. These optimization strategies contribute to the outstanding empirical\\nperformance of the LLM-Embedder. Notably, it yields remarkable enhancements in\\nretrieval augmentation for LLMs, surpassing both general-purpose and\\ntask-specific retrievers in various evaluation scenarios. Our checkpoint and\\nsource code are publicly available at\\nhttps://github.com/FlagOpen/FlagEmbedding.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>GABO: Graph Augmentations with Bi-level Optimization</td>\n",
       "      <td>http://arxiv.org/abs/2104.00722v1</td>\n",
       "      <td>Data augmentation refers to a wide range of techniques for improving model\\ngeneralization by augmenting training examples. Oftentimes such methods require\\ndomain knowledge about the dataset at hand, spawning a plethora of recent\\nliterature surrounding automated techniques for data augmentation. In this work\\nwe apply one such method, bilevel optimization, to tackle the problem of graph\\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\\nmakes it the most effective augmenter for this classifier on the leaderboard.\\nThis framework combines a GIN layer augmentation generator with a bias\\ntransformation and outperforms the same classifier augmented using the\\nstate-of-the-art FLAG augmentation.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation</td>\n",
       "      <td>http://arxiv.org/abs/2411.07021v2</td>\n",
       "      <td>Retrieval-augmented generation (RAG) has shown impressive capability in\\nproviding reliable answer predictions and addressing hallucination problems. A\\ntypical RAG implementation uses powerful retrieval models to extract external\\ninformation and large language models (LLMs) to generate answers. In contrast,\\nrecent LLM-based retrieval has gained attention for its substantial\\nimprovements in information retrieval (IR) due to the LLMs' semantic\\nunderstanding capability. However, directly applying LLM to RAG systems\\npresents challenges. This may cause feature locality problems as massive\\nparametric knowledge can hinder effective usage of global information across\\nthe corpus; for example, an LLM-based retriever often inputs document summaries\\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\\nintroduce variance, further weakening performance as a retriever.\\n  To address these issues, we propose a novel two-stage fine-tuning\\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\\nis constructed by integrating LoRA-based representation learning to tackle\\nfeature locality issues. To enhance retrieval performance, we develop two\\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\\nvariance. In the generation stage, a refined fine-tuning method is employed to\\nimprove LLM accuracy in generating answers based on retrieved information.\\nExperimental results show that Invar-RAG significantly outperforms existing\\nbaselines across three open-domain question answering (ODQA) datasets. Code is\\navailable in the Supplementary Material for reproducibility.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2410.20724v2</td>\n",
       "      <td>Large Language Models (LLMs) demonstrate strong reasoning abilities but face\\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\\ngrounding LLM outputs in structured external knowledge from KGs. However,\\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\\nextending the KG-based RAG framework that retrieves subgraphs and leverages\\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\\nfor efficient and flexible subgraph retrieval while encoding directional\\nstructural distances to enhance retrieval effectiveness. The size of retrieved\\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\\nLLM's capabilities. This design strikes a balance between model complexity and\\nreasoning power, enabling scalable and generalizable retrieval processes.\\nNotably, based on our retrieved subgraphs, smaller LLMs like\\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\\naccuracy, and reliability by reducing hallucinations and improving response\\ngrounding.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases</td>\n",
       "      <td>http://arxiv.org/abs/2403.10446v1</td>\n",
       "      <td>We proposed an end-to-end system design towards utilizing Retrieval Augmented\\nGeneration (RAG) to improve the factual accuracy of Large Language Models\\n(LLMs) for domain-specific and time-sensitive queries related to private\\nknowledge-bases. Our system integrates RAG pipeline with upstream datasets\\nprocessing and downstream performance evaluation. Addressing the challenge of\\nLLM hallucinations, we finetune models with a curated dataset which originates\\nfrom CMU's extensive resources and annotated with the teacher model. Our\\nexperiments demonstrate the system's effectiveness in generating more accurate\\nanswers to domain-specific and time-sensitive inquiries. The results also\\nrevealed the limitations of fine-tuning LLMs with small-scale and skewed\\ndatasets. This research highlights the potential of RAG systems in augmenting\\nLLMs with external datasets for improved performance in knowledge-intensive\\ntasks. Our code and models are available on Github.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding</td>\n",
       "      <td>http://arxiv.org/abs/2501.07861v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\\nhold promise in knowledge-intensive tasks but face limitations in complex\\nmulti-step reasoning. While recent methods have integrated RAG with\\nchain-of-thought reasoning or test-time search using Process Reward Models\\n(PRMs), these approaches encounter challenges such as a lack of explanations,\\nbias in PRM training data, early-step bias in PRM scores, and insufficient\\npost-training optimization of reasoning potential. To address these issues, we\\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\\nscoring and a Process Explanation Model (PEM) for generating natural language\\nexplanations, enabling step refinement. During post-training, it utilizes Monte\\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\\nhigh-quality step-level preference data, optimized through Iterative Preference\\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\\ntraining data, mitigated by balanced annotation methods and stronger\\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\\nthrough a temporal-difference-based look-ahead search strategy. Experimental\\nresults on multi-step reasoning benchmarks demonstrate significant\\nimprovements, underscoring ReARTeR's potential to advance the reasoning\\ncapabilities of RAG systems.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs</td>\n",
       "      <td>http://arxiv.org/abs/2308.12574v2</td>\n",
       "      <td>The integration of retrieved passages and large language models (LLMs), such\\nas ChatGPTs, has significantly contributed to improving open-domain question\\nanswering. However, there is still a lack of exploration regarding the optimal\\napproach for incorporating retrieved passages into the answer generation\\nprocess. This paper aims to fill this gap by investigating different methods of\\ncombining retrieved passages with LLMs to enhance answer generation. We begin\\nby examining the limitations of a commonly-used concatenation approach.\\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\\nwhen the correct document is among the top-k retrieved passages. To address\\nthis issue, we explore four alternative strategies for integrating the\\nretrieved passages with the LLMs. These strategies include two single-round\\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\\nthat incorporate feedback loops. Through comprehensive analyses and\\nexperiments, we provide insightful observations on how to effectively leverage\\nretrieved passages to enhance the answer generation capability of LLMs.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     title  \\\n",
       "6                                                                     Context-Augmented Code Generation Using Programming Knowledge Graphs   \n",
       "10                                                  Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models   \n",
       "14                                                        The Effects of Hallucinations in Synthetic Training Data for Relation Extraction   \n",
       "29                                                      The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation   \n",
       "37                                                                    RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots   \n",
       "77                                                            Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources   \n",
       "91                                                           Seven Failure Points When Engineering a Retrieval Augmented Generation System   \n",
       "102                                    Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models   \n",
       "108                                                                                     Retrieve Anything To Augment Large Language Models   \n",
       "117                                                                                   GABO: Graph Augmentations with Bi-level Optimization   \n",
       "127                                                                       Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation   \n",
       "158             Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation   \n",
       "165  Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases   \n",
       "187                                                              ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding   \n",
       "189                               Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs   \n",
       "\n",
       "                              entry_id  \\\n",
       "6    http://arxiv.org/abs/2410.18251v1   \n",
       "10   http://arxiv.org/abs/2410.15116v1   \n",
       "14   http://arxiv.org/abs/2410.08393v1   \n",
       "29   http://arxiv.org/abs/2407.18044v1   \n",
       "37   http://arxiv.org/abs/2403.01193v3   \n",
       "77   http://arxiv.org/abs/2406.07136v1   \n",
       "91   http://arxiv.org/abs/2401.05856v1   \n",
       "102  http://arxiv.org/abs/2411.07820v2   \n",
       "108  http://arxiv.org/abs/2310.07554v2   \n",
       "117  http://arxiv.org/abs/2104.00722v1   \n",
       "127  http://arxiv.org/abs/2411.07021v2   \n",
       "158  http://arxiv.org/abs/2410.20724v2   \n",
       "165  http://arxiv.org/abs/2403.10446v1   \n",
       "187  http://arxiv.org/abs/2501.07861v1   \n",
       "189  http://arxiv.org/abs/2308.12574v2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        summary  \\\n",
       "6                                                                                                                                                                                                                                                                                                                                                    Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\\nimproved code generation, but, they frequently face difficulties when dealing\\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\\naddresses this issue by retrieving and integrating external knowledge at the\\ninference time. However, retrieval models often fail to find most relevant\\ncontext, and generation models, with limited context capacity, can hallucinate\\nwhen given irrelevant data. We present a novel framework that leverages a\\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\\nThis approach enables fine-grained code retrieval by focusing on the most\\nrelevant segments while reducing irrelevant context through a tree-pruning\\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\\nhallucinations by selectively integrating non-RAG solutions. We propose two\\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\\ntree pruning to enhance retrieval precision, a re-ranking method for robust\\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\\ncode augmentation with relevant comments and docstrings.   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                       Generation of plausible but incorrect factual information, often termed\\nhallucination, has attracted significant research interest. Retrieval-augmented\\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\\nemerges as a promising method to reduce hallucination. However, existing RALMs\\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\\naddress this challenge, we propose COFT, a novel\\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\\nknowledge graph to extract potential key entities in a given context. Second,\\n\\textit{scorer} measures the importance of each entity by calculating its\\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\\nentities with a dynamic threshold algorithm and highlights the corresponding\\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\\nexperiments on the knowledge hallucination benchmark demonstrate the\\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\\nscore metric. Moreover, COFT also exhibits remarkable versatility across\\nvarious long-form tasks, such as reading comprehension and question answering.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                     Relation extraction is crucial for constructing knowledge graphs, with large\\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\\nevaluating models. Generative data augmentation (GDA) is a common approach to\\nexpand such datasets. However, this approach often introduces hallucinations,\\nsuch as spurious facts, whose impact on relation extraction remains\\nunderexplored. In this paper, we examine the effects of hallucinations on the\\nperformance of relation extraction on the document and sentence levels. Our\\nempirical study reveals that hallucinations considerably compromise the ability\\nof models to extract relations from text, with recall reductions between 19.1%\\nand 39.2%. We identify that relevant hallucinations impair the model's\\nperformance, while irrelevant hallucinations have a minimal impact.\\nAdditionally, we develop methods for the detection of hallucinations to improve\\ndata quality and model performance. Our approaches successfully classify texts\\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\\n92.2%. These methods not only assist in removing hallucinations but also help\\nin estimating their prevalence within datasets, which is crucial for selecting\\nhigh-quality data. Overall, our work confirms the profound impact of relevant\\nhallucinations on the effectiveness of relation extraction models.   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                   Digital health chatbots powered by Large Language Models (LLMs) have the\\npotential to significantly improve personal health management for chronic\\nconditions by providing accessible and on-demand health coaching and\\nquestion-answering. However, these chatbots risk providing unverified and\\ninaccurate information because LLMs generate responses based on patterns\\nlearned from diverse internet data. Retrieval Augmented Generation (RAG) can\\nhelp mitigate hallucinations and inaccuracies in LLM responses by grounding it\\non reliable content. However, efficiently and accurately retrieving most\\nrelevant set of content for real-time user questions remains a challenge. In\\nthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a\\nnovel approach that pre-computes a database of potential queries from a content\\nbase using LLMs. For an incoming patient question, QB-RAG efficiently matches\\nit against this pre-generated query database using vector search, improving\\nalignment between user questions and the content. We establish a theoretical\\nfoundation for QB-RAG and provide a comparative analysis of existing retrieval\\nenhancement techniques for RAG systems. Finally, our empirical evaluation\\ndemonstrates that QB-RAG significantly improves the accuracy of healthcare\\nquestion answering, paving the way for robust and trustworthy LLM applications\\nin digital health.   \n",
       "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\\nof artificial intelligence. However, their tendency to hallucinate -- generate\\nplausible but false information -- poses a significant challenge. This issue is\\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\\nGeneration (RAG) can counter hallucinations by integrating external knowledge\\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\\nin some cases, but can still be misled when prompts directly contradict the\\nmodel's pre-trained understanding. These findings highlight the complex nature\\nof hallucinations and the need for more robust solutions to ensure LLM\\nreliability in real-world applications. We offer practical recommendations for\\nRAG deployment and discuss implications for the development of more trustworthy\\nLLMs.   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Query expansion has been employed for a long time to improve the accuracy of\\nquery retrievers. Earlier works relied on pseudo-relevance feedback (PRF)\\ntechniques, which augment a query with terms extracted from documents retrieved\\nin a first stage. However, the documents may be noisy hindering the\\neffectiveness of the ranking. To avoid this, recent studies have instead used\\nLarge Language Models (LLMs) to generate additional content to expand a query.\\nThese techniques are prone to hallucination and also focus on the LLM usage\\ncost. However, the cost may be dominated by the retrieval in several important\\npractical scenarios, where the corpus is only available via APIs which charge a\\nfee per retrieved document. We propose combining classic PRF techniques with\\nLLMs and create a progressive query expansion algorithm ProQE that iteratively\\nexpands the query as it retrieves more documents. ProQE is compatible with both\\nsparse and dense retrieval systems. Our experimental results on four retrieval\\ndatasets show that ProQE outperforms state-of-the-art baselines by 37% and is\\nthe most cost-effective.   \n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Software engineers are increasingly adding semantic search capabilities to\\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\\nRAG system involves finding documents that semantically match a query and then\\npassing the documents to a large language model (LLM) such as ChatGPT to\\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\\nproblem of hallucinated responses from LLMs, b) link sources/references to\\ngenerated responses, and c) remove the need for annotating documents with\\nmeta-data. However, RAG systems suffer from limitations inherent to information\\nretrieval systems and from reliance on LLMs. In this paper, we present an\\nexperience report on the failure points of RAG systems from three case studies\\nfrom separate domains: research, education, and biomedical. We share the\\nlessons learned and present 7 failure points to consider when designing a RAG\\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\\nsystem is only feasible during operation, and 2) the robustness of a RAG system\\nevolves rather than designed in at the start. We conclude with a list of\\npotential research directions on RAG systems for the software engineering\\ncommunity.   \n",
       "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel\\napproach designed to bridge the pre-retrieval information gap in\\nRetrieval-Augmented Generation (RAG) systems through query optimization\\ntailored to meet the specific knowledge requirements of Large Language Models\\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\\nframework begins by extracting parametric knowledge from LLMs, followed by\\nusing a specialized query optimizer for refining these queries. This process\\nensures the retrieval of only the most pertinent information essential for\\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\\ncomputational costs, we propose a trainable scheme for our pipeline that\\nutilizes a smaller, tunable model as the query optimizer, which is refined\\nthrough knowledge distillation from a larger teacher model. Our evaluations on\\nvarious question-answering (QA) datasets and with different retrieval systems\\nshow that ERRR consistently outperforms existing baselines, proving to be a\\nversatile and cost-effective module for improving the utility and accuracy of\\nRAG systems.   \n",
       "108  Large language models (LLMs) face significant challenges stemming from their\\ninherent limitations in knowledge, memory, alignment, and action. These\\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\\nfrom the external world, such as knowledge base, memory store, demonstration\\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\\nbridging the gap between LLMs and the external assistance. However,\\nconventional methods encounter two pressing issues. On the one hand, the\\ngeneral-purpose retrievers are not properly optimized for the retrieval\\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\\nrequired versatility, hindering their performance across the diverse retrieval\\naugmentation scenarios.\\n  In this work, we present a novel approach, the LLM-Embedder, which\\ncomprehensively supports the diverse retrieval augmentation needs of LLMs with\\none unified embedding model. Training such a unified model is non-trivial, as\\nvarious retrieval tasks aim to capture distinct semantic relationships, often\\nsubject to mutual interference. To address this challenge, we systematically\\noptimize our training methodology. This includes reward formulation based on\\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\\nfine-tuning with explicit instructions, and homogeneous in-batch negative\\nsampling. These optimization strategies contribute to the outstanding empirical\\nperformance of the LLM-Embedder. Notably, it yields remarkable enhancements in\\nretrieval augmentation for LLMs, surpassing both general-purpose and\\ntask-specific retrievers in various evaluation scenarios. Our checkpoint and\\nsource code are publicly available at\\nhttps://github.com/FlagOpen/FlagEmbedding.   \n",
       "117                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Data augmentation refers to a wide range of techniques for improving model\\ngeneralization by augmenting training examples. Oftentimes such methods require\\ndomain knowledge about the dataset at hand, spawning a plethora of recent\\nliterature surrounding automated techniques for data augmentation. In this work\\nwe apply one such method, bilevel optimization, to tackle the problem of graph\\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\\nmakes it the most effective augmenter for this classifier on the leaderboard.\\nThis framework combines a GIN layer augmentation generator with a bias\\ntransformation and outperforms the same classifier augmented using the\\nstate-of-the-art FLAG augmentation.   \n",
       "127                                                                                                                                            Retrieval-augmented generation (RAG) has shown impressive capability in\\nproviding reliable answer predictions and addressing hallucination problems. A\\ntypical RAG implementation uses powerful retrieval models to extract external\\ninformation and large language models (LLMs) to generate answers. In contrast,\\nrecent LLM-based retrieval has gained attention for its substantial\\nimprovements in information retrieval (IR) due to the LLMs' semantic\\nunderstanding capability. However, directly applying LLM to RAG systems\\npresents challenges. This may cause feature locality problems as massive\\nparametric knowledge can hinder effective usage of global information across\\nthe corpus; for example, an LLM-based retriever often inputs document summaries\\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\\nintroduce variance, further weakening performance as a retriever.\\n  To address these issues, we propose a novel two-stage fine-tuning\\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\\nis constructed by integrating LoRA-based representation learning to tackle\\nfeature locality issues. To enhance retrieval performance, we develop two\\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\\nvariance. In the generation stage, a refined fine-tuning method is employed to\\nimprove LLM accuracy in generating answers based on retrieved information.\\nExperimental results show that Invar-RAG significantly outperforms existing\\nbaselines across three open-domain question answering (ODQA) datasets. Code is\\navailable in the Supplementary Material for reproducibility.   \n",
       "158                                                                                                                                                                   Large Language Models (LLMs) demonstrate strong reasoning abilities but face\\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\\ngrounding LLM outputs in structured external knowledge from KGs. However,\\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\\nextending the KG-based RAG framework that retrieves subgraphs and leverages\\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\\nfor efficient and flexible subgraph retrieval while encoding directional\\nstructural distances to enhance retrieval effectiveness. The size of retrieved\\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\\nLLM's capabilities. This design strikes a balance between model complexity and\\nreasoning power, enabling scalable and generalizable retrieval processes.\\nNotably, based on our retrieved subgraphs, smaller LLMs like\\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\\naccuracy, and reliability by reducing hallucinations and improving response\\ngrounding.   \n",
       "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We proposed an end-to-end system design towards utilizing Retrieval Augmented\\nGeneration (RAG) to improve the factual accuracy of Large Language Models\\n(LLMs) for domain-specific and time-sensitive queries related to private\\nknowledge-bases. Our system integrates RAG pipeline with upstream datasets\\nprocessing and downstream performance evaluation. Addressing the challenge of\\nLLM hallucinations, we finetune models with a curated dataset which originates\\nfrom CMU's extensive resources and annotated with the teacher model. Our\\nexperiments demonstrate the system's effectiveness in generating more accurate\\nanswers to domain-specific and time-sensitive inquiries. The results also\\nrevealed the limitations of fine-tuning LLMs with small-scale and skewed\\ndatasets. This research highlights the potential of RAG systems in augmenting\\nLLMs with external datasets for improved performance in knowledge-intensive\\ntasks. Our code and models are available on Github.   \n",
       "187                                                                               Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\\nhold promise in knowledge-intensive tasks but face limitations in complex\\nmulti-step reasoning. While recent methods have integrated RAG with\\nchain-of-thought reasoning or test-time search using Process Reward Models\\n(PRMs), these approaches encounter challenges such as a lack of explanations,\\nbias in PRM training data, early-step bias in PRM scores, and insufficient\\npost-training optimization of reasoning potential. To address these issues, we\\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\\nscoring and a Process Explanation Model (PEM) for generating natural language\\nexplanations, enabling step refinement. During post-training, it utilizes Monte\\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\\nhigh-quality step-level preference data, optimized through Iterative Preference\\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\\ntraining data, mitigated by balanced annotation methods and stronger\\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\\nthrough a temporal-difference-based look-ahead search strategy. Experimental\\nresults on multi-step reasoning benchmarks demonstrate significant\\nimprovements, underscoring ReARTeR's potential to advance the reasoning\\ncapabilities of RAG systems.   \n",
       "189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The integration of retrieved passages and large language models (LLMs), such\\nas ChatGPTs, has significantly contributed to improving open-domain question\\nanswering. However, there is still a lack of exploration regarding the optimal\\napproach for incorporating retrieved passages into the answer generation\\nprocess. This paper aims to fill this gap by investigating different methods of\\ncombining retrieved passages with LLMs to enhance answer generation. We begin\\nby examining the limitations of a commonly-used concatenation approach.\\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\\nwhen the correct document is among the top-k retrieved passages. To address\\nthis issue, we explore four alternative strategies for integrating the\\nretrieved passages with the LLMs. These strategies include two single-round\\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\\nthat incorporate feedback loops. Through comprehensive analyses and\\nexperiments, we provide insightful observations on how to effectively leverage\\nretrieved passages to enhance the answer generation capability of LLMs.   \n",
       "\n",
       "     is_relevant  \n",
       "6              4  \n",
       "10             4  \n",
       "14             4  \n",
       "29             5  \n",
       "37             5  \n",
       "77             5  \n",
       "91             4  \n",
       "102            5  \n",
       "108            5  \n",
       "117            5  \n",
       "127            5  \n",
       "158            4  \n",
       "165            5  \n",
       "187            5  \n",
       "189            5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arixv_result_df.query('is_direct == True')\\\n",
    "    [['title','entry_id', 'summary', 'is_relevant']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
