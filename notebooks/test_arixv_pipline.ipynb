{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.arxiv_search import expand_arxiv_query, run_multi_arixv_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"\"\"RAG(retrieval augmented generation) sometimes the sources retrieved are relevant but not enough to answer the question user ask. How do research approach this case\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = expand_arxiv_query(problem_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Relevant source retrieval in RAG',\n",
       " 'Dealing with inadequate sources in RAG',\n",
       " 'Improving source retrieval in Retrieval Augmented Generation',\n",
       " 'Strategies to enhance relevancy of retrieved sources in RAG',\n",
       " 'Techniques for better source retrieval in RAG',\n",
       " 'Addressing relevancy in source retrieval in RAG',\n",
       " 'Approaches to improve source usefulness in RAG',\n",
       " 'Optimizing source retrieval in Retrieval Augmented Generation',\n",
       " 'Methods to boost relevant source retrieval in Retrieval Augmented Generation',\n",
       " 'Resolving relevance issue in source retrieval in RAG']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "arxiv_query_results = run_multi_arixv_queries(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_query_results.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten arxiv_query_results\n",
    "all_results = []\n",
    "for query in arxiv_query_results:\n",
    "    for result in arxiv_query_results[query]:\n",
    "        all_results.append(result)\n",
    "\n",
    "# make arxiv query results a dataframe and create a new dataframe with only unique entry_id\n",
    "import pandas as pd\n",
    "\n",
    "arixv_result_df = pd.DataFrame(all_results)\n",
    "unique_arixv_result_df = arixv_result_df.drop_duplicates(subset='entry_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 13), (95, 13))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.shape, unique_arixv_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>links</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.07176v1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2405.19207v1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2402.17497v1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2402.11891v1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.03780v1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2404.07221v2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2406.13213v2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.01782v1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2210.02627v1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2407.11005v1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   authors  categories  comment  doi  \\\n",
       "entry_id                                                               \n",
       "http://arxiv.org/abs/2410.07176v1        7           7        7    0   \n",
       "http://arxiv.org/abs/2405.19207v1        7           7        7    0   \n",
       "http://arxiv.org/abs/2402.17497v1        7           7        0    0   \n",
       "http://arxiv.org/abs/2402.11891v1        6           6        0    0   \n",
       "http://arxiv.org/abs/2410.03780v1        6           6        0    0   \n",
       "http://arxiv.org/abs/2404.07221v2        6           6        0    0   \n",
       "http://arxiv.org/abs/2406.13213v2        6           6        6    0   \n",
       "http://arxiv.org/abs/2410.01782v1        5           5        5    0   \n",
       "http://arxiv.org/abs/2210.02627v1        5           5        5    0   \n",
       "http://arxiv.org/abs/2407.11005v1        5           5        0    0   \n",
       "\n",
       "                                   journal_ref  links  pdf_url  \\\n",
       "entry_id                                                         \n",
       "http://arxiv.org/abs/2410.07176v1            0      7        7   \n",
       "http://arxiv.org/abs/2405.19207v1            0      7        7   \n",
       "http://arxiv.org/abs/2402.17497v1            0      7        7   \n",
       "http://arxiv.org/abs/2402.11891v1            0      6        6   \n",
       "http://arxiv.org/abs/2410.03780v1            0      6        6   \n",
       "http://arxiv.org/abs/2404.07221v2            0      6        6   \n",
       "http://arxiv.org/abs/2406.13213v2            0      6        6   \n",
       "http://arxiv.org/abs/2410.01782v1            0      5        5   \n",
       "http://arxiv.org/abs/2210.02627v1            0      5        5   \n",
       "http://arxiv.org/abs/2407.11005v1            0      5        5   \n",
       "\n",
       "                                   primary_category  published  summary  \\\n",
       "entry_id                                                                  \n",
       "http://arxiv.org/abs/2410.07176v1                 7          7        7   \n",
       "http://arxiv.org/abs/2405.19207v1                 7          7        7   \n",
       "http://arxiv.org/abs/2402.17497v1                 7          7        7   \n",
       "http://arxiv.org/abs/2402.11891v1                 6          6        6   \n",
       "http://arxiv.org/abs/2410.03780v1                 6          6        6   \n",
       "http://arxiv.org/abs/2404.07221v2                 6          6        6   \n",
       "http://arxiv.org/abs/2406.13213v2                 6          6        6   \n",
       "http://arxiv.org/abs/2410.01782v1                 5          5        5   \n",
       "http://arxiv.org/abs/2210.02627v1                 5          5        5   \n",
       "http://arxiv.org/abs/2407.11005v1                 5          5        5   \n",
       "\n",
       "                                   title  updated  \n",
       "entry_id                                           \n",
       "http://arxiv.org/abs/2410.07176v1      7        7  \n",
       "http://arxiv.org/abs/2405.19207v1      7        7  \n",
       "http://arxiv.org/abs/2402.17497v1      7        7  \n",
       "http://arxiv.org/abs/2402.11891v1      6        6  \n",
       "http://arxiv.org/abs/2410.03780v1      6        6  \n",
       "http://arxiv.org/abs/2404.07221v2      6        6  \n",
       "http://arxiv.org/abs/2406.13213v2      6        6  \n",
       "http://arxiv.org/abs/2410.01782v1      5        5  \n",
       "http://arxiv.org/abs/2210.02627v1      5        5  \n",
       "http://arxiv.org/abs/2407.11005v1      5        5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.groupby('entry_id').count().sort_values('title', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.prompts.content_curation import create_arxiv_filtering_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_arxiv_filtering_prompt in module askharrison.prompts.content_curation:\n",
      "\n",
      "create_arxiv_filtering_prompt(problem_statement: str, doc_abstract: str)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(create_arxiv_filtering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt for each arxiv entry\n",
    "arxiv_reranking_prompts = [create_arxiv_filtering_prompt(problem_statement, \n",
    "                                         record['title']+\"\\n\"+record['summary']) for record in unique_arixv_result_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.llm_models import parallel_llm_processor, process_question, safe_eval, extract_python_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 95/95 [00:30<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "reranking_llm_response = parallel_llm_processor(arxiv_reranking_prompts, llm_function=process_question, \n",
    "                                                max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95, 13), 95)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_responses_results = [safe_eval(extract_python_code(response)) for response in reranking_llm_response]\n",
    "# filter out empty responses\n",
    "\n",
    "unique_arixv_result_df.shape, len(llm_responses_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_93080\\658567496.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_93080\\658567496.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_93080\\658567496.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
     ]
    }
   ],
   "source": [
    "# extract reasoning, is_direct, 'is_relevant' from llm_responses_results if it is not empty, and add to unique_arixv_result_df\n",
    "unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase max column width in pandas\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems</td>\n",
       "      <td>http://arxiv.org/abs/2407.11005v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) has become a standard architectural\\npattern for incorporating domain-specific knowledge into user-facing chat\\napplications powered by Large Language Models (LLMs). RAG systems are\\ncharacterized by (1) a document retriever that queries a domain-specific corpus\\nfor context information relevant to an input query, and (2) an LLM that\\ngenerates a response based on the provided query and context. However,\\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\\nunified evaluation criteria and annotated datasets. In response, we introduce\\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\\nexamples. It covers five unique industry-specific domains and various RAG task\\ntypes. RAGBench examples are sourced from industry corpora such as user\\nmanuals, making it particularly relevant for industry applications. Further, we\\nformalize the TRACe evaluation framework: a set of explainable and actionable\\nRAG evaluation metrics applicable across all RAG domains. We release the\\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\\nenabling actionable feedback for continuous improvement of production\\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\\nRAG evaluation task. We identify areas where existing approaches fall short and\\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\\nevaluation systems.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2410.01782v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\\naccuracy of Large Language Models (LLMs), but existing methods often suffer\\nfrom limited reasoning capabilities in effectively using the retrieved\\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\\ncapabilities in RAG with open-source LLMs. Our framework transforms an\\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\\nmodel capable of handling complex reasoning tasks, including both single- and\\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\\ndistractors that appear relevant but are misleading. As a result, Open-RAG\\nleverages latent learning, dynamically selecting relevant experts and\\nintegrating external knowledge effectively for more accurate and contextually\\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\\nto determine retrieval necessity and balance the trade-off between performance\\ngain and inference speed. Experimental results show that the Llama2-7B-based\\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\\nour code and models at https://openragmoe.github.io/</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A Multi-Source Retrieval Question Answering Framework Based on RAG</td>\n",
       "      <td>http://arxiv.org/abs/2405.19207v1</td>\n",
       "      <td>With the rapid development of large-scale language models,\\nRetrieval-Augmented Generation (RAG) has been widely adopted. However, existing\\nRAG paradigms are inevitably influenced by erroneous retrieval information,\\nthereby reducing the reliability and correctness of generated results.\\nTherefore, to improve the relevance of retrieval information, this study\\nproposes a method that replaces traditional retrievers with GPT-3.5, leveraging\\nits vast corpus knowledge to generate retrieval information. We also propose a\\nweb retrieval based method to implement fine-grained knowledge retrieval,\\nUtilizing the powerful reasoning capability of GPT-3.5 to realize semantic\\npartitioning of problem.In order to mitigate the illusion of GPT retrieval and\\nreduce noise in Web retrieval,we proposes a multi-source retrieval framework,\\nnamed MSRAG, which combines GPT retrieval with web retrieval. Experiments on\\nmultiple knowledge-intensive QA datasets demonstrate that the proposed\\nframework in this study performs better than existing RAG framework in\\nenhancing the overall efficiency and accuracy of QA systems.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Improving Retrieval for RAG based Question Answering Models on Financial Documents</td>\n",
       "      <td>http://arxiv.org/abs/2404.07221v2</td>\n",
       "      <td>The effectiveness of Large Language Models (LLMs) in generating accurate\\nresponses relies heavily on the quality of input provided, particularly when\\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\\nsignificant advancements in LLMs' response quality in recent years, users may\\nstill encounter inaccuracies or irrelevant answers; these issues often stem\\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\\nthe RAG process. This paper explores the existing constraints of RAG pipelines\\nand introduces methodologies for enhancing text retrieval. It delves into\\nstrategies such as sophisticated chunking techniques, query expansion, the\\nincorporation of metadata annotations, the application of re-ranking\\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\\napproaches can substantially improve the retrieval quality, thereby elevating\\nthe overall performance and reliability of LLMs in processing and responding to\\nqueries.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</td>\n",
       "      <td>http://arxiv.org/abs/2401.15391v1</td>\n",
       "      <td>Retrieval-augmented generation (RAG) augments large language models (LLM) by\\nretrieving relevant knowledge, showing promising potential in mitigating LLM\\nhallucinations and enhancing response quality, thereby facilitating the great\\nadoption of LLMs in practice. However, we find that existing RAG systems are\\ninadequate in answering multi-hop queries, which require retrieving and\\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-hop queries, their ground-truth\\nanswers, and the associated supporting evidence. We detail the procedure of\\nbuilding the dataset, utilizing an English news article dataset as the\\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\\nMultiHop-RAG in two experiments. The first experiment compares different\\nembedding models for retrieving evidence for multi-hop queries. In the second\\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\\nqueries given the evidence. Both experiments reveal that existing RAG methods\\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\\nMultiHop-RAG will be a valuable resource for the community in developing\\neffective RAG systems, thereby facilitating greater adoption of LLMs in\\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\\nhttps://github.com/yixuantt/MultiHop-RAG/.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG</td>\n",
       "      <td>http://arxiv.org/abs/2410.05983v1</td>\n",
       "      <td>Retrieval-augmented generation (RAG) empowers large language models (LLMs) to\\nutilize external knowledge sources. The increasing capacity of LLMs to process\\nlonger input sequences opens up avenues for providing more retrieved\\ninformation, to potentially enhance the quality of generated outputs. It is\\nplausible to assume that a larger retrieval set would contain more relevant\\ninformation (higher recall), that might result in improved performance.\\nHowever, our empirical findings demonstrate that for many long-context LLMs,\\nthe quality of generated output initially improves first, but then subsequently\\ndeclines as the number of retrieved passages increases. This paper investigates\\nthis phenomenon, identifying the detrimental impact of retrieved \"hard\\nnegatives\" as a key contributor. To mitigate this and enhance the robustness of\\nlong-context LLM-based RAG, we propose both training-free and training-based\\napproaches. We first showcase the effectiveness of retrieval reordering as a\\nsimple yet powerful training-free optimization. Furthermore, we explore\\ntraining-based methods, specifically RAG-specific implicit LLM fine-tuning and\\nRAG-oriented fine-tuning with intermediate reasoning, demonstrating their\\ncapacity for substantial performance gains. Finally, we conduct a systematic\\nanalysis of design choices for these training-based methods, including data\\ndistribution, retriever selection, and training context length.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems</td>\n",
       "      <td>http://arxiv.org/abs/2409.19804v1</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) have recently gained significant\\nattention for their enhanced ability to integrate external knowledge sources in\\nopen-domain question answering (QA) tasks. However, it remains unclear how\\nthese models address fairness concerns, particularly with respect to sensitive\\nattributes such as gender, geographic location, and other demographic factors.\\nFirst, as language models evolve to prioritize utility, like improving exact\\nmatch accuracy, fairness may have been largely overlooked. Second, RAG methods\\nare complex pipelines, making it hard to identify and address biases, as each\\ncomponent is optimized for different goals. In this paper, we aim to\\nempirically evaluate fairness in several RAG methods. We propose a fairness\\nevaluation framework tailored to RAG methods, using scenario-based questions\\nand analyzing disparities across demographic attributes. The experimental\\nresults indicate that, despite recent advances in utility-driven optimization,\\nfairness issues persist in both the retrieval and generation stages,\\nhighlighting the need for more targeted fairness interventions within RAG\\npipelines. We will release our dataset and code upon acceptance of the paper.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In</td>\n",
       "      <td>http://arxiv.org/abs/2305.17331v1</td>\n",
       "      <td>Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation</td>\n",
       "      <td>http://arxiv.org/abs/2408.03623v1</td>\n",
       "      <td>Code comment generation aims to generate high-quality comments from source\\ncode automatically and has been studied for years. Recent studies proposed to\\nintegrate information retrieval techniques with neural generation models to\\ntackle this problem, i.e., Retrieval-Augmented Comment Generation (RACG)\\napproaches, and achieved state-of-the-art results. However, the retrievers in\\nprevious work are built independently of their generators. This results in that\\nthe retrieved exemplars are not necessarily the most useful ones for generating\\ncomments, limiting the performance of existing approaches. To address this\\nlimitation, we propose a novel training strategy to enable the retriever to\\nlearn from the feedback of the generator and retrieve exemplars for generation.\\nSpecifically, during training, we use the retriever to retrieve the top-k\\nexemplars and calculate their retrieval scores, and use the generator to\\ncalculate a generation loss for the sample based on each exemplar. By aligning\\nhigh-score exemplars retrieved by the retriever with low-loss exemplars\\nobserved by the generator, the retriever can learn to retrieve exemplars that\\ncan best improve the quality of the generated comments. Based on this strategy,\\nwe propose a novel RACG approach named JOINTCOM and evaluate it on two\\nreal-world datasets, JCSD and PCSD. The experimental results demonstrate that\\nour approach surpasses the state-of-the-art baselines by 7.3% to 30.0% in terms\\nof five metrics on the two datasets. We also conduct a human evaluation to\\ncompare JOINTCOM with the best-performing baselines. The results indicate that\\nJOINTCOM outperforms the baselines, producing comments that are more natural,\\ninformative, and useful.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs</td>\n",
       "      <td>http://arxiv.org/abs/2406.14277v2</td>\n",
       "      <td>Retrieval-augmented generation (RAG) has received much attention for\\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\\nparametric knowledge of large language models (LLMs). While previous approaches\\nfocused on processing retrieved passages to remove irrelevant context, they\\nstill rely heavily on the quality of retrieved passages which can degrade if\\nthe question is ambiguous or complex. In this paper, we propose a simple yet\\nefficient method called question and passage augmentation (QPaug) via LLMs for\\nopen-domain QA. QPaug first decomposes the original questions into\\nmultiple-step sub-questions. By augmenting the original question with detailed\\nsub-questions and planning, we are able to make the query more specific on what\\nneeds to be retrieved, improving the retrieval performance. In addition, to\\ncompensate for the case where the retrieved passages contain distracting\\ninformation or divided opinions, we augment the retrieved passages with\\nself-generated passages by LLMs to guide the answer extraction. Experimental\\nresults show that QPaug outperforms the previous state-of-the-art and achieves\\nsignificant performance gain over existing RAG methods. The source code is\\navailable at \\url{https://github.com/kmswin1/QPaug}.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Retrieval-augmented code completion for local projects using large language models</td>\n",
       "      <td>http://arxiv.org/abs/2408.05026v1</td>\n",
       "      <td>The use of large language models (LLMs) is becoming increasingly widespread\\namong software developers. However, privacy and computational requirements are\\nproblematic with commercial solutions and the use of LLMs. In this work, we\\nfocus on using LLMs with around 160 million parameters that are suitable for\\nlocal execution and augmentation with retrieval from local projects. We train\\ntwo models based on the transformer architecture, the generative model GPT-2\\nand the retrieval-adapted RETRO model, on open-source Python files, and\\nempirically evaluate and compare them, confirming the benefits of vector\\nembedding based retrieval. Further, we improve our models' performance with\\nIn-context retrieval-augmented generation, which retrieves code snippets based\\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\\ngeneration on larger models and conclude that, despite its simplicity, the\\napproach is more suitable than using the RETRO architecture. We highlight the\\nkey role of proper tokenization in achieving the full potential of LLMs in code\\ncompletion.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test Interpretation in Clinical Medicine</td>\n",
       "      <td>http://arxiv.org/abs/2409.18986v1</td>\n",
       "      <td>Accurate interpretation of lab results is crucial in clinical medicine, yet\\nmost patient portals use universal normal ranges, ignoring factors like age and\\ngender. This study introduces Lab-AI, an interactive system that offers\\npersonalized normal ranges using Retrieval-Augmented Generation (RAG) from\\ncredible health sources. Lab-AI has two modules: factor retrieval and normal\\nrange retrieval. We tested these on 68 lab tests-30 with conditional factors\\nand 38 without. For tests with factors, normal ranges depend on\\npatient-specific information. Our results show that GPT-4-turbo with RAG\\nachieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal\\nrange retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by\\n29.1% in factor retrieval and showed 60.9% and 52.9% improvements in\\nquestion-level and lab-level performance, respectively, for normal range\\nretrieval. These findings highlight Lab-AI's potential to enhance patient\\nunderstanding of lab results.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ARKS: Active Retrieval in Knowledge Soup for Code Generation</td>\n",
       "      <td>http://arxiv.org/abs/2402.12317v1</td>\n",
       "      <td>Recently the retrieval-augmented generation (RAG) paradigm has raised much\\nattention for its potential in incorporating external knowledge into large\\nlanguage models (LLMs) without further training. While widely explored in\\nnatural language applications, its utilization in code generation remains\\nunder-explored. In this paper, we introduce Active Retrieval in Knowledge Soup\\n(ARKS), an advanced strategy for generalizing large language models for code.\\nIn contrast to relying on a single source, we construct a knowledge soup\\nintegrating web search, documentation, execution feedback, and evolved code\\nsnippets. We employ an active retrieval strategy that iteratively refines the\\nquery and updates the knowledge soup. To assess the performance of ARKS, we\\ncompile a new benchmark comprising realistic coding problems associated with\\nfrequently updated libraries and long-tail programming languages. Experimental\\nresults on ChatGPT and CodeLlama demonstrate a substantial improvement in the\\naverage execution accuracy of ARKS on LLMs. The analysis confirms the\\neffectiveness of our proposed knowledge soup and active retrieval strategies,\\noffering rich insights into the construction of effective retrieval-augmented\\ncode generation (RACG) pipelines. Our model, code, and data are available at\\nhttps://arks-codegen.github.io.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Exploring Information Retrieval Landscapes: An Investigation of a Novel Evaluation Techniques and Comparative Document Splitting Methods</td>\n",
       "      <td>http://arxiv.org/abs/2409.08479v2</td>\n",
       "      <td>The performance of Retrieval-Augmented Generation (RAG) systems in\\ninformation retrieval is significantly influenced by the characteristics of the\\ndocuments being processed. In this study, the structured nature of textbooks,\\nthe conciseness of articles, and the narrative complexity of novels are shown\\nto require distinct retrieval strategies. A comparative evaluation of multiple\\ndocument-splitting methods reveals that the Recursive Character Splitter\\noutperforms the Token-based Splitter in preserving contextual integrity. A\\nnovel evaluation technique is introduced, utilizing an open-source model to\\ngenerate a comprehensive dataset of question-and-answer pairs, simulating\\nrealistic retrieval scenarios to enhance testing efficiency and metric\\nreliability. The evaluation employs weighted scoring metrics, including\\nSequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy\\nand relevance. This approach establishes a refined standard for evaluating the\\nprecision of RAG systems, with future research focusing on optimizing chunk and\\noverlap sizes to improve retrieval accuracy and efficiency.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers</td>\n",
       "      <td>http://arxiv.org/abs/2404.07220v2</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\\nprivate knowledge base of documents with Large Language Models (LLM) to build\\nGenerative Q\\&amp;A (Question-Answering) systems. However, RAG accuracy becomes\\nincreasingly challenging as the corpus of documents scales up, with Retrievers\\nplaying an outsized role in the overall RAG accuracy by extracting the most\\nrelevant document from the corpus to provide context to the LLM. In this paper,\\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\\nquery strategies. Our study achieves better retrieval results and sets new\\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\\ndemonstrate far superior results on Generative Q\\&amp;A datasets like SQUAD, even\\nsurpassing fine-tuning performance.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2407.12216v2</td>\n",
       "      <td>Large Language Models (LLMs) are proficient at generating coherent and\\ncontextually relevant text but face challenges when addressing\\nknowledge-intensive queries in domain-specific and factual question-answering\\ntasks. Retrieval-augmented generation (RAG) systems mitigate this by\\nincorporating external knowledge sources, such as structured knowledge graphs\\n(KGs). However, LLMs often struggle to produce accurate answers despite access\\nto KG-extracted information containing necessary facts. Our study investigates\\nthis dilemma by analyzing error patterns in existing KG-based RAG methods and\\nidentifying eight critical failure points. We observed that these errors\\npredominantly occur due to insufficient focus on discerning the question's\\nintent and adequately gathering relevant context from the knowledge graph\\nfacts. Drawing on this analysis, we propose the Mindful-RAG approach, a\\nframework designed for intent-based and contextually aligned knowledge\\nretrieval. This method explicitly targets the identified failures and offers\\nimprovements in the correctness and relevance of responses provided by LLMs,\\nrepresenting a significant step forward from existing methods.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Learning Retrieval Augmentation for Personalized Dialogue Generation</td>\n",
       "      <td>http://arxiv.org/abs/2406.18847v1</td>\n",
       "      <td>Personalized dialogue generation, focusing on generating highly tailored\\nresponses by leveraging persona profiles and dialogue context, has gained\\nsignificant attention in conversational AI applications. However, persona\\nprofiles, a prevalent setting in current personalized dialogue datasets,\\ntypically composed of merely four to five sentences, may not offer\\ncomprehensive descriptions of the persona about the agent, posing a challenge\\nto generate truly personalized dialogues. To handle this problem, we propose\\n$\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for\\n$\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration\\n($\\textbf{LAPDOG}$), which studies the potential of leveraging external\\nknowledge for persona dialogue generation. Specifically, the proposed LAPDOG\\nmodel consists of a story retriever and a dialogue generator. The story\\nretriever uses a given persona profile as queries to retrieve relevant\\ninformation from the story document, which serves as a supplementary context to\\naugment the persona profile. The dialogue generator utilizes both the dialogue\\nhistory and the augmented persona profile to generate personalized responses.\\nFor optimization, we adopt a joint training framework that collaboratively\\nlearns the story retriever and dialogue generator, where the story retriever is\\noptimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for\\nthe dialogue generator to generate personalized responses. Experiments\\nconducted on the CONVAI2 dataset with ROCStory as a supplementary data source\\nshow that the proposed LAPDOG method substantially outperforms the baselines,\\nindicating the effectiveness of the proposed method. The LAPDOG model code is\\npublicly available for further exploration.\\nhttps://github.com/hqsiswiliam/LAPDOG</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2406.00456v1</td>\n",
       "      <td>Integrating information from different reference data sources is a major\\nchallenge for Retrieval-Augmented Generation (RAG) systems because each\\nknowledge source adopts a unique data structure and follows different\\nconventions. Retrieving from multiple knowledge sources with one fixed strategy\\nusually leads to under-exploitation of information. To mitigate this drawback,\\ninspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that\\ndynamically determines the optimal granularity of a knowledge database based on\\ninput queries using a router. The router is efficiently trained with a newly\\nproposed loss function employing soft labels. We further extend MoG to\\nMix-of-Granularity-Graph (MoGG), where reference documents are pre-processed\\ninto graphs, enabling the retrieval of relevant information from distantly\\nsituated chunks. Extensive experiments demonstrate that both MoG and MoGG\\neffectively predict optimal granularity levels, significantly enhancing the\\nperformance of the RAG system in downstream tasks. The code of both MoG and\\nMoGG will be made public.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Boosting legal case retrieval by query content selection with large language models</td>\n",
       "      <td>http://arxiv.org/abs/2312.03494v1</td>\n",
       "      <td>Legal case retrieval, which aims to retrieve relevant cases to a given query\\ncase, benefits judgment justice and attracts increasing attention. Unlike\\ngeneric retrieval queries, legal case queries are typically long and the\\ndefinition of relevance is closely related to legal-specific elements.\\nTherefore, legal case queries may suffer from noise and sparsity of salient\\ncontent, which hinders retrieval models from perceiving correct information in\\na query. While previous studies have paid attention to improving retrieval\\nmodels and understanding relevance judgments, we focus on enhancing legal case\\nretrieval by utilizing the salient content in legal case queries. We first\\nannotate the salient content in queries manually and investigate how sparse and\\ndense retrieval models attend to those content. Then we experiment with various\\nquery content selection methods utilizing large language models (LLMs) to\\nextract or summarize salient content and incorporate it into the retrieval\\nmodels. Experimental results show that reformulating long queries using LLMs\\nimproves the performance of both sparse and dense models in legal case\\nretrieval.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        title  \\\n",
       "1                                                                  RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems   \n",
       "3                                                     Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models   \n",
       "6                                                                          A Multi-Source Retrieval Question Answering Framework Based on RAG   \n",
       "8                                                          Improving Retrieval for RAG based Question Answering Models on Financial Documents   \n",
       "16                                                            MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries   \n",
       "17                                                                   Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG   \n",
       "38                                       Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems   \n",
       "41                                               Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In   \n",
       "43                                                         Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation   \n",
       "48                                                        QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs   \n",
       "52                                                         Retrieval-augmented code completion for local projects using large language models   \n",
       "53                                 Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test Interpretation in Clinical Medicine   \n",
       "55                                                                               ARKS: Active Retrieval in Knowledge Soup for Code Generation   \n",
       "65   Exploring Information Retrieval Landscapes: An Investigation of a Novel Evaluation Techniques and Comparative Document Splitting Methods   \n",
       "76                Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers   \n",
       "116                                                               Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation   \n",
       "147                                                                      Learning Retrieval Augmentation for Personalized Dialogue Generation   \n",
       "156                                                  Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation   \n",
       "175                                                       Boosting legal case retrieval by query content selection with large language models   \n",
       "\n",
       "                              entry_id  \\\n",
       "1    http://arxiv.org/abs/2407.11005v1   \n",
       "3    http://arxiv.org/abs/2410.01782v1   \n",
       "6    http://arxiv.org/abs/2405.19207v1   \n",
       "8    http://arxiv.org/abs/2404.07221v2   \n",
       "16   http://arxiv.org/abs/2401.15391v1   \n",
       "17   http://arxiv.org/abs/2410.05983v1   \n",
       "38   http://arxiv.org/abs/2409.19804v1   \n",
       "41   http://arxiv.org/abs/2305.17331v1   \n",
       "43   http://arxiv.org/abs/2408.03623v1   \n",
       "48   http://arxiv.org/abs/2406.14277v2   \n",
       "52   http://arxiv.org/abs/2408.05026v1   \n",
       "53   http://arxiv.org/abs/2409.18986v1   \n",
       "55   http://arxiv.org/abs/2402.12317v1   \n",
       "65   http://arxiv.org/abs/2409.08479v2   \n",
       "76   http://arxiv.org/abs/2404.07220v2   \n",
       "116  http://arxiv.org/abs/2407.12216v2   \n",
       "147  http://arxiv.org/abs/2406.18847v1   \n",
       "156  http://arxiv.org/abs/2406.00456v1   \n",
       "175  http://arxiv.org/abs/2312.03494v1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               summary  \\\n",
       "1                                                                                                                                                                                                              Retrieval-Augmented Generation (RAG) has become a standard architectural\\npattern for incorporating domain-specific knowledge into user-facing chat\\napplications powered by Large Language Models (LLMs). RAG systems are\\ncharacterized by (1) a document retriever that queries a domain-specific corpus\\nfor context information relevant to an input query, and (2) an LLM that\\ngenerates a response based on the provided query and context. However,\\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\\nunified evaluation criteria and annotated datasets. In response, we introduce\\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\\nexamples. It covers five unique industry-specific domains and various RAG task\\ntypes. RAGBench examples are sourced from industry corpora such as user\\nmanuals, making it particularly relevant for industry applications. Further, we\\nformalize the TRACe evaluation framework: a set of explainable and actionable\\nRAG evaluation metrics applicable across all RAG domains. We release the\\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\\nenabling actionable feedback for continuous improvement of production\\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\\nRAG evaluation task. We identify areas where existing approaches fall short and\\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\\nevaluation systems.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\\naccuracy of Large Language Models (LLMs), but existing methods often suffer\\nfrom limited reasoning capabilities in effectively using the retrieved\\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\\ncapabilities in RAG with open-source LLMs. Our framework transforms an\\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\\nmodel capable of handling complex reasoning tasks, including both single- and\\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\\ndistractors that appear relevant but are misleading. As a result, Open-RAG\\nleverages latent learning, dynamically selecting relevant experts and\\nintegrating external knowledge effectively for more accurate and contextually\\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\\nto determine retrieval necessity and balance the trade-off between performance\\ngain and inference speed. Experimental results show that the Llama2-7B-based\\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\\nour code and models at https://openragmoe.github.io/   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        With the rapid development of large-scale language models,\\nRetrieval-Augmented Generation (RAG) has been widely adopted. However, existing\\nRAG paradigms are inevitably influenced by erroneous retrieval information,\\nthereby reducing the reliability and correctness of generated results.\\nTherefore, to improve the relevance of retrieval information, this study\\nproposes a method that replaces traditional retrievers with GPT-3.5, leveraging\\nits vast corpus knowledge to generate retrieval information. We also propose a\\nweb retrieval based method to implement fine-grained knowledge retrieval,\\nUtilizing the powerful reasoning capability of GPT-3.5 to realize semantic\\npartitioning of problem.In order to mitigate the illusion of GPT retrieval and\\nreduce noise in Web retrieval,we proposes a multi-source retrieval framework,\\nnamed MSRAG, which combines GPT retrieval with web retrieval. Experiments on\\nmultiple knowledge-intensive QA datasets demonstrate that the proposed\\nframework in this study performs better than existing RAG framework in\\nenhancing the overall efficiency and accuracy of QA systems.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The effectiveness of Large Language Models (LLMs) in generating accurate\\nresponses relies heavily on the quality of input provided, particularly when\\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\\nsignificant advancements in LLMs' response quality in recent years, users may\\nstill encounter inaccuracies or irrelevant answers; these issues often stem\\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\\nthe RAG process. This paper explores the existing constraints of RAG pipelines\\nand introduces methodologies for enhancing text retrieval. It delves into\\nstrategies such as sophisticated chunking techniques, query expansion, the\\nincorporation of metadata annotations, the application of re-ranking\\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\\napproaches can substantially improve the retrieval quality, thereby elevating\\nthe overall performance and reliability of LLMs in processing and responding to\\nqueries.   \n",
       "16                                                                                                                                                                              Retrieval-augmented generation (RAG) augments large language models (LLM) by\\nretrieving relevant knowledge, showing promising potential in mitigating LLM\\nhallucinations and enhancing response quality, thereby facilitating the great\\nadoption of LLMs in practice. However, we find that existing RAG systems are\\ninadequate in answering multi-hop queries, which require retrieving and\\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\\nknowledge base, a large collection of multi-hop queries, their ground-truth\\nanswers, and the associated supporting evidence. We detail the procedure of\\nbuilding the dataset, utilizing an English news article dataset as the\\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\\nMultiHop-RAG in two experiments. The first experiment compares different\\nembedding models for retrieving evidence for multi-hop queries. In the second\\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\\nqueries given the evidence. Both experiments reveal that existing RAG methods\\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\\nMultiHop-RAG will be a valuable resource for the community in developing\\neffective RAG systems, thereby facilitating greater adoption of LLMs in\\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\\nhttps://github.com/yixuantt/MultiHop-RAG/.   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                         Retrieval-augmented generation (RAG) empowers large language models (LLMs) to\\nutilize external knowledge sources. The increasing capacity of LLMs to process\\nlonger input sequences opens up avenues for providing more retrieved\\ninformation, to potentially enhance the quality of generated outputs. It is\\nplausible to assume that a larger retrieval set would contain more relevant\\ninformation (higher recall), that might result in improved performance.\\nHowever, our empirical findings demonstrate that for many long-context LLMs,\\nthe quality of generated output initially improves first, but then subsequently\\ndeclines as the number of retrieved passages increases. This paper investigates\\nthis phenomenon, identifying the detrimental impact of retrieved \"hard\\nnegatives\" as a key contributor. To mitigate this and enhance the robustness of\\nlong-context LLM-based RAG, we propose both training-free and training-based\\napproaches. We first showcase the effectiveness of retrieval reordering as a\\nsimple yet powerful training-free optimization. Furthermore, we explore\\ntraining-based methods, specifically RAG-specific implicit LLM fine-tuning and\\nRAG-oriented fine-tuning with intermediate reasoning, demonstrating their\\ncapacity for substantial performance gains. Finally, we conduct a systematic\\nanalysis of design choices for these training-based methods, including data\\ndistribution, retriever selection, and training context length.   \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         RAG (Retrieval-Augmented Generation) have recently gained significant\\nattention for their enhanced ability to integrate external knowledge sources in\\nopen-domain question answering (QA) tasks. However, it remains unclear how\\nthese models address fairness concerns, particularly with respect to sensitive\\nattributes such as gender, geographic location, and other demographic factors.\\nFirst, as language models evolve to prioritize utility, like improving exact\\nmatch accuracy, fairness may have been largely overlooked. Second, RAG methods\\nare complex pipelines, making it hard to identify and address biases, as each\\ncomponent is optimized for different goals. In this paper, we aim to\\nempirically evaluate fairness in several RAG methods. We propose a fairness\\nevaluation framework tailored to RAG methods, using scenario-based questions\\nand analyzing disparities across demographic attributes. The experimental\\nresults indicate that, despite recent advances in utility-driven optimization,\\nfairness issues persist in both the retrieval and generation stages,\\nhighlighting the need for more targeted fairness interventions within RAG\\npipelines. We will release our dataset and code upon acceptance of the paper.   \n",
       "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.   \n",
       "43                                                                                              Code comment generation aims to generate high-quality comments from source\\ncode automatically and has been studied for years. Recent studies proposed to\\nintegrate information retrieval techniques with neural generation models to\\ntackle this problem, i.e., Retrieval-Augmented Comment Generation (RACG)\\napproaches, and achieved state-of-the-art results. However, the retrievers in\\nprevious work are built independently of their generators. This results in that\\nthe retrieved exemplars are not necessarily the most useful ones for generating\\ncomments, limiting the performance of existing approaches. To address this\\nlimitation, we propose a novel training strategy to enable the retriever to\\nlearn from the feedback of the generator and retrieve exemplars for generation.\\nSpecifically, during training, we use the retriever to retrieve the top-k\\nexemplars and calculate their retrieval scores, and use the generator to\\ncalculate a generation loss for the sample based on each exemplar. By aligning\\nhigh-score exemplars retrieved by the retriever with low-loss exemplars\\nobserved by the generator, the retriever can learn to retrieve exemplars that\\ncan best improve the quality of the generated comments. Based on this strategy,\\nwe propose a novel RACG approach named JOINTCOM and evaluate it on two\\nreal-world datasets, JCSD and PCSD. The experimental results demonstrate that\\nour approach surpasses the state-of-the-art baselines by 7.3% to 30.0% in terms\\nof five metrics on the two datasets. We also conduct a human evaluation to\\ncompare JOINTCOM with the best-performing baselines. The results indicate that\\nJOINTCOM outperforms the baselines, producing comments that are more natural,\\ninformative, and useful.   \n",
       "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Retrieval-augmented generation (RAG) has received much attention for\\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\\nparametric knowledge of large language models (LLMs). While previous approaches\\nfocused on processing retrieved passages to remove irrelevant context, they\\nstill rely heavily on the quality of retrieved passages which can degrade if\\nthe question is ambiguous or complex. In this paper, we propose a simple yet\\nefficient method called question and passage augmentation (QPaug) via LLMs for\\nopen-domain QA. QPaug first decomposes the original questions into\\nmultiple-step sub-questions. By augmenting the original question with detailed\\nsub-questions and planning, we are able to make the query more specific on what\\nneeds to be retrieved, improving the retrieval performance. In addition, to\\ncompensate for the case where the retrieved passages contain distracting\\ninformation or divided opinions, we augment the retrieved passages with\\nself-generated passages by LLMs to guide the answer extraction. Experimental\\nresults show that QPaug outperforms the previous state-of-the-art and achieves\\nsignificant performance gain over existing RAG methods. The source code is\\navailable at \\url{https://github.com/kmswin1/QPaug}.   \n",
       "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The use of large language models (LLMs) is becoming increasingly widespread\\namong software developers. However, privacy and computational requirements are\\nproblematic with commercial solutions and the use of LLMs. In this work, we\\nfocus on using LLMs with around 160 million parameters that are suitable for\\nlocal execution and augmentation with retrieval from local projects. We train\\ntwo models based on the transformer architecture, the generative model GPT-2\\nand the retrieval-adapted RETRO model, on open-source Python files, and\\nempirically evaluate and compare them, confirming the benefits of vector\\nembedding based retrieval. Further, we improve our models' performance with\\nIn-context retrieval-augmented generation, which retrieves code snippets based\\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\\ngeneration on larger models and conclude that, despite its simplicity, the\\napproach is more suitable than using the RETRO architecture. We highlight the\\nkey role of proper tokenization in achieving the full potential of LLMs in code\\ncompletion.   \n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Accurate interpretation of lab results is crucial in clinical medicine, yet\\nmost patient portals use universal normal ranges, ignoring factors like age and\\ngender. This study introduces Lab-AI, an interactive system that offers\\npersonalized normal ranges using Retrieval-Augmented Generation (RAG) from\\ncredible health sources. Lab-AI has two modules: factor retrieval and normal\\nrange retrieval. We tested these on 68 lab tests-30 with conditional factors\\nand 38 without. For tests with factors, normal ranges depend on\\npatient-specific information. Our results show that GPT-4-turbo with RAG\\nachieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal\\nrange retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by\\n29.1% in factor retrieval and showed 60.9% and 52.9% improvements in\\nquestion-level and lab-level performance, respectively, for normal range\\nretrieval. These findings highlight Lab-AI's potential to enhance patient\\nunderstanding of lab results.   \n",
       "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Recently the retrieval-augmented generation (RAG) paradigm has raised much\\nattention for its potential in incorporating external knowledge into large\\nlanguage models (LLMs) without further training. While widely explored in\\nnatural language applications, its utilization in code generation remains\\nunder-explored. In this paper, we introduce Active Retrieval in Knowledge Soup\\n(ARKS), an advanced strategy for generalizing large language models for code.\\nIn contrast to relying on a single source, we construct a knowledge soup\\nintegrating web search, documentation, execution feedback, and evolved code\\nsnippets. We employ an active retrieval strategy that iteratively refines the\\nquery and updates the knowledge soup. To assess the performance of ARKS, we\\ncompile a new benchmark comprising realistic coding problems associated with\\nfrequently updated libraries and long-tail programming languages. Experimental\\nresults on ChatGPT and CodeLlama demonstrate a substantial improvement in the\\naverage execution accuracy of ARKS on LLMs. The analysis confirms the\\neffectiveness of our proposed knowledge soup and active retrieval strategies,\\noffering rich insights into the construction of effective retrieval-augmented\\ncode generation (RACG) pipelines. Our model, code, and data are available at\\nhttps://arks-codegen.github.io.   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The performance of Retrieval-Augmented Generation (RAG) systems in\\ninformation retrieval is significantly influenced by the characteristics of the\\ndocuments being processed. In this study, the structured nature of textbooks,\\nthe conciseness of articles, and the narrative complexity of novels are shown\\nto require distinct retrieval strategies. A comparative evaluation of multiple\\ndocument-splitting methods reveals that the Recursive Character Splitter\\noutperforms the Token-based Splitter in preserving contextual integrity. A\\nnovel evaluation technique is introduced, utilizing an open-source model to\\ngenerate a comprehensive dataset of question-and-answer pairs, simulating\\nrealistic retrieval scenarios to enhance testing efficiency and metric\\nreliability. The evaluation employs weighted scoring metrics, including\\nSequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy\\nand relevance. This approach establishes a refined standard for evaluating the\\nprecision of RAG systems, with future research focusing on optimizing chunk and\\noverlap sizes to improve retrieval accuracy and efficiency.   \n",
       "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\\nprivate knowledge base of documents with Large Language Models (LLM) to build\\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\\nincreasingly challenging as the corpus of documents scales up, with Retrievers\\nplaying an outsized role in the overall RAG accuracy by extracting the most\\nrelevant document from the corpus to provide context to the LLM. In this paper,\\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\\nquery strategies. Our study achieves better retrieval results and sets new\\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\\nsurpassing fine-tuning performance.   \n",
       "116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Large Language Models (LLMs) are proficient at generating coherent and\\ncontextually relevant text but face challenges when addressing\\nknowledge-intensive queries in domain-specific and factual question-answering\\ntasks. Retrieval-augmented generation (RAG) systems mitigate this by\\nincorporating external knowledge sources, such as structured knowledge graphs\\n(KGs). However, LLMs often struggle to produce accurate answers despite access\\nto KG-extracted information containing necessary facts. Our study investigates\\nthis dilemma by analyzing error patterns in existing KG-based RAG methods and\\nidentifying eight critical failure points. We observed that these errors\\npredominantly occur due to insufficient focus on discerning the question's\\nintent and adequately gathering relevant context from the knowledge graph\\nfacts. Drawing on this analysis, we propose the Mindful-RAG approach, a\\nframework designed for intent-based and contextually aligned knowledge\\nretrieval. This method explicitly targets the identified failures and offers\\nimprovements in the correctness and relevance of responses provided by LLMs,\\nrepresenting a significant step forward from existing methods.   \n",
       "147  Personalized dialogue generation, focusing on generating highly tailored\\nresponses by leveraging persona profiles and dialogue context, has gained\\nsignificant attention in conversational AI applications. However, persona\\nprofiles, a prevalent setting in current personalized dialogue datasets,\\ntypically composed of merely four to five sentences, may not offer\\ncomprehensive descriptions of the persona about the agent, posing a challenge\\nto generate truly personalized dialogues. To handle this problem, we propose\\n$\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for\\n$\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration\\n($\\textbf{LAPDOG}$), which studies the potential of leveraging external\\nknowledge for persona dialogue generation. Specifically, the proposed LAPDOG\\nmodel consists of a story retriever and a dialogue generator. The story\\nretriever uses a given persona profile as queries to retrieve relevant\\ninformation from the story document, which serves as a supplementary context to\\naugment the persona profile. The dialogue generator utilizes both the dialogue\\nhistory and the augmented persona profile to generate personalized responses.\\nFor optimization, we adopt a joint training framework that collaboratively\\nlearns the story retriever and dialogue generator, where the story retriever is\\noptimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for\\nthe dialogue generator to generate personalized responses. Experiments\\nconducted on the CONVAI2 dataset with ROCStory as a supplementary data source\\nshow that the proposed LAPDOG method substantially outperforms the baselines,\\nindicating the effectiveness of the proposed method. The LAPDOG model code is\\npublicly available for further exploration.\\nhttps://github.com/hqsiswiliam/LAPDOG   \n",
       "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Integrating information from different reference data sources is a major\\nchallenge for Retrieval-Augmented Generation (RAG) systems because each\\nknowledge source adopts a unique data structure and follows different\\nconventions. Retrieving from multiple knowledge sources with one fixed strategy\\nusually leads to under-exploitation of information. To mitigate this drawback,\\ninspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that\\ndynamically determines the optimal granularity of a knowledge database based on\\ninput queries using a router. The router is efficiently trained with a newly\\nproposed loss function employing soft labels. We further extend MoG to\\nMix-of-Granularity-Graph (MoGG), where reference documents are pre-processed\\ninto graphs, enabling the retrieval of relevant information from distantly\\nsituated chunks. Extensive experiments demonstrate that both MoG and MoGG\\neffectively predict optimal granularity levels, significantly enhancing the\\nperformance of the RAG system in downstream tasks. The code of both MoG and\\nMoGG will be made public.   \n",
       "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Legal case retrieval, which aims to retrieve relevant cases to a given query\\ncase, benefits judgment justice and attracts increasing attention. Unlike\\ngeneric retrieval queries, legal case queries are typically long and the\\ndefinition of relevance is closely related to legal-specific elements.\\nTherefore, legal case queries may suffer from noise and sparsity of salient\\ncontent, which hinders retrieval models from perceiving correct information in\\na query. While previous studies have paid attention to improving retrieval\\nmodels and understanding relevance judgments, we focus on enhancing legal case\\nretrieval by utilizing the salient content in legal case queries. We first\\nannotate the salient content in queries manually and investigate how sparse and\\ndense retrieval models attend to those content. Then we experiment with various\\nquery content selection methods utilizing large language models (LLMs) to\\nextract or summarize salient content and incorporate it into the retrieval\\nmodels. Experimental results show that reformulating long queries using LLMs\\nimproves the performance of both sparse and dense models in legal case\\nretrieval.   \n",
       "\n",
       "     is_relevant  \n",
       "1            5.0  \n",
       "3            5.0  \n",
       "6            5.0  \n",
       "8            5.0  \n",
       "16           5.0  \n",
       "17           5.0  \n",
       "38           5.0  \n",
       "41           5.0  \n",
       "43           5.0  \n",
       "48           5.0  \n",
       "52           5.0  \n",
       "53           5.0  \n",
       "55           5.0  \n",
       "65           5.0  \n",
       "76           5.0  \n",
       "116          5.0  \n",
       "147          5.0  \n",
       "156          5.0  \n",
       "175          5.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arixv_result_df.query('is_direct == True')\\\n",
    "    [['title','entry_id', 'summary', 'is_relevant']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
