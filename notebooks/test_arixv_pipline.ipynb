{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.arxiv_search import expand_arxiv_query, run_multi_arixv_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"\"\"RAG(retrieval augmented generation) sometimes the sources retrieved are relevant but not enough to answer the question user ask. How do research approach this case\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"\"\"reduce hallucination in RAG(retrieval augmented generation)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = expand_arxiv_query(problem_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAG model hallucination reduction techniques',\n",
       " 'Reducing hallucination in Retrieval Augmented Generation',\n",
       " 'Methods to minimize hallucination in RAG models',\n",
       " 'RAG model accuracy improvement strategies',\n",
       " 'Techniques for efficient and accurate RAG models',\n",
       " 'Overcoming hallucination in RAG models',\n",
       " 'Mitigating hallucination in Retrieval Augmented Generation',\n",
       " 'Accuracy enhancement in RAG models',\n",
       " 'Approaches to reduce hallucination in RAG',\n",
       " 'Solutions for hallucination in Retrieval Augmented Generation']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "arxiv_query_results = run_multi_arixv_queries(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_query_results.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten arxiv_query_results\n",
    "all_results = []\n",
    "for query in arxiv_query_results:\n",
    "    for result in arxiv_query_results[query]:\n",
    "        all_results.append(result)\n",
    "\n",
    "# make arxiv query results a dataframe and create a new dataframe with only unique entry_id\n",
    "import pandas as pd\n",
    "\n",
    "arixv_result_df = pd.DataFrame(all_results)\n",
    "unique_arixv_result_df = arixv_result_df.drop_duplicates(subset='entry_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 13), (116, 13))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.shape, unique_arixv_result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>links</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>updated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.11414v2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2407.12325v1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2411.12759v1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.00396v2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2408.15533v2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2412.04235v1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.13085v1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2410.18251v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2408.00555v1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2407.19994v3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   authors  categories  comment  doi  \\\n",
       "entry_id                                                               \n",
       "http://arxiv.org/abs/2410.11414v2        7           7        7    0   \n",
       "http://arxiv.org/abs/2407.12325v1        7           7        0    0   \n",
       "http://arxiv.org/abs/2411.12759v1        6           6        0    0   \n",
       "http://arxiv.org/abs/2401.00396v2        5           5        0    0   \n",
       "http://arxiv.org/abs/2408.15533v2        5           5        0    0   \n",
       "http://arxiv.org/abs/2412.04235v1        4           4        0    0   \n",
       "http://arxiv.org/abs/2410.13085v1        4           4        0    0   \n",
       "http://arxiv.org/abs/2410.18251v1        3           3        3    0   \n",
       "http://arxiv.org/abs/2408.00555v1        3           3        0    0   \n",
       "http://arxiv.org/abs/2407.19994v3        3           3        0    3   \n",
       "\n",
       "                                   journal_ref  links  pdf_url  \\\n",
       "entry_id                                                         \n",
       "http://arxiv.org/abs/2410.11414v2            0      7        7   \n",
       "http://arxiv.org/abs/2407.12325v1            0      7        7   \n",
       "http://arxiv.org/abs/2411.12759v1            0      6        6   \n",
       "http://arxiv.org/abs/2401.00396v2            0      5        5   \n",
       "http://arxiv.org/abs/2408.15533v2            0      5        5   \n",
       "http://arxiv.org/abs/2412.04235v1            0      4        4   \n",
       "http://arxiv.org/abs/2410.13085v1            0      4        4   \n",
       "http://arxiv.org/abs/2410.18251v1            0      3        3   \n",
       "http://arxiv.org/abs/2408.00555v1            0      3        3   \n",
       "http://arxiv.org/abs/2407.19994v3            3      3        3   \n",
       "\n",
       "                                   primary_category  published  summary  \\\n",
       "entry_id                                                                  \n",
       "http://arxiv.org/abs/2410.11414v2                 7          7        7   \n",
       "http://arxiv.org/abs/2407.12325v1                 7          7        7   \n",
       "http://arxiv.org/abs/2411.12759v1                 6          6        6   \n",
       "http://arxiv.org/abs/2401.00396v2                 5          5        5   \n",
       "http://arxiv.org/abs/2408.15533v2                 5          5        5   \n",
       "http://arxiv.org/abs/2412.04235v1                 4          4        4   \n",
       "http://arxiv.org/abs/2410.13085v1                 4          4        4   \n",
       "http://arxiv.org/abs/2410.18251v1                 3          3        3   \n",
       "http://arxiv.org/abs/2408.00555v1                 3          3        3   \n",
       "http://arxiv.org/abs/2407.19994v3                 3          3        3   \n",
       "\n",
       "                                   title  updated  \n",
       "entry_id                                           \n",
       "http://arxiv.org/abs/2410.11414v2      7        7  \n",
       "http://arxiv.org/abs/2407.12325v1      7        7  \n",
       "http://arxiv.org/abs/2411.12759v1      6        6  \n",
       "http://arxiv.org/abs/2401.00396v2      5        5  \n",
       "http://arxiv.org/abs/2408.15533v2      5        5  \n",
       "http://arxiv.org/abs/2412.04235v1      4        4  \n",
       "http://arxiv.org/abs/2410.13085v1      4        4  \n",
       "http://arxiv.org/abs/2410.18251v1      3        3  \n",
       "http://arxiv.org/abs/2408.00555v1      3        3  \n",
       "http://arxiv.org/abs/2407.19994v3      3        3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arixv_result_df.groupby('entry_id').count().sort_values('title', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.prompts.content_curation import create_arxiv_filtering_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_arxiv_filtering_prompt in module askharrison.prompts.content_curation:\n",
      "\n",
      "create_arxiv_filtering_prompt(problem_statement: str, doc_abstract: str)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(create_arxiv_filtering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt for each arxiv entry\n",
    "arxiv_reranking_prompts = [create_arxiv_filtering_prompt(problem_statement, \n",
    "                                         record['title']+\"\\n\"+record['summary']) for record in unique_arixv_result_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from askharrison.llm_models import parallel_llm_processor, process_question, safe_eval, extract_python_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 116/116 [00:57<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "reranking_llm_response = parallel_llm_processor(arxiv_reranking_prompts, llm_function=process_question, \n",
    "                                                max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116, 13), 116)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_responses_results = [safe_eval(extract_python_code(response)) for response in reranking_llm_response]\n",
    "# filter out empty responses\n",
    "\n",
    "unique_arixv_result_df.shape, len(llm_responses_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
      "C:\\Users\\alist\\AppData\\Local\\Temp\\ipykernel_103088\\658567496.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
     ]
    }
   ],
   "source": [
    "# extract reasoning, is_direct, 'is_relevant' from llm_responses_results if it is not empty, and add to unique_arixv_result_df\n",
    "unique_arixv_result_df['reasoning'] = [response['reasoning'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_direct'] = [response['is_direct'] if response else None for response in llm_responses_results]\n",
    "unique_arixv_result_df['is_relevant'] = [response['is_relevant'] if response else None for response in llm_responses_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase max column width in pandas\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arixv_result_df.query('is_direct == True').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots</td>\n",
       "      <td>http://arxiv.org/abs/2412.04235v1</td>\n",
       "      <td>I combine detection and mitigation techniques to addresses hallucinations in\\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\\nintroducing the Negative Missing Information Scoring System (NMISS), which\\naccounts for contextual relevance in responses. While RAG mitigates\\nhallucinations by grounding answers in external data, NMISS refines the\\nevaluation by identifying cases where traditional metrics incorrectly flag\\ncontextually accurate responses as hallucinations. I use Italian health news\\narticles as context to evaluate LLM performance. Results show that Gemma2 and\\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\\nbenefit significantly from NMISS, highlighting their ability to provide richer\\ncontextual information. This combined approach offers new insights into the\\nreduction and more accurate assessment of hallucinations in LLMs, with\\napplications in real-world healthcare tasks and other domains.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost</td>\n",
       "      <td>http://arxiv.org/abs/2406.00975v2</td>\n",
       "      <td>Retriever Augmented Generation (RAG) systems have become pivotal in enhancing\\nthe capabilities of language models by incorporating external knowledge\\nretrieval mechanisms. However, a significant challenge in deploying these\\nsystems in industry applications is the detection and mitigation of\\nhallucinations: instances where the model generates information that is not\\ngrounded in the retrieved context. Addressing this issue is crucial for\\nensuring the reliability and accuracy of responses generated by large language\\nmodels (LLMs) in diverse industry settings. Current hallucination detection\\ntechniques fail to deliver accuracy, low latency, and low cost simultaneously.\\nWe introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination\\ndetection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and\\ncommercial evaluation frameworks on the hallucination detection task, with 97%\\nand 91% reduction in cost and latency, respectively. Luna is lightweight and\\ngeneralizes across multiple industry verticals and out-of-domain data, making\\nit an ideal candidate for industry LLM applications.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation</td>\n",
       "      <td>http://arxiv.org/abs/2408.15533v2</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) has become a primary technique for\\nmitigating hallucinations in large language models (LLMs). However, incomplete\\nknowledge extraction and insufficient understanding can still mislead LLMs to\\nproduce irrelevant or even contradictory responses, which means hallucinations\\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\\ninput and output of the RAG generator. We then apply further extraction and\\nresampling to the relevance matrix. The processed relevance data are input into\\nmultiple classifiers to determine whether the output contains hallucinations.\\nTo the best of our knowledge, this is the first time that LRP has been used for\\ndetecting RAG hallucinations, and extensive experiments demonstrate that\\nLRP4RAG outperforms existing baselines.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed articles</td>\n",
       "      <td>http://arxiv.org/abs/2412.16701v1</td>\n",
       "      <td>Recent advancements in generative AI have flourished the development of\\nhighly adept Large Language Models (LLMs) that integrate diverse data types to\\nempower decision-making. Among these, Multimodal Retrieval-Augmented Generation\\n(RAG) applications are promising for their capability to combine the strengths\\nof information retrieval and generative models, enhancing their utility across\\nvarious domains, including biomedical research. This paper introduces\\nAlzheimerRAG, a Multimodal RAG pipeline tool for biomedical research use cases,\\nprimarily focusing on Alzheimer's disease from PubMed articles. Our pipeline\\nincorporates multimodal fusion techniques to integrate textual and visual data\\nprocessing by efficiently indexing and accessing vast amounts of biomedical\\nliterature. Preliminary experimental results against benchmarks, such as BioASQ\\nand PubMedQA, have returned improved results in information retrieval and\\nsynthesis of domain-specific information. We also demonstrate a case study with\\nour RAG pipeline across different Alzheimer's clinical scenarios. We infer that\\nAlzheimerRAG can generate responses with accuracy non-inferior to humans and\\nwith low rates of hallucination. Overall, a reduction in cognitive task load is\\nobserved, which allows researchers to gain multimodal insights, improving\\nunderstanding and treatment of Alzheimer's disease.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Searching for Best Practices in Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2407.01219v1</td>\n",
       "      <td>Retrieval-augmented generation (RAG) techniques have proven to be effective\\nin integrating up-to-date information, mitigating hallucinations, and enhancing\\nresponse quality, particularly in specialized domains. While many RAG\\napproaches have been proposed to enhance large language models through\\nquery-dependent retrievals, these approaches still suffer from their complex\\nimplementation and prolonged response times. Typically, a RAG workflow involves\\nmultiple processing steps, each of which can be executed in various ways. Here,\\nwe investigate existing RAG approaches and their potential combinations to\\nidentify optimal RAG practices. Through extensive experiments, we suggest\\nseveral strategies for deploying RAG that balance both performance and\\nefficiency. Moreover, we demonstrate that multimodal retrieval techniques can\\nsignificantly enhance question-answering capabilities about visual inputs and\\naccelerate the generation of multimodal content using a \"retrieval as\\ngeneration\" strategy.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2407.18044v1</td>\n",
       "      <td>Digital health chatbots powered by Large Language Models (LLMs) have the\\npotential to significantly improve personal health management for chronic\\nconditions by providing accessible and on-demand health coaching and\\nquestion-answering. However, these chatbots risk providing unverified and\\ninaccurate information because LLMs generate responses based on patterns\\nlearned from diverse internet data. Retrieval Augmented Generation (RAG) can\\nhelp mitigate hallucinations and inaccuracies in LLM responses by grounding it\\non reliable content. However, efficiently and accurately retrieving most\\nrelevant set of content for real-time user questions remains a challenge. In\\nthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a\\nnovel approach that pre-computes a database of potential queries from a content\\nbase using LLMs. For an incoming patient question, QB-RAG efficiently matches\\nit against this pre-generated query database using vector search, improving\\nalignment between user questions and the content. We establish a theoretical\\nfoundation for QB-RAG and provide a comparative analysis of existing retrieval\\nenhancement techniques for RAG systems. Finally, our empirical evaluation\\ndemonstrates that QB-RAG significantly improves the accuracy of healthcare\\nquestion answering, paving the way for robust and trustworthy LLM applications\\nin digital health.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems</td>\n",
       "      <td>http://arxiv.org/abs/2403.09727v1</td>\n",
       "      <td>The development of generative large language models (G-LLM) opened up new\\nopportunities for the development of new types of knowledge-based systems\\nsimilar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented\\nGeneration (RAG) are the techniques that can be used to implement domain\\nadaptation for the development of G-LLM-based knowledge systems. In our study,\\nusing ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine\\nthe performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2\\nlanguage models. Based on measurements shown on different datasets, we\\ndemonstrate that RAG-based constructions are more efficient than models\\nproduced with FN. We point out that connecting RAG and FN is not trivial,\\nbecause connecting FN models with RAG can cause a decrease in performance.\\nFurthermore, we outline a simple RAG-based architecture which, on average,\\noutperforms the FN models by 16% in terms of the ROGUE score, 15% in the case\\nof the BLEU score, and 53% based on the cosine similarity. This shows the\\nsignificant advantage of RAG over FN in terms of hallucination, which is not\\noffset by the fact that the average 8% better METEOR score of FN models\\nindicates greater creativity compared to RAG.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Retrieval-Augmented Generation for Large Language Models: A Survey</td>\n",
       "      <td>http://arxiv.org/abs/2312.10997v5</td>\n",
       "      <td>Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks</td>\n",
       "      <td>http://arxiv.org/abs/2411.14110v1</td>\n",
       "      <td>While large language models (LLMs) have achieved notable success in\\ngenerative tasks, they still face limitations, such as lacking up-to-date\\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\\nenhances LLM performance by integrating external knowledge bases, providing\\nadditional context which significantly improves accuracy and knowledge\\ncoverage. However, building these external knowledge bases often requires\\nsubstantial resources and may involve sensitive information. In this paper, we\\npropose an agent-based automated privacy attack called RAG-Thief, which can\\nextract a scalable amount of private data from the private database used in RAG\\napplications. We conduct a systematic study on the privacy risks associated\\nwith RAG applications, revealing that the vulnerability of LLMs makes the\\nprivate knowledge bases suffer significant privacy risks. Unlike previous\\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\\nstarts with an initial adversarial query and learns from model responses,\\nprogressively generating new queries to extract as many chunks from the\\nknowledge base as possible. Experimental results show that our RAG-Thief can\\nextract over 70% information from the private knowledge bases within customized\\nRAG applications deployed on local machines and real-world platforms, including\\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\\nvulnerabilities in current RAG applications and underscore the pressing need\\nfor stronger safeguards.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2409.11353v3</td>\n",
       "      <td>Hallucination, the generation of factually incorrect content, is a growing\\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\\nmethods are often isolated and insufficient for domain-specific needs, lacking\\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\\nMitigations and EvaluationS), an integrated framework and library addressing\\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\\nhallucinations in LLMs, featuring automated test set generation, multifaceted\\nbenchmarking, and adaptable mitigation strategies. It automates test set\\ncreation from any corpus, ensuring high data quality, diversity, and\\ncost-efficiency through techniques like batch processing, weighted sampling,\\nand counterfactual validation. THaMES assesses a model's ability to detect and\\nreduce hallucinations across various tasks, including text generation and\\nbinary classification, applying optimal mitigation strategies like In-Context\\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\\nof academic papers, political news, and Wikipedia reveal that commercial models\\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\\nevaluation tasks.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation</td>\n",
       "      <td>http://arxiv.org/abs/2408.00555v1</td>\n",
       "      <td>Despite the remarkable ability of large vision-language models (LVLMs) in\\nimage comprehension, these models frequently generate plausible yet factually\\nincorrect responses, a phenomenon known as hallucination.Recently, in large\\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\\nknowledge resources has been proven as a promising solution to mitigate\\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\\nbehind the widespread applications of LVLM. Moreover, when transferred to\\naugmenting LVLMs, sometimes the hallucination degree of the model is even\\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\\nintroduce a novel framework, the Active Retrieval-Augmented large\\nvision-language model (ARA), specifically designed to address hallucinations by\\nincorporating three critical dimensions: (i) dissecting the retrieval targets\\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\\nmost effective retrieval methods and filtering out the reliable retrieval\\nresults. (iii) timing the retrieval process to coincide with episodes of low\\ncertainty, while circumventing unnecessary retrieval during periods of high\\ncertainty. To assess the capability of our proposed ARA model in reducing\\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\\ncan effectively mitigate the hallucination problem. We hope that this study can\\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\\nfor reducing hallucinations with more effective retrieval and minimal retrieval\\noccurrences.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DelucionQA: Detecting Hallucinations in Domain-specific Question Answering</td>\n",
       "      <td>http://arxiv.org/abs/2312.05200v1</td>\n",
       "      <td>Hallucination is a well-known phenomenon in text generated by large language\\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\\napplication scenarios e.g., summarization, question-answering (QA) etc. For\\napplications requiring high reliability (e.g., customer-facing assistants), the\\npotential existence of hallucination in LLM-generated text is a critical\\nproblem. The amount of hallucination can be reduced by leveraging information\\nretrieval to provide relevant background information to the LLM. However, LLMs\\ncan still generate hallucinatory content for various reasons (e.g.,\\nprioritizing its parametric knowledge over the context, failure to capture the\\nrelevant information from the context, etc.). Detecting hallucinations through\\nautomated methods is thus paramount. To facilitate research in this direction,\\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\\npropose a set of hallucination detection methods to serve as baselines for\\nfuture works from the research community. Analysis and case study are also\\nprovided to share valuable insights on hallucination phenomena in the target\\nscenario.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Reducing hallucination in structured outputs via Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2404.08189v1</td>\n",
       "      <td>A common and fundamental limitation of Generative AI (GenAI) is its\\npropensity to hallucinate. While large language models (LLM) have taken the\\nworld by storm, without eliminating or at least reducing hallucinations,\\nreal-world GenAI systems may face challenges in user adoption. In the process\\nof deploying an enterprise application that produces workflows based on natural\\nlanguage requirements, we devised a system leveraging Retrieval Augmented\\nGeneration (RAG) to greatly improve the quality of the structured output that\\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\\nsystem significantly reduces hallucinations in the output and improves the\\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\\nusing a small, well-trained retriever encoder can reduce the size of the\\naccompanying LLM, thereby making deployments of LLM-based systems less\\nresource-intensive.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Context-Augmented Code Generation Using Programming Knowledge Graphs</td>\n",
       "      <td>http://arxiv.org/abs/2410.18251v1</td>\n",
       "      <td>Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\\nimproved code generation, but, they frequently face difficulties when dealing\\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\\naddresses this issue by retrieving and integrating external knowledge at the\\ninference time. However, retrieval models often fail to find most relevant\\ncontext, and generation models, with limited context capacity, can hallucinate\\nwhen given irrelevant data. We present a novel framework that leverages a\\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\\nThis approach enables fine-grained code retrieval by focusing on the most\\nrelevant segments while reducing irrelevant context through a tree-pruning\\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\\nhallucinations by selectively integrating non-RAG solutions. We propose two\\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\\ntree pruning to enhance retrieval precision, a re-ranking method for robust\\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\\ncode augmentation with relevant comments and docstrings.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mitigating Entity-Level Hallucination in Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2407.09417v2</td>\n",
       "      <td>The emergence of Large Language Models (LLMs) has revolutionized how users\\naccess information, shifting from traditional search engines to direct\\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\\ngenerate coherent yet factually inaccurate responses. This hallucination\\nphenomenon has led to users' distrust in information retrieval systems based on\\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\\nAugmentation based on hallucination Detection (DRAD) as a novel method to\\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\\nretrieval augmentation by dynamically adapting the retrieval process based on\\nreal-time hallucination detection. It features two main components: Real-time\\nHallucination Detection (RHD) for identifying potential hallucinations without\\nexternal models, and Self-correction based on External Knowledge (SEK) for\\ncorrecting these errors using external knowledge. Experiment results show that\\nDRAD demonstrates superior performance in both detecting and mitigating\\nhallucinations in LLMs. All of our code and data are open-sourced at\\nhttps://github.com/oneal2000/EntityHallucination.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Context Tuning for Retrieval Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2312.05708v1</td>\n",
       "      <td>Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval</td>\n",
       "      <td>http://arxiv.org/abs/2405.06545v1</td>\n",
       "      <td>Large language models (LLMs) have demonstrated remarkable capabilities across\\nvarious domains, although their susceptibility to hallucination poses\\nsignificant challenges for their deployment in critical areas such as\\nhealthcare. To address this issue, retrieving relevant facts from knowledge\\ngraphs (KGs) is considered a promising method. Existing KG-augmented approaches\\ntend to be resource-intensive, requiring multiple rounds of retrieval and\\nverification for each factoid, which impedes their application in real-world\\nscenarios.\\n  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval\\n(Re-KGR) to augment the factuality of LLMs' responses with less retrieval\\nefforts in the medical field. Our approach leverages the attribution of\\nnext-token predictive probability distributions across different tokens, and\\nvarious model layers to primarily identify tokens with a high potential for\\nhallucination, reducing verification rounds by refining knowledge triples\\nassociated with these tokens. Moreover, we rectify inaccurate content using\\nretrieved knowledge in the post-processing stage, which improves the\\ntruthfulness of generated responses. Experimental results on a medical dataset\\ndemonstrate that our approach can enhance the factual capability of LLMs across\\nvarious foundational models as evidenced by the highest scores on truthfulness.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models</td>\n",
       "      <td>http://arxiv.org/abs/2410.15116v1</td>\n",
       "      <td>Generation of plausible but incorrect factual information, often termed\\nhallucination, has attracted significant research interest. Retrieval-augmented\\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\\nemerges as a promising method to reduce hallucination. However, existing RALMs\\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\\naddress this challenge, we propose COFT, a novel\\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\\nknowledge graph to extract potential key entities in a given context. Second,\\n\\textit{scorer} measures the importance of each entity by calculating its\\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\\nentities with a dynamic threshold algorithm and highlights the corresponding\\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\\nexperiments on the knowledge hallucination benchmark demonstrate the\\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\\nscore metric. Moreover, COFT also exhibits remarkable versatility across\\nvarious long-form tasks, such as reading comprehension and question answering.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Halu-J: Critique-Based Hallucination Judge</td>\n",
       "      <td>http://arxiv.org/abs/2407.12943v1</td>\n",
       "      <td>Large language models (LLMs) frequently generate non-factual content, known\\nas hallucinations. Existing retrieval-augmented-based hallucination detection\\napproaches typically address this by framing it as a classification task,\\nevaluating hallucinations based on their consistency with retrieved evidence.\\nHowever, this approach usually lacks detailed explanations for these\\nevaluations and does not assess the reliability of these explanations.\\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\\npartially relevant evidence retrieval, impairing the detection process.\\nMoreover, while real-world hallucination detection requires analyzing multiple\\npieces of evidence, current systems usually treat all evidence uniformly\\nwithout considering its relevance to the content. To address these challenges,\\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\\nparameters. Halu-J enhances hallucination detection by selecting pertinent\\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\\ncapability in critique generation and evidence selection. We also introduce\\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination</td>\n",
       "      <td>http://arxiv.org/abs/2410.17783v1</td>\n",
       "      <td>While ongoing advancements in Large Language Models have demonstrated\\nremarkable success across various NLP tasks, Retrieval Augmented Generation\\nModel stands out to be highly effective on downstream applications like\\nQuestion Answering. Recently, RAG-end2end model further optimized the\\narchitecture and achieved notable performance improvements on domain\\nadaptation. However, the effectiveness of these RAG-based architectures remains\\nrelatively unexplored when fine-tuned on specialized domains such as customer\\nservice for building a reliable conversational AI system. Furthermore, a\\ncritical challenge persists in reducing the occurrence of hallucinations while\\nmaintaining high domain-specific accuracy. In this paper, we investigated the\\nperformance of diverse RAG and RAG-like architectures through domain adaptation\\nand evaluated their ability to generate accurate and relevant response grounded\\nin the contextual knowledge base. To facilitate the evaluation of the models,\\nwe constructed a novel dataset HotelConvQA, sourced from wide range of\\nhotel-related conversations and fine-tuned all the models on our domain\\nspecific dataset. We also addressed a critical research gap on determining the\\nimpact of domain adaptation on reducing hallucinations across different RAG\\narchitectures, an aspect that was not properly measured in prior work. Our\\nevaluation shows positive results in all metrics by employing domain\\nadaptation, demonstrating strong performance on QA tasks and providing insights\\ninto their efficacy in reducing hallucinations. Our findings clearly indicate\\nthat domain adaptation not only enhances the models' performance on QA tasks\\nbut also significantly reduces hallucination across all evaluated RAG\\narchitectures.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making</td>\n",
       "      <td>http://arxiv.org/abs/2409.10011v2</td>\n",
       "      <td>Large language models (LLMs) have significantly advanced natural language\\nprocessing tasks, yet they are susceptible to generating inaccurate or\\nunreliable responses, a phenomenon known as hallucination. In critical domains\\nsuch as health and medicine, these hallucinations can pose serious risks. This\\npaper introduces HALO, a novel framework designed to enhance the accuracy and\\nreliability of medical question-answering (QA) systems by focusing on the\\ndetection and mitigation of hallucinations. Our approach generates multiple\\nvariations of a given query using LLMs and retrieves relevant information from\\nexternal open knowledge bases to enrich the context. We utilize maximum\\nmarginal relevance scoring to prioritize the retrieved context, which is then\\nprovided to LLMs for answer generation, thereby reducing the risk of\\nhallucinations. The integration of LangChain further streamlines this process,\\nresulting in a notable and robust increase in the accuracy of both open-source\\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\\nto 70%). This framework underscores the critical importance of addressing\\nhallucinations in medical QA systems, ultimately improving clinical\\ndecision-making and patient care. The open-source HALO is available at:\\nhttps://github.com/ResponsibleAILab/HALO.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Evaluating the Retrieval Component in LLM-Based Question Answering Systems</td>\n",
       "      <td>http://arxiv.org/abs/2406.06458v1</td>\n",
       "      <td>Question answering systems (QA) utilizing Large Language Models (LLMs)\\nheavily depend on the retrieval component to provide them with domain-specific\\ninformation and reduce the risk of generating inaccurate responses or\\nhallucinations. Although the evaluation of retrievers dates back to the early\\nresearch in Information Retrieval, assessing their performance within LLM-based\\nchatbots remains a challenge.\\n  This study proposes a straightforward baseline for evaluating retrievers in\\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\\nthat this evaluation framework provides a better image of how the retriever\\nperforms and is more aligned with the overall performance of the QA system.\\nAlthough conventional metrics such as precision, recall, and F1 score may not\\nfully capture LLMs' capabilities - as they can yield accurate responses despite\\nimperfect retrievers - our method considers LLMs' strengths to ignore\\nirrelevant contexts, as well as potential errors and hallucinations in their\\nresponses.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The Effects of Hallucinations in Synthetic Training Data for Relation Extraction</td>\n",
       "      <td>http://arxiv.org/abs/2410.08393v1</td>\n",
       "      <td>Relation extraction is crucial for constructing knowledge graphs, with large\\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\\nevaluating models. Generative data augmentation (GDA) is a common approach to\\nexpand such datasets. However, this approach often introduces hallucinations,\\nsuch as spurious facts, whose impact on relation extraction remains\\nunderexplored. In this paper, we examine the effects of hallucinations on the\\nperformance of relation extraction on the document and sentence levels. Our\\nempirical study reveals that hallucinations considerably compromise the ability\\nof models to extract relations from text, with recall reductions between 19.1%\\nand 39.2%. We identify that relevant hallucinations impair the model's\\nperformance, while irrelevant hallucinations have a minimal impact.\\nAdditionally, we develop methods for the detection of hallucinations to improve\\ndata quality and model performance. Our approaches successfully classify texts\\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\\n92.2%. These methods not only assist in removing hallucinations but also help\\nin estimating their prevalence within datasets, which is crucial for selecting\\nhigh-quality data. Overall, our work confirms the profound impact of relevant\\nhallucinations on the effectiveness of relation extraction models.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</td>\n",
       "      <td>http://arxiv.org/abs/2411.02959v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\\nmajor source of external knowledge used in RAG systems, and many commercial\\nsystems such as ChatGPT and Perplexity have used Web search engines as their\\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\\ndownload HTML sources of the results, and then extract plain texts from the\\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\\nthe generation. However, much of the structural and semantic information\\ninherent in HTML, such as headings and table structures, is lost during this\\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\\nRAG. We believe HTML is better than plain text in modeling knowledge in\\nexternal documents, and most LLMs possess robust capacities to understand HTML.\\nHowever, utilizing HTML presents new challenges. HTML contains additional\\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\\ninput tokens and noise to the RAG system. To address this issue, we propose\\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\\nminimizing the loss of information. Specifically, we design a two-step\\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\\nsuperiority of using HTML in RAG systems.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>DMQR-RAG: Diverse Multi-Query Rewriting for RAG</td>\n",
       "      <td>http://arxiv.org/abs/2411.13154v1</td>\n",
       "      <td>Large language models often encounter challenges with static knowledge and\\nhallucinations, which undermine their reliability. Retrieval-augmented\\ngeneration (RAG) mitigates these issues by incorporating external information.\\nHowever, user queries frequently contain noise and intent deviations,\\nnecessitating query rewriting to improve the relevance of retrieved documents.\\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\\ndesigned to improve the performance of both document retrieval and final\\nresponses in RAG. Specifically, we investigate how queries with varying\\ninformation quantities can retrieve a diverse array of documents, presenting\\nfour rewriting strategies that operate at different levels of information to\\nenhance the performance of baseline approaches. Additionally, we propose an\\nadaptive strategy selection method that minimizes the number of rewrites while\\noptimizing overall performance. Our methods have been rigorously validated\\nthrough extensive experiments conducted in both academic and industry settings.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain</td>\n",
       "      <td>http://arxiv.org/abs/2408.10343v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) systems are showing promising potential,\\nand are becoming increasingly relevant in AI-powered legal applications.\\nExisting benchmarks, such as LegalBench, assess the generative capabilities of\\nLarge Language Models (LLMs) in the legal domain, but there is a critical gap\\nin evaluating the retrieval component of RAG systems. To address this, we\\nintroduce LegalBench-RAG, the first benchmark specifically designed to evaluate\\nthe retrieval step of RAG pipelines within the legal space. LegalBench-RAG\\nemphasizes precise retrieval by focusing on extracting minimal, highly relevant\\ntext segments from legal documents. These highly relevant snippets are\\npreferred over retrieving document IDs, or large sequences of imprecise chunks,\\nboth of which can exceed context window limitations. Long context windows cost\\nmore to process, induce higher latency, and lead LLMs to forget or hallucinate\\ninformation. Additionally, precise results allow LLMs to generate citations for\\nthe end user. The LegalBench-RAG benchmark is constructed by retracing the\\ncontext used in LegalBench queries back to their original locations within the\\nlegal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus\\nof over 79M characters, entirely human-annotated by legal experts. We also\\nintroduce LegalBench-RAG-mini, a lightweight version for rapid iteration and\\nexperimentation. By providing a dedicated benchmark for legal retrieval,\\nLegalBench-RAG serves as a critical tool for companies and researchers focused\\non enhancing the accuracy and performance of RAG systems in the legal domain.\\nThe LegalBench-RAG dataset is publicly available at\\nhttps://github.com/zeroentropy-cc/legalbenchrag.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Mask-based Membership Inference Attacks for Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2410.20142v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) has been an effective approach to\\nmitigate hallucinations in large language models (LLMs) by incorporating\\nup-to-date and domain-specific knowledge. Recently, there has been a trend of\\nstoring up-to-date or copyrighted data in RAG knowledge databases instead of\\nusing it for LLM training. This practice has raised concerns about Membership\\nInference Attacks (MIAs), which aim to detect if a specific target document is\\nstored in the RAG system's knowledge database so as to protect the rights of\\ndata producers. While research has focused on enhancing the trustworthiness of\\nRAG systems, existing MIAs for RAG systems remain largely insufficient.\\nPrevious work either relies solely on the RAG system's judgment or is easily\\ninfluenced by other documents or the LLM's internal knowledge, which is\\nunreliable and lacks explainability. To address these limitations, we propose a\\nMask-Based Membership Inference Attacks (MBA) framework. Our framework first\\nemploys a masking algorithm that effectively masks a certain number of words in\\nthe target document. The masked text is then used to prompt the RAG system, and\\nthe RAG system is required to predict the mask values. If the target document\\nappears in the knowledge database, the masked text will retrieve the complete\\ntarget document as context, allowing for accurate mask prediction. Finally, we\\nadopt a simple yet effective threshold-based method to infer the membership of\\ntarget document by analyzing the accuracy of mask prediction. Our mask-based\\napproach is more document-specific, making the RAG system's generation less\\nsusceptible to distractions from other documents or the LLM's internal\\nknowledge. Extensive experiments demonstrate the effectiveness of our approach\\ncompared to existing baseline models.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems</td>\n",
       "      <td>http://arxiv.org/abs/2412.12322v1</td>\n",
       "      <td>We present RAG Playground, an open-source framework for systematic evaluation\\nof Retrieval-Augmented Generation (RAG) systems. The framework implements and\\ncompares three retrieval approaches: naive vector search, reranking, and hybrid\\nvector-keyword search, combined with ReAct agents using different prompting\\nstrategies. We introduce a comprehensive evaluation framework with novel\\nmetrics and provide empirical results comparing different language models\\n(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our\\nexperiments demonstrate significant performance improvements through hybrid\\nsearch methods and structured self-evaluation prompting, achieving up to 72.7%\\npass rate on our multi-metric evaluation framework. The results also highlight\\nthe importance of prompt engineering in RAG systems, with our custom-prompted\\nagents showing consistent improvements in retrieval accuracy and response\\nquality.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Retrieval-Augmented Generation for Domain-Specific Question Answering: A Case Study on Pittsburgh and CMU</td>\n",
       "      <td>http://arxiv.org/abs/2411.13691v1</td>\n",
       "      <td>We designed a Retrieval-Augmented Generation (RAG) system to provide large\\nlanguage models with relevant documents for answering domain-specific questions\\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\\nsubpages using a greedy scraping strategy and employed a hybrid annotation\\nprocess, combining manual and Mistral-generated question-answer pairs,\\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\\ndocument retrieval accuracy. Experimental results show that the RAG system\\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\\nenhancing answer precision and relevance, while identifying areas for further\\noptimization in document retrieval and model training.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions</td>\n",
       "      <td>http://arxiv.org/abs/2410.12837v1</td>\n",
       "      <td>This paper presents a comprehensive study of Retrieval-Augmented Generation\\n(RAG), tracing its evolution from foundational concepts to the current state of\\nthe art. RAG combines retrieval mechanisms with generative language models to\\nenhance the accuracy of outputs, addressing key limitations of LLMs. The study\\nexplores the basic architecture of RAG, focusing on how retrieval and\\ngeneration are integrated to handle knowledge-intensive tasks. A detailed\\nreview of the significant technological advancements in RAG is provided,\\nincluding key innovations in retrieval-augmented language models and\\napplications across various domains such as question-answering, summarization,\\nand knowledge-based tasks. Recent research breakthroughs are discussed,\\nhighlighting novel methods for improving retrieval efficiency. Furthermore, the\\npaper examines ongoing challenges such as scalability, bias, and ethical\\nconcerns in deployment. Future research directions are proposed, focusing on\\nimproving the robustness of RAG models, expanding the scope of application of\\nRAG models, and addressing societal implications. This survey aims to serve as\\na foundational resource for researchers and practitioners in understanding the\\npotential of RAG and its trajectory in natural language processing.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots</td>\n",
       "      <td>http://arxiv.org/abs/2403.01193v3</td>\n",
       "      <td>Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\\nof artificial intelligence. However, their tendency to hallucinate -- generate\\nplausible but false information -- poses a significant challenge. This issue is\\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\\nGeneration (RAG) can counter hallucinations by integrating external knowledge\\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\\nin some cases, but can still be misled when prompts directly contradict the\\nmodel's pre-trained understanding. These findings highlight the complex nature\\nof hallucinations and the need for more robust solutions to ensure LLM\\nreliability in real-world applications. We offer practical recommendations for\\nRAG deployment and discuss implications for the development of more trustworthy\\nLLMs.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Plan$\\times$RAG: Planning-guided Retrieval Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2410.20753v1</td>\n",
       "      <td>We introduce Planning-guided Retrieval Augmented Generation\\n(Plan$\\times$RAG), a novel framework that augments the\\n\\emph{retrieve-then-reason} paradigm of existing RAG frameworks to\\n\\emph{plan-then-retrieve}. Plan$\\times$RAG formulates a reasoning plan as a\\ndirected acyclic graph (DAG), decomposing queries into interrelated atomic\\nsub-queries. Answer generation follows the DAG structure, allowing significant\\ngains in efficiency through parallelized retrieval and generation. While\\nstate-of-the-art RAG solutions require extensive data generation and\\nfine-tuning of language models (LMs), Plan$\\times$RAG incorporates frozen LMs\\nas plug-and-play experts to generate high-quality answers. Compared to existing\\nRAG solutions, Plan$\\times$RAG demonstrates significant improvements in\\nreducing hallucinations and bolstering attribution due to its structured\\nsub-query decomposition. Overall, Plan$\\times$RAG offers a new perspective on\\nintegrating external knowledge in LMs while ensuring attribution by design,\\ncontributing towards more reliable LM-based systems.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Seven Failure Points When Engineering a Retrieval Augmented Generation System</td>\n",
       "      <td>http://arxiv.org/abs/2401.05856v1</td>\n",
       "      <td>Software engineers are increasingly adding semantic search capabilities to\\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\\nRAG system involves finding documents that semantically match a query and then\\npassing the documents to a large language model (LLM) such as ChatGPT to\\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\\nproblem of hallucinated responses from LLMs, b) link sources/references to\\ngenerated responses, and c) remove the need for annotating documents with\\nmeta-data. However, RAG systems suffer from limitations inherent to information\\nretrieval systems and from reliance on LLMs. In this paper, we present an\\nexperience report on the failure points of RAG systems from three case studies\\nfrom separate domains: research, education, and biomedical. We share the\\nlessons learned and present 7 failure points to consider when designing a RAG\\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\\nsystem is only feasible during operation, and 2) the robustness of a RAG system\\nevolves rather than designed in at the start. We conclude with a list of\\npotential research directions on RAG systems for the software engineering\\ncommunity.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Federated Recommendation via Hybrid Retrieval Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2403.04256v1</td>\n",
       "      <td>Federated Recommendation (FR) emerges as a novel paradigm that enables\\nprivacy-preserving recommendations. However, traditional FR systems usually\\nrepresent users/items with discrete identities (IDs), suffering from\\nperformance degradation due to the data sparsity and heterogeneity in FR. On\\nthe other hand, Large Language Models (LLMs) as recommenders have proven\\neffective across various recommendation scenarios. Yet, LLM-based recommenders\\nencounter challenges such as low inference efficiency and potential\\nhallucination, compromising their performance in real-world scenarios. To this\\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\\nprocess, mining ID-based user patterns and text-based item features. Next, the\\nretrieved results are converted into text prompts and fed into GPT for\\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\\nto extract generalized features from data and exploit pretrained knowledge\\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\\nRAG approach also prevents LLM hallucination, improving the recommendation\\nperformance for real-world users. Experimental results on diverse benchmark\\ndatasets demonstrate the superior performance of GPT-FedRec against\\nstate-of-the-art baseline methods.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems</td>\n",
       "      <td>http://arxiv.org/abs/2404.02103v2</td>\n",
       "      <td>Retrieval Augmented Generation (RAG) has become a popular application for\\nlarge language models. It is preferable that successful RAG systems provide\\naccurate answers that are supported by being grounded in a passage without any\\nhallucinations. While considerable work is required for building a full RAG\\npipeline, being able to benchmark performance is also necessary. We present\\nClapNQ, a benchmark Long-form Question Answering dataset for the full RAG\\npipeline. ClapNQ includes long answers with grounded gold passages from Natural\\nQuestions (NQ) and a corpus to perform either retrieval, generation, or the\\nfull RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full\\npassage, and cohesive, meaning that the answer is composed fluently, often by\\nintegrating multiple pieces of the passage that are not contiguous. RAG models\\nmust adapt to these properties to be successful at ClapNQ. We present baseline\\nexperiments and analysis for ClapNQ that highlight areas where there is still\\nsignificant room for improvement in grounded RAG. CLAPNQ is publicly available\\nat https://github.com/primeqa/clapnq</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System</td>\n",
       "      <td>http://arxiv.org/abs/2412.13163v2</td>\n",
       "      <td>Organizations seeking to utilize Large Language Models (LLMs) for knowledge\\nquerying and analysis often encounter challenges in maintaining an LLM\\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\\nsolution for organizations looking to overcome the challenges of maintaining\\nproprietary models and to help reduce LLM hallucinations in their query\\nresponses. However, RAG comes with its own issues regarding scaling data\\npipelines across tiered-access and disparate data sources. In many scenarios,\\nit is necessary to query beyond a single data silo to provide richer and more\\nrelevant context for an LLM. Analyzing data sources within and across\\norganizational trust boundaries is often limited by complex data-sharing\\npolicies that prohibit centralized data storage, therefore, inhibit the fast\\nand effective setup and scaling of RAG solutions. In this paper, we introduce\\nConfidential Computing (CC) techniques as a solution for secure Federated\\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\\nacross a decentralized network of data providers by ensuring context\\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\\nMIRAGE benchmarking dataset.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Banishing LLM Hallucinations Requires Rethinking Generalization</td>\n",
       "      <td>http://arxiv.org/abs/2406.17642v1</td>\n",
       "      <td>Despite their powerful chat, coding, and reasoning abilities, Large Language\\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\\nhallucinations are a consequence of a balance between creativity and\\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\\nexternal knowledge sources. Through extensive systematic experiments, we show\\nthat these traditional approaches fail to explain why LLMs hallucinate in\\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\\ncorroborate these experimental findings with a theoretical construction showing\\nthat simple neural networks trained to predict the next token hallucinate when\\nthe training loss is above a threshold as it usually does in practice when\\ntraining on internet scale data. We interpret our findings by comparing against\\ntraditional retrieval methods for mitigating hallucinations. We use our\\nfindings to design a first generation model for removing hallucinations --\\nLamini-1 -- that stores facts in a massive mixture of millions of memory\\nexperts that are retrieved dynamically.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under Adversarial Poisoning Attacks</td>\n",
       "      <td>http://arxiv.org/abs/2412.16708v1</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) systems have emerged as a promising\\nsolution to mitigate LLM hallucinations and enhance their performance in\\nknowledge-intensive domains. However, these systems are vulnerable to\\nadversarial poisoning attacks, where malicious passages injected into retrieval\\ndatabases can mislead the model into generating factually incorrect outputs. In\\nthis paper, we investigate both the retrieval and the generation components of\\nRAG systems to understand how to enhance their robustness against such attacks.\\nFrom the retrieval perspective, we analyze why and how the adversarial contexts\\nare retrieved and assess how the quality of the retrieved passages impacts\\ndownstream generation. From a generation perspective, we evaluate whether LLMs'\\nadvanced critical thinking and internal knowledge capabilities can be leveraged\\nto mitigate the impact of adversarial contexts, i.e., using skeptical prompting\\nas a self-defense mechanism. Our experiments and findings provide actionable\\ninsights into designing safer and more resilient retrieval-augmented\\nframeworks, paving the way for their reliable deployment in real-world\\napplications.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems</td>\n",
       "      <td>http://arxiv.org/abs/2410.14167v1</td>\n",
       "      <td>This study aims to improve the accuracy and quality of large-scale language\\nmodels (LLMs) in answering questions by integrating Elasticsearch into the\\nRetrieval Augmented Generation (RAG) framework. The experiment uses the\\nStanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\\ncompares the performance of different retrieval methods, including traditional\\nmethods based on keyword matching or semantic similarity calculation, BM25-RAG\\nand TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\\nES-RAG not only has obvious advantages in retrieval efficiency but also\\nperforms well in key indicators such as accuracy, which is 0.51 percentage\\npoints higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\\ncapabilities and rich configuration options enable the entire\\nquestion-answering system to better handle complex queries and provide more\\nflexible and efficient responses based on the diverse needs of users. Future\\nresearch directions can further explore how to optimize the interaction\\nmechanism between Elasticsearch and LLM, such as introducing higher-level\\nsemantic understanding and context-awareness capabilities, to achieve a more\\nintelligent and humanized question-answering experience.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Clustered Retrieved Augmented Generation (CRAG)</td>\n",
       "      <td>http://arxiv.org/abs/2406.00029v1</td>\n",
       "      <td>Providing external knowledge to Large Language Models (LLMs) is a key point\\nfor using these models in real-world applications for several reasons, such as\\nincorporating up-to-date content in a real-time manner, providing access to\\ndomain-specific knowledge, and contributing to hallucination prevention. The\\nvector database-based Retrieval Augmented Generation (RAG) approach has been\\nwidely adopted to this end. Thus, any part of external knowledge can be\\nretrieved and provided to some LLM as the input context. Despite RAG approach's\\nsuccess, it still might be unfeasible for some applications, because the\\ncontext retrieved can demand a longer context window than the size supported by\\nLLM. Even when the context retrieved fits into the context window size, the\\nnumber of tokens might be expressive and, consequently, impact costs and\\nprocessing time, becoming impractical for most applications. To address these,\\nwe propose CRAG, a novel approach able to effectively reduce the number of\\nprompting tokens without degrading the quality of the response generated\\ncompared to a solution using RAG. Through our experiments, we show that CRAG\\ncan reduce the number of tokens by at least 46\\%, achieving more than 90\\% in\\nsome cases, compared to RAG. Moreover, the number of tokens with CRAG does not\\nincrease considerably when the number of reviews analyzed is higher, unlike\\nRAG, where the number of tokens is almost 9x higher when there are 75 reviews\\ncompared to 4 reviews.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</td>\n",
       "      <td>http://arxiv.org/abs/2412.02592v1</td>\n",
       "      <td>Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\\nintegrating external knowledge to reduce hallucinations and incorporate\\nup-to-date information without retraining. As an essential part of RAG,\\nexternal knowledge bases are commonly built by extracting structured data from\\nunstructured PDF documents using Optical Character Recognition (OCR). However,\\ngiven the imperfect prediction of OCR and the inherent non-uniform\\nrepresentation of structured data, knowledge bases inevitably contain various\\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\\ncarefully selected unstructured PDF documents from six real-world RAG\\napplication domains, along with Q&amp;As derived from multimodal elements in\\ndocuments, challenging existing OCR solutions used for RAG To better understand\\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\\nthat none is competent for constructing high-quality knowledge bases for RAG\\nsystems. We then systematically evaluate the impact of these two noise types\\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\\nsystems. Code: https://github.com/opendatalab/OHR-Bench</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           title  \\\n",
       "0                                                                Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots   \n",
       "1                                    Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost   \n",
       "3                                       LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation   \n",
       "5                                                                    AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed articles   \n",
       "7                                                                                 Searching for Best Practices in Retrieval-Augmented Generation   \n",
       "8                                                             The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation   \n",
       "10      Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems   \n",
       "12                                                                            Retrieval-Augmented Generation for Large Language Models: A Survey   \n",
       "13                      RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks   \n",
       "19                                               THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models   \n",
       "21                                                  Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation   \n",
       "23                                                                    DelucionQA: Detecting Hallucinations in Domain-specific Question Answering   \n",
       "24                                                               Reducing hallucination in structured outputs via Retrieval-Augmented Generation   \n",
       "25                                                                          Context-Augmented Code Generation Using Programming Knowledge Graphs   \n",
       "26                                                                                Mitigating Entity-Level Hallucination in Large Language Models   \n",
       "27                                                                                             Context Tuning for Retrieval Augmented Generation   \n",
       "28                                           Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval   \n",
       "29                                                        Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models   \n",
       "30                                                                                                    Halu-J: Critique-Based Hallucination Judge   \n",
       "31                   Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination   \n",
       "33   HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making   \n",
       "35                                                                    Evaluating the Retrieval Component in LLM-Based Question Answering Systems   \n",
       "38                                                              The Effects of Hallucinations in Synthetic Training Data for Relation Extraction   \n",
       "43                                                       HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems   \n",
       "47                                                                                               DMQR-RAG: Diverse Multi-Query Rewriting for RAG   \n",
       "48                                                            LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain   \n",
       "56                                                                    Mask-based Membership Inference Attacks for Retrieval-Augmented Generation   \n",
       "62                           RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems   \n",
       "63                                     Retrieval-Augmented Generation for Domain-Specific Question Answering: A Case Study on Pittsburgh and CMU   \n",
       "72                            A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions   \n",
       "107                                                                         RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots   \n",
       "111                                                                              Plan$\\times$RAG: Planning-guided Retrieval Augmented Generation   \n",
       "112                                                                Seven Failure Points When Engineering a Retrieval Augmented Generation System   \n",
       "113                                                                           Federated Recommendation via Hybrid Retrieval Augmented Generation   \n",
       "115                                                        CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems   \n",
       "119                                                                     C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System   \n",
       "125                                                                              Banishing LLM Hallucinations Requires Rethinking Generalization   \n",
       "132                                       Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under Adversarial Poisoning Attacks   \n",
       "157                                         Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems   \n",
       "172                                                                                              Clustered Retrieved Augmented Generation (CRAG)   \n",
       "177                                                    OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation   \n",
       "\n",
       "                              entry_id  \\\n",
       "0    http://arxiv.org/abs/2412.04235v1   \n",
       "1    http://arxiv.org/abs/2406.00975v2   \n",
       "3    http://arxiv.org/abs/2408.15533v2   \n",
       "5    http://arxiv.org/abs/2412.16701v1   \n",
       "7    http://arxiv.org/abs/2407.01219v1   \n",
       "8    http://arxiv.org/abs/2407.18044v1   \n",
       "10   http://arxiv.org/abs/2403.09727v1   \n",
       "12   http://arxiv.org/abs/2312.10997v5   \n",
       "13   http://arxiv.org/abs/2411.14110v1   \n",
       "19   http://arxiv.org/abs/2409.11353v3   \n",
       "21   http://arxiv.org/abs/2408.00555v1   \n",
       "23   http://arxiv.org/abs/2312.05200v1   \n",
       "24   http://arxiv.org/abs/2404.08189v1   \n",
       "25   http://arxiv.org/abs/2410.18251v1   \n",
       "26   http://arxiv.org/abs/2407.09417v2   \n",
       "27   http://arxiv.org/abs/2312.05708v1   \n",
       "28   http://arxiv.org/abs/2405.06545v1   \n",
       "29   http://arxiv.org/abs/2410.15116v1   \n",
       "30   http://arxiv.org/abs/2407.12943v1   \n",
       "31   http://arxiv.org/abs/2410.17783v1   \n",
       "33   http://arxiv.org/abs/2409.10011v2   \n",
       "35   http://arxiv.org/abs/2406.06458v1   \n",
       "38   http://arxiv.org/abs/2410.08393v1   \n",
       "43   http://arxiv.org/abs/2411.02959v1   \n",
       "47   http://arxiv.org/abs/2411.13154v1   \n",
       "48   http://arxiv.org/abs/2408.10343v1   \n",
       "56   http://arxiv.org/abs/2410.20142v1   \n",
       "62   http://arxiv.org/abs/2412.12322v1   \n",
       "63   http://arxiv.org/abs/2411.13691v1   \n",
       "72   http://arxiv.org/abs/2410.12837v1   \n",
       "107  http://arxiv.org/abs/2403.01193v3   \n",
       "111  http://arxiv.org/abs/2410.20753v1   \n",
       "112  http://arxiv.org/abs/2401.05856v1   \n",
       "113  http://arxiv.org/abs/2403.04256v1   \n",
       "115  http://arxiv.org/abs/2404.02103v2   \n",
       "119  http://arxiv.org/abs/2412.13163v2   \n",
       "125  http://arxiv.org/abs/2406.17642v1   \n",
       "132  http://arxiv.org/abs/2412.16708v1   \n",
       "157  http://arxiv.org/abs/2410.14167v1   \n",
       "172  http://arxiv.org/abs/2406.00029v1   \n",
       "177  http://arxiv.org/abs/2412.02592v1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 summary  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I combine detection and mitigation techniques to addresses hallucinations in\\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\\nintroducing the Negative Missing Information Scoring System (NMISS), which\\naccounts for contextual relevance in responses. While RAG mitigates\\nhallucinations by grounding answers in external data, NMISS refines the\\nevaluation by identifying cases where traditional metrics incorrectly flag\\ncontextually accurate responses as hallucinations. I use Italian health news\\narticles as context to evaluate LLM performance. Results show that Gemma2 and\\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\\nbenefit significantly from NMISS, highlighting their ability to provide richer\\ncontextual information. This combined approach offers new insights into the\\nreduction and more accurate assessment of hallucinations in LLMs, with\\napplications in real-world healthcare tasks and other domains.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Retriever Augmented Generation (RAG) systems have become pivotal in enhancing\\nthe capabilities of language models by incorporating external knowledge\\nretrieval mechanisms. However, a significant challenge in deploying these\\nsystems in industry applications is the detection and mitigation of\\nhallucinations: instances where the model generates information that is not\\ngrounded in the retrieved context. Addressing this issue is crucial for\\nensuring the reliability and accuracy of responses generated by large language\\nmodels (LLMs) in diverse industry settings. Current hallucination detection\\ntechniques fail to deliver accuracy, low latency, and low cost simultaneously.\\nWe introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination\\ndetection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and\\ncommercial evaluation frameworks on the hallucination detection task, with 97%\\nand 91% reduction in cost and latency, respectively. Luna is lightweight and\\ngeneralizes across multiple industry verticals and out-of-domain data, making\\nit an ideal candidate for industry LLM applications.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Retrieval-Augmented Generation (RAG) has become a primary technique for\\nmitigating hallucinations in large language models (LLMs). However, incomplete\\nknowledge extraction and insufficient understanding can still mislead LLMs to\\nproduce irrelevant or even contradictory responses, which means hallucinations\\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\\ninput and output of the RAG generator. We then apply further extraction and\\nresampling to the relevance matrix. The processed relevance data are input into\\nmultiple classifiers to determine whether the output contains hallucinations.\\nTo the best of our knowledge, this is the first time that LRP has been used for\\ndetecting RAG hallucinations, and extensive experiments demonstrate that\\nLRP4RAG outperforms existing baselines.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                               Recent advancements in generative AI have flourished the development of\\nhighly adept Large Language Models (LLMs) that integrate diverse data types to\\nempower decision-making. Among these, Multimodal Retrieval-Augmented Generation\\n(RAG) applications are promising for their capability to combine the strengths\\nof information retrieval and generative models, enhancing their utility across\\nvarious domains, including biomedical research. This paper introduces\\nAlzheimerRAG, a Multimodal RAG pipeline tool for biomedical research use cases,\\nprimarily focusing on Alzheimer's disease from PubMed articles. Our pipeline\\nincorporates multimodal fusion techniques to integrate textual and visual data\\nprocessing by efficiently indexing and accessing vast amounts of biomedical\\nliterature. Preliminary experimental results against benchmarks, such as BioASQ\\nand PubMedQA, have returned improved results in information retrieval and\\nsynthesis of domain-specific information. We also demonstrate a case study with\\nour RAG pipeline across different Alzheimer's clinical scenarios. We infer that\\nAlzheimerRAG can generate responses with accuracy non-inferior to humans and\\nwith low rates of hallucination. Overall, a reduction in cognitive task load is\\nobserved, which allows researchers to gain multimodal insights, improving\\nunderstanding and treatment of Alzheimer's disease.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Retrieval-augmented generation (RAG) techniques have proven to be effective\\nin integrating up-to-date information, mitigating hallucinations, and enhancing\\nresponse quality, particularly in specialized domains. While many RAG\\napproaches have been proposed to enhance large language models through\\nquery-dependent retrievals, these approaches still suffer from their complex\\nimplementation and prolonged response times. Typically, a RAG workflow involves\\nmultiple processing steps, each of which can be executed in various ways. Here,\\nwe investigate existing RAG approaches and their potential combinations to\\nidentify optimal RAG practices. Through extensive experiments, we suggest\\nseveral strategies for deploying RAG that balance both performance and\\nefficiency. Moreover, we demonstrate that multimodal retrieval techniques can\\nsignificantly enhance question-answering capabilities about visual inputs and\\naccelerate the generation of multimodal content using a \"retrieval as\\ngeneration\" strategy.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                             Digital health chatbots powered by Large Language Models (LLMs) have the\\npotential to significantly improve personal health management for chronic\\nconditions by providing accessible and on-demand health coaching and\\nquestion-answering. However, these chatbots risk providing unverified and\\ninaccurate information because LLMs generate responses based on patterns\\nlearned from diverse internet data. Retrieval Augmented Generation (RAG) can\\nhelp mitigate hallucinations and inaccuracies in LLM responses by grounding it\\non reliable content. However, efficiently and accurately retrieving most\\nrelevant set of content for real-time user questions remains a challenge. In\\nthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a\\nnovel approach that pre-computes a database of potential queries from a content\\nbase using LLMs. For an incoming patient question, QB-RAG efficiently matches\\nit against this pre-generated query database using vector search, improving\\nalignment between user questions and the content. We establish a theoretical\\nfoundation for QB-RAG and provide a comparative analysis of existing retrieval\\nenhancement techniques for RAG systems. Finally, our empirical evaluation\\ndemonstrates that QB-RAG significantly improves the accuracy of healthcare\\nquestion answering, paving the way for robust and trustworthy LLM applications\\nin digital health.   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The development of generative large language models (G-LLM) opened up new\\nopportunities for the development of new types of knowledge-based systems\\nsimilar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented\\nGeneration (RAG) are the techniques that can be used to implement domain\\nadaptation for the development of G-LLM-based knowledge systems. In our study,\\nusing ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine\\nthe performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2\\nlanguage models. Based on measurements shown on different datasets, we\\ndemonstrate that RAG-based constructions are more efficient than models\\nproduced with FN. We point out that connecting RAG and FN is not trivial,\\nbecause connecting FN models with RAG can cause a decrease in performance.\\nFurthermore, we outline a simple RAG-based architecture which, on average,\\noutperforms the FN models by 16% in terms of the ROGUE score, 15% in the case\\nof the BLEU score, and 53% based on the cosine similarity. This shows the\\nsignificant advantage of RAG over FN in terms of hallucination, which is not\\noffset by the fact that the average 8% better METEOR score of FN models\\nindicates greater creativity compared to RAG.   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.   \n",
       "13                                                                                                                                                                                                                                                                                    While large language models (LLMs) have achieved notable success in\\ngenerative tasks, they still face limitations, such as lacking up-to-date\\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\\nenhances LLM performance by integrating external knowledge bases, providing\\nadditional context which significantly improves accuracy and knowledge\\ncoverage. However, building these external knowledge bases often requires\\nsubstantial resources and may involve sensitive information. In this paper, we\\npropose an agent-based automated privacy attack called RAG-Thief, which can\\nextract a scalable amount of private data from the private database used in RAG\\napplications. We conduct a systematic study on the privacy risks associated\\nwith RAG applications, revealing that the vulnerability of LLMs makes the\\nprivate knowledge bases suffer significant privacy risks. Unlike previous\\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\\nstarts with an initial adversarial query and learns from model responses,\\nprogressively generating new queries to extract as many chunks from the\\nknowledge base as possible. Experimental results show that our RAG-Thief can\\nextract over 70% information from the private knowledge bases within customized\\nRAG applications deployed on local machines and real-world platforms, including\\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\\nvulnerabilities in current RAG applications and underscore the pressing need\\nfor stronger safeguards.   \n",
       "19                                                                                                                                                                                                                                                                                                                                                     Hallucination, the generation of factually incorrect content, is a growing\\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\\nmethods are often isolated and insufficient for domain-specific needs, lacking\\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\\nMitigations and EvaluationS), an integrated framework and library addressing\\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\\nhallucinations in LLMs, featuring automated test set generation, multifaceted\\nbenchmarking, and adaptable mitigation strategies. It automates test set\\ncreation from any corpus, ensuring high data quality, diversity, and\\ncost-efficiency through techniques like batch processing, weighted sampling,\\nand counterfactual validation. THaMES assesses a model's ability to detect and\\nreduce hallucinations across various tasks, including text generation and\\nbinary classification, applying optimal mitigation strategies like In-Context\\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\\nof academic papers, political news, and Wikipedia reveal that commercial models\\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\\nevaluation tasks.   \n",
       "21                              Despite the remarkable ability of large vision-language models (LVLMs) in\\nimage comprehension, these models frequently generate plausible yet factually\\nincorrect responses, a phenomenon known as hallucination.Recently, in large\\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\\nknowledge resources has been proven as a promising solution to mitigate\\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\\nbehind the widespread applications of LVLM. Moreover, when transferred to\\naugmenting LVLMs, sometimes the hallucination degree of the model is even\\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\\nintroduce a novel framework, the Active Retrieval-Augmented large\\nvision-language model (ARA), specifically designed to address hallucinations by\\nincorporating three critical dimensions: (i) dissecting the retrieval targets\\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\\nmost effective retrieval methods and filtering out the reliable retrieval\\nresults. (iii) timing the retrieval process to coincide with episodes of low\\ncertainty, while circumventing unnecessary retrieval during periods of high\\ncertainty. To assess the capability of our proposed ARA model in reducing\\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\\ncan effectively mitigate the hallucination problem. We hope that this study can\\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\\nfor reducing hallucinations with more effective retrieval and minimal retrieval\\noccurrences.   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Hallucination is a well-known phenomenon in text generated by large language\\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\\napplication scenarios e.g., summarization, question-answering (QA) etc. For\\napplications requiring high reliability (e.g., customer-facing assistants), the\\npotential existence of hallucination in LLM-generated text is a critical\\nproblem. The amount of hallucination can be reduced by leveraging information\\nretrieval to provide relevant background information to the LLM. However, LLMs\\ncan still generate hallucinatory content for various reasons (e.g.,\\nprioritizing its parametric knowledge over the context, failure to capture the\\nrelevant information from the context, etc.). Detecting hallucinations through\\nautomated methods is thus paramount. To facilitate research in this direction,\\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\\npropose a set of hallucination detection methods to serve as baselines for\\nfuture works from the research community. Analysis and case study are also\\nprovided to share valuable insights on hallucination phenomena in the target\\nscenario.   \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A common and fundamental limitation of Generative AI (GenAI) is its\\npropensity to hallucinate. While large language models (LLM) have taken the\\nworld by storm, without eliminating or at least reducing hallucinations,\\nreal-world GenAI systems may face challenges in user adoption. In the process\\nof deploying an enterprise application that produces workflows based on natural\\nlanguage requirements, we devised a system leveraging Retrieval Augmented\\nGeneration (RAG) to greatly improve the quality of the structured output that\\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\\nsystem significantly reduces hallucinations in the output and improves the\\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\\nusing a small, well-trained retriever encoder can reduce the size of the\\naccompanying LLM, thereby making deployments of LLM-based systems less\\nresource-intensive.   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                            Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\\nimproved code generation, but, they frequently face difficulties when dealing\\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\\naddresses this issue by retrieving and integrating external knowledge at the\\ninference time. However, retrieval models often fail to find most relevant\\ncontext, and generation models, with limited context capacity, can hallucinate\\nwhen given irrelevant data. We present a novel framework that leverages a\\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\\nThis approach enables fine-grained code retrieval by focusing on the most\\nrelevant segments while reducing irrelevant context through a tree-pruning\\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\\nhallucinations by selectively integrating non-RAG solutions. We propose two\\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\\ntree pruning to enhance retrieval precision, a re-ranking method for robust\\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\\ncode augmentation with relevant comments and docstrings.   \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The emergence of Large Language Models (LLMs) has revolutionized how users\\naccess information, shifting from traditional search engines to direct\\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\\ngenerate coherent yet factually inaccurate responses. This hallucination\\nphenomenon has led to users' distrust in information retrieval systems based on\\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\\nAugmentation based on hallucination Detection (DRAD) as a novel method to\\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\\nretrieval augmentation by dynamically adapting the retrieval process based on\\nreal-time hallucination detection. It features two main components: Real-time\\nHallucination Detection (RHD) for identifying potential hallucinations without\\nexternal models, and Self-correction based on External Knowledge (SEK) for\\ncorrecting these errors using external knowledge. Experiment results show that\\nDRAD demonstrates superior performance in both detecting and mitigating\\nhallucinations in LLMs. All of our code and data are open-sourced at\\nhttps://github.com/oneal2000/EntityHallucination.   \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                        Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                         Large language models (LLMs) have demonstrated remarkable capabilities across\\nvarious domains, although their susceptibility to hallucination poses\\nsignificant challenges for their deployment in critical areas such as\\nhealthcare. To address this issue, retrieving relevant facts from knowledge\\ngraphs (KGs) is considered a promising method. Existing KG-augmented approaches\\ntend to be resource-intensive, requiring multiple rounds of retrieval and\\nverification for each factoid, which impedes their application in real-world\\nscenarios.\\n  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval\\n(Re-KGR) to augment the factuality of LLMs' responses with less retrieval\\nefforts in the medical field. Our approach leverages the attribution of\\nnext-token predictive probability distributions across different tokens, and\\nvarious model layers to primarily identify tokens with a high potential for\\nhallucination, reducing verification rounds by refining knowledge triples\\nassociated with these tokens. Moreover, we rectify inaccurate content using\\nretrieved knowledge in the post-processing stage, which improves the\\ntruthfulness of generated responses. Experimental results on a medical dataset\\ndemonstrate that our approach can enhance the factual capability of LLMs across\\nvarious foundational models as evidenced by the highest scores on truthfulness.   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                Generation of plausible but incorrect factual information, often termed\\nhallucination, has attracted significant research interest. Retrieval-augmented\\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\\nemerges as a promising method to reduce hallucination. However, existing RALMs\\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\\naddress this challenge, we propose COFT, a novel\\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\\nknowledge graph to extract potential key entities in a given context. Second,\\n\\textit{scorer} measures the importance of each entity by calculating its\\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\\nentities with a dynamic threshold algorithm and highlights the corresponding\\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\\nexperiments on the knowledge hallucination benchmark demonstrate the\\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\\nscore metric. Moreover, COFT also exhibits remarkable versatility across\\nvarious long-form tasks, such as reading comprehension and question answering.   \n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Large language models (LLMs) frequently generate non-factual content, known\\nas hallucinations. Existing retrieval-augmented-based hallucination detection\\napproaches typically address this by framing it as a classification task,\\nevaluating hallucinations based on their consistency with retrieved evidence.\\nHowever, this approach usually lacks detailed explanations for these\\nevaluations and does not assess the reliability of these explanations.\\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\\npartially relevant evidence retrieval, impairing the detection process.\\nMoreover, while real-world hallucination detection requires analyzing multiple\\npieces of evidence, current systems usually treat all evidence uniformly\\nwithout considering its relevance to the content. To address these challenges,\\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\\nparameters. Halu-J enhances hallucination detection by selecting pertinent\\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\\ncapability in critique generation and evidence selection. We also introduce\\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .   \n",
       "31                                                               While ongoing advancements in Large Language Models have demonstrated\\nremarkable success across various NLP tasks, Retrieval Augmented Generation\\nModel stands out to be highly effective on downstream applications like\\nQuestion Answering. Recently, RAG-end2end model further optimized the\\narchitecture and achieved notable performance improvements on domain\\nadaptation. However, the effectiveness of these RAG-based architectures remains\\nrelatively unexplored when fine-tuned on specialized domains such as customer\\nservice for building a reliable conversational AI system. Furthermore, a\\ncritical challenge persists in reducing the occurrence of hallucinations while\\nmaintaining high domain-specific accuracy. In this paper, we investigated the\\nperformance of diverse RAG and RAG-like architectures through domain adaptation\\nand evaluated their ability to generate accurate and relevant response grounded\\nin the contextual knowledge base. To facilitate the evaluation of the models,\\nwe constructed a novel dataset HotelConvQA, sourced from wide range of\\nhotel-related conversations and fine-tuned all the models on our domain\\nspecific dataset. We also addressed a critical research gap on determining the\\nimpact of domain adaptation on reducing hallucinations across different RAG\\narchitectures, an aspect that was not properly measured in prior work. Our\\nevaluation shows positive results in all metrics by employing domain\\nadaptation, demonstrating strong performance on QA tasks and providing insights\\ninto their efficacy in reducing hallucinations. Our findings clearly indicate\\nthat domain adaptation not only enhances the models' performance on QA tasks\\nbut also significantly reduces hallucination across all evaluated RAG\\narchitectures.   \n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Large language models (LLMs) have significantly advanced natural language\\nprocessing tasks, yet they are susceptible to generating inaccurate or\\nunreliable responses, a phenomenon known as hallucination. In critical domains\\nsuch as health and medicine, these hallucinations can pose serious risks. This\\npaper introduces HALO, a novel framework designed to enhance the accuracy and\\nreliability of medical question-answering (QA) systems by focusing on the\\ndetection and mitigation of hallucinations. Our approach generates multiple\\nvariations of a given query using LLMs and retrieves relevant information from\\nexternal open knowledge bases to enrich the context. We utilize maximum\\nmarginal relevance scoring to prioritize the retrieved context, which is then\\nprovided to LLMs for answer generation, thereby reducing the risk of\\nhallucinations. The integration of LangChain further streamlines this process,\\nresulting in a notable and robust increase in the accuracy of both open-source\\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\\nto 70%). This framework underscores the critical importance of addressing\\nhallucinations in medical QA systems, ultimately improving clinical\\ndecision-making and patient care. The open-source HALO is available at:\\nhttps://github.com/ResponsibleAILab/HALO.   \n",
       "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Question answering systems (QA) utilizing Large Language Models (LLMs)\\nheavily depend on the retrieval component to provide them with domain-specific\\ninformation and reduce the risk of generating inaccurate responses or\\nhallucinations. Although the evaluation of retrievers dates back to the early\\nresearch in Information Retrieval, assessing their performance within LLM-based\\nchatbots remains a challenge.\\n  This study proposes a straightforward baseline for evaluating retrievers in\\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\\nthat this evaluation framework provides a better image of how the retriever\\nperforms and is more aligned with the overall performance of the QA system.\\nAlthough conventional metrics such as precision, recall, and F1 score may not\\nfully capture LLMs' capabilities - as they can yield accurate responses despite\\nimperfect retrievers - our method considers LLMs' strengths to ignore\\nirrelevant contexts, as well as potential errors and hallucinations in their\\nresponses.   \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                              Relation extraction is crucial for constructing knowledge graphs, with large\\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\\nevaluating models. Generative data augmentation (GDA) is a common approach to\\nexpand such datasets. However, this approach often introduces hallucinations,\\nsuch as spurious facts, whose impact on relation extraction remains\\nunderexplored. In this paper, we examine the effects of hallucinations on the\\nperformance of relation extraction on the document and sentence levels. Our\\nempirical study reveals that hallucinations considerably compromise the ability\\nof models to extract relations from text, with recall reductions between 19.1%\\nand 39.2%. We identify that relevant hallucinations impair the model's\\nperformance, while irrelevant hallucinations have a minimal impact.\\nAdditionally, we develop methods for the detection of hallucinations to improve\\ndata quality and model performance. Our approaches successfully classify texts\\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\\n92.2%. These methods not only assist in removing hallucinations but also help\\nin estimating their prevalence within datasets, which is crucial for selecting\\nhigh-quality data. Overall, our work confirms the profound impact of relevant\\nhallucinations on the effectiveness of relation extraction models.   \n",
       "43                                                                                                                                                                                                                                                           Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\\nmajor source of external knowledge used in RAG systems, and many commercial\\nsystems such as ChatGPT and Perplexity have used Web search engines as their\\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\\ndownload HTML sources of the results, and then extract plain texts from the\\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\\nthe generation. However, much of the structural and semantic information\\ninherent in HTML, such as headings and table structures, is lost during this\\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\\nRAG. We believe HTML is better than plain text in modeling knowledge in\\nexternal documents, and most LLMs possess robust capacities to understand HTML.\\nHowever, utilizing HTML presents new challenges. HTML contains additional\\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\\ninput tokens and noise to the RAG system. To address this issue, we propose\\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\\nminimizing the loss of information. Specifically, we design a two-step\\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\\nsuperiority of using HTML in RAG systems.   \n",
       "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Large language models often encounter challenges with static knowledge and\\nhallucinations, which undermine their reliability. Retrieval-augmented\\ngeneration (RAG) mitigates these issues by incorporating external information.\\nHowever, user queries frequently contain noise and intent deviations,\\nnecessitating query rewriting to improve the relevance of retrieved documents.\\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\\ndesigned to improve the performance of both document retrieval and final\\nresponses in RAG. Specifically, we investigate how queries with varying\\ninformation quantities can retrieve a diverse array of documents, presenting\\nfour rewriting strategies that operate at different levels of information to\\nenhance the performance of baseline approaches. Additionally, we propose an\\nadaptive strategy selection method that minimizes the number of rewrites while\\noptimizing overall performance. Our methods have been rigorously validated\\nthrough extensive experiments conducted in both academic and industry settings.   \n",
       "48                                                                                       Retrieval-Augmented Generation (RAG) systems are showing promising potential,\\nand are becoming increasingly relevant in AI-powered legal applications.\\nExisting benchmarks, such as LegalBench, assess the generative capabilities of\\nLarge Language Models (LLMs) in the legal domain, but there is a critical gap\\nin evaluating the retrieval component of RAG systems. To address this, we\\nintroduce LegalBench-RAG, the first benchmark specifically designed to evaluate\\nthe retrieval step of RAG pipelines within the legal space. LegalBench-RAG\\nemphasizes precise retrieval by focusing on extracting minimal, highly relevant\\ntext segments from legal documents. These highly relevant snippets are\\npreferred over retrieving document IDs, or large sequences of imprecise chunks,\\nboth of which can exceed context window limitations. Long context windows cost\\nmore to process, induce higher latency, and lead LLMs to forget or hallucinate\\ninformation. Additionally, precise results allow LLMs to generate citations for\\nthe end user. The LegalBench-RAG benchmark is constructed by retracing the\\ncontext used in LegalBench queries back to their original locations within the\\nlegal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus\\nof over 79M characters, entirely human-annotated by legal experts. We also\\nintroduce LegalBench-RAG-mini, a lightweight version for rapid iteration and\\nexperimentation. By providing a dedicated benchmark for legal retrieval,\\nLegalBench-RAG serves as a critical tool for companies and researchers focused\\non enhancing the accuracy and performance of RAG systems in the legal domain.\\nThe LegalBench-RAG dataset is publicly available at\\nhttps://github.com/zeroentropy-cc/legalbenchrag.   \n",
       "56   Retrieval-Augmented Generation (RAG) has been an effective approach to\\nmitigate hallucinations in large language models (LLMs) by incorporating\\nup-to-date and domain-specific knowledge. Recently, there has been a trend of\\nstoring up-to-date or copyrighted data in RAG knowledge databases instead of\\nusing it for LLM training. This practice has raised concerns about Membership\\nInference Attacks (MIAs), which aim to detect if a specific target document is\\nstored in the RAG system's knowledge database so as to protect the rights of\\ndata producers. While research has focused on enhancing the trustworthiness of\\nRAG systems, existing MIAs for RAG systems remain largely insufficient.\\nPrevious work either relies solely on the RAG system's judgment or is easily\\ninfluenced by other documents or the LLM's internal knowledge, which is\\nunreliable and lacks explainability. To address these limitations, we propose a\\nMask-Based Membership Inference Attacks (MBA) framework. Our framework first\\nemploys a masking algorithm that effectively masks a certain number of words in\\nthe target document. The masked text is then used to prompt the RAG system, and\\nthe RAG system is required to predict the mask values. If the target document\\nappears in the knowledge database, the masked text will retrieve the complete\\ntarget document as context, allowing for accurate mask prediction. Finally, we\\nadopt a simple yet effective threshold-based method to infer the membership of\\ntarget document by analyzing the accuracy of mask prediction. Our mask-based\\napproach is more document-specific, making the RAG system's generation less\\nsusceptible to distractions from other documents or the LLM's internal\\nknowledge. Extensive experiments demonstrate the effectiveness of our approach\\ncompared to existing baseline models.   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We present RAG Playground, an open-source framework for systematic evaluation\\nof Retrieval-Augmented Generation (RAG) systems. The framework implements and\\ncompares three retrieval approaches: naive vector search, reranking, and hybrid\\nvector-keyword search, combined with ReAct agents using different prompting\\nstrategies. We introduce a comprehensive evaluation framework with novel\\nmetrics and provide empirical results comparing different language models\\n(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our\\nexperiments demonstrate significant performance improvements through hybrid\\nsearch methods and structured self-evaluation prompting, achieving up to 72.7%\\npass rate on our multi-metric evaluation framework. The results also highlight\\nthe importance of prompt engineering in RAG systems, with our custom-prompted\\nagents showing consistent improvements in retrieval accuracy and response\\nquality.   \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We designed a Retrieval-Augmented Generation (RAG) system to provide large\\nlanguage models with relevant documents for answering domain-specific questions\\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\\nsubpages using a greedy scraping strategy and employed a hybrid annotation\\nprocess, combining manual and Mistral-generated question-answer pairs,\\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\\ndocument retrieval accuracy. Experimental results show that the RAG system\\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\\nenhancing answer precision and relevance, while identifying areas for further\\noptimization in document retrieval and model training.   \n",
       "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This paper presents a comprehensive study of Retrieval-Augmented Generation\\n(RAG), tracing its evolution from foundational concepts to the current state of\\nthe art. RAG combines retrieval mechanisms with generative language models to\\nenhance the accuracy of outputs, addressing key limitations of LLMs. The study\\nexplores the basic architecture of RAG, focusing on how retrieval and\\ngeneration are integrated to handle knowledge-intensive tasks. A detailed\\nreview of the significant technological advancements in RAG is provided,\\nincluding key innovations in retrieval-augmented language models and\\napplications across various domains such as question-answering, summarization,\\nand knowledge-based tasks. Recent research breakthroughs are discussed,\\nhighlighting novel methods for improving retrieval efficiency. Furthermore, the\\npaper examines ongoing challenges such as scalability, bias, and ethical\\nconcerns in deployment. Future research directions are proposed, focusing on\\nimproving the robustness of RAG models, expanding the scope of application of\\nRAG models, and addressing societal implications. This survey aims to serve as\\na foundational resource for researchers and practitioners in understanding the\\npotential of RAG and its trajectory in natural language processing.   \n",
       "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\\nof artificial intelligence. However, their tendency to hallucinate -- generate\\nplausible but false information -- poses a significant challenge. This issue is\\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\\nGeneration (RAG) can counter hallucinations by integrating external knowledge\\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\\nin some cases, but can still be misled when prompts directly contradict the\\nmodel's pre-trained understanding. These findings highlight the complex nature\\nof hallucinations and the need for more robust solutions to ensure LLM\\nreliability in real-world applications. We offer practical recommendations for\\nRAG deployment and discuss implications for the development of more trustworthy\\nLLMs.   \n",
       "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We introduce Planning-guided Retrieval Augmented Generation\\n(Plan$\\times$RAG), a novel framework that augments the\\n\\emph{retrieve-then-reason} paradigm of existing RAG frameworks to\\n\\emph{plan-then-retrieve}. Plan$\\times$RAG formulates a reasoning plan as a\\ndirected acyclic graph (DAG), decomposing queries into interrelated atomic\\nsub-queries. Answer generation follows the DAG structure, allowing significant\\ngains in efficiency through parallelized retrieval and generation. While\\nstate-of-the-art RAG solutions require extensive data generation and\\nfine-tuning of language models (LMs), Plan$\\times$RAG incorporates frozen LMs\\nas plug-and-play experts to generate high-quality answers. Compared to existing\\nRAG solutions, Plan$\\times$RAG demonstrates significant improvements in\\nreducing hallucinations and bolstering attribution due to its structured\\nsub-query decomposition. Overall, Plan$\\times$RAG offers a new perspective on\\nintegrating external knowledge in LMs while ensuring attribution by design,\\ncontributing towards more reliable LM-based systems.   \n",
       "112                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Software engineers are increasingly adding semantic search capabilities to\\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\\nRAG system involves finding documents that semantically match a query and then\\npassing the documents to a large language model (LLM) such as ChatGPT to\\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\\nproblem of hallucinated responses from LLMs, b) link sources/references to\\ngenerated responses, and c) remove the need for annotating documents with\\nmeta-data. However, RAG systems suffer from limitations inherent to information\\nretrieval systems and from reliance on LLMs. In this paper, we present an\\nexperience report on the failure points of RAG systems from three case studies\\nfrom separate domains: research, education, and biomedical. We share the\\nlessons learned and present 7 failure points to consider when designing a RAG\\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\\nsystem is only feasible during operation, and 2) the robustness of a RAG system\\nevolves rather than designed in at the start. We conclude with a list of\\npotential research directions on RAG systems for the software engineering\\ncommunity.   \n",
       "113                                                                                                                                                                                                                                                                                                                                                                     Federated Recommendation (FR) emerges as a novel paradigm that enables\\nprivacy-preserving recommendations. However, traditional FR systems usually\\nrepresent users/items with discrete identities (IDs), suffering from\\nperformance degradation due to the data sparsity and heterogeneity in FR. On\\nthe other hand, Large Language Models (LLMs) as recommenders have proven\\neffective across various recommendation scenarios. Yet, LLM-based recommenders\\nencounter challenges such as low inference efficiency and potential\\nhallucination, compromising their performance in real-world scenarios. To this\\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\\nprocess, mining ID-based user patterns and text-based item features. Next, the\\nretrieved results are converted into text prompts and fed into GPT for\\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\\nto extract generalized features from data and exploit pretrained knowledge\\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\\nRAG approach also prevents LLM hallucination, improving the recommendation\\nperformance for real-world users. Experimental results on diverse benchmark\\ndatasets demonstrate the superior performance of GPT-FedRec against\\nstate-of-the-art baseline methods.   \n",
       "115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Retrieval Augmented Generation (RAG) has become a popular application for\\nlarge language models. It is preferable that successful RAG systems provide\\naccurate answers that are supported by being grounded in a passage without any\\nhallucinations. While considerable work is required for building a full RAG\\npipeline, being able to benchmark performance is also necessary. We present\\nClapNQ, a benchmark Long-form Question Answering dataset for the full RAG\\npipeline. ClapNQ includes long answers with grounded gold passages from Natural\\nQuestions (NQ) and a corpus to perform either retrieval, generation, or the\\nfull RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full\\npassage, and cohesive, meaning that the answer is composed fluently, often by\\nintegrating multiple pieces of the passage that are not contiguous. RAG models\\nmust adapt to these properties to be successful at ClapNQ. We present baseline\\nexperiments and analysis for ClapNQ that highlight areas where there is still\\nsignificant room for improvement in grounded RAG. CLAPNQ is publicly available\\nat https://github.com/primeqa/clapnq   \n",
       "119                                                                                                                                                                                                                                                                                                                                                                    Organizations seeking to utilize Large Language Models (LLMs) for knowledge\\nquerying and analysis often encounter challenges in maintaining an LLM\\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\\nsolution for organizations looking to overcome the challenges of maintaining\\nproprietary models and to help reduce LLM hallucinations in their query\\nresponses. However, RAG comes with its own issues regarding scaling data\\npipelines across tiered-access and disparate data sources. In many scenarios,\\nit is necessary to query beyond a single data silo to provide richer and more\\nrelevant context for an LLM. Analyzing data sources within and across\\norganizational trust boundaries is often limited by complex data-sharing\\npolicies that prohibit centralized data storage, therefore, inhibit the fast\\nand effective setup and scaling of RAG solutions. In this paper, we introduce\\nConfidential Computing (CC) techniques as a solution for secure Federated\\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\\nacross a decentralized network of data providers by ensuring context\\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\\nMIRAGE benchmarking dataset.   \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Despite their powerful chat, coding, and reasoning abilities, Large Language\\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\\nhallucinations are a consequence of a balance between creativity and\\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\\nexternal knowledge sources. Through extensive systematic experiments, we show\\nthat these traditional approaches fail to explain why LLMs hallucinate in\\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\\ncorroborate these experimental findings with a theoretical construction showing\\nthat simple neural networks trained to predict the next token hallucinate when\\nthe training loss is above a threshold as it usually does in practice when\\ntraining on internet scale data. We interpret our findings by comparing against\\ntraditional retrieval methods for mitigating hallucinations. We use our\\nfindings to design a first generation model for removing hallucinations --\\nLamini-1 -- that stores facts in a massive mixture of millions of memory\\nexperts that are retrieved dynamically.   \n",
       "132                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Retrieval-Augmented Generation (RAG) systems have emerged as a promising\\nsolution to mitigate LLM hallucinations and enhance their performance in\\nknowledge-intensive domains. However, these systems are vulnerable to\\nadversarial poisoning attacks, where malicious passages injected into retrieval\\ndatabases can mislead the model into generating factually incorrect outputs. In\\nthis paper, we investigate both the retrieval and the generation components of\\nRAG systems to understand how to enhance their robustness against such attacks.\\nFrom the retrieval perspective, we analyze why and how the adversarial contexts\\nare retrieved and assess how the quality of the retrieved passages impacts\\ndownstream generation. From a generation perspective, we evaluate whether LLMs'\\nadvanced critical thinking and internal knowledge capabilities can be leveraged\\nto mitigate the impact of adversarial contexts, i.e., using skeptical prompting\\nas a self-defense mechanism. Our experiments and findings provide actionable\\ninsights into designing safer and more resilient retrieval-augmented\\nframeworks, paving the way for their reliable deployment in real-world\\napplications.   \n",
       "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This study aims to improve the accuracy and quality of large-scale language\\nmodels (LLMs) in answering questions by integrating Elasticsearch into the\\nRetrieval Augmented Generation (RAG) framework. The experiment uses the\\nStanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\\ncompares the performance of different retrieval methods, including traditional\\nmethods based on keyword matching or semantic similarity calculation, BM25-RAG\\nand TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\\nES-RAG not only has obvious advantages in retrieval efficiency but also\\nperforms well in key indicators such as accuracy, which is 0.51 percentage\\npoints higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\\ncapabilities and rich configuration options enable the entire\\nquestion-answering system to better handle complex queries and provide more\\nflexible and efficient responses based on the diverse needs of users. Future\\nresearch directions can further explore how to optimize the interaction\\nmechanism between Elasticsearch and LLM, such as introducing higher-level\\nsemantic understanding and context-awareness capabilities, to achieve a more\\nintelligent and humanized question-answering experience.   \n",
       "172                                                                                                                                                                                                                                                                                                                                              Providing external knowledge to Large Language Models (LLMs) is a key point\\nfor using these models in real-world applications for several reasons, such as\\nincorporating up-to-date content in a real-time manner, providing access to\\ndomain-specific knowledge, and contributing to hallucination prevention. The\\nvector database-based Retrieval Augmented Generation (RAG) approach has been\\nwidely adopted to this end. Thus, any part of external knowledge can be\\nretrieved and provided to some LLM as the input context. Despite RAG approach's\\nsuccess, it still might be unfeasible for some applications, because the\\ncontext retrieved can demand a longer context window than the size supported by\\nLLM. Even when the context retrieved fits into the context window size, the\\nnumber of tokens might be expressive and, consequently, impact costs and\\nprocessing time, becoming impractical for most applications. To address these,\\nwe propose CRAG, a novel approach able to effectively reduce the number of\\nprompting tokens without degrading the quality of the response generated\\ncompared to a solution using RAG. Through our experiments, we show that CRAG\\ncan reduce the number of tokens by at least 46\\%, achieving more than 90\\% in\\nsome cases, compared to RAG. Moreover, the number of tokens with CRAG does not\\nincrease considerably when the number of reviews analyzed is higher, unlike\\nRAG, where the number of tokens is almost 9x higher when there are 75 reviews\\ncompared to 4 reviews.   \n",
       "177                                                                                                                                                                                                                                                   Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\\nintegrating external knowledge to reduce hallucinations and incorporate\\nup-to-date information without retraining. As an essential part of RAG,\\nexternal knowledge bases are commonly built by extracting structured data from\\nunstructured PDF documents using Optical Character Recognition (OCR). However,\\ngiven the imperfect prediction of OCR and the inherent non-uniform\\nrepresentation of structured data, knowledge bases inevitably contain various\\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350\\ncarefully selected unstructured PDF documents from six real-world RAG\\napplication domains, along with Q&As derived from multimodal elements in\\ndocuments, challenging existing OCR solutions used for RAG To better understand\\nOCR's impact on RAG systems, we identify two primary types of OCR noise:\\nSemantic Noise and Formatting Noise and apply perturbation to generate a set of\\nstructured data with varying degrees of each OCR noise. Using OHRBench, we\\nfirst conduct a comprehensive evaluation of current OCR solutions and reveal\\nthat none is competent for constructing high-quality knowledge bases for RAG\\nsystems. We then systematically evaluate the impact of these two noise types\\nand demonstrate the vulnerability of RAG systems. Furthermore, we discuss the\\npotential of employing Vision-Language Models (VLMs) without OCR in RAG\\nsystems. Code: https://github.com/opendatalab/OHR-Bench   \n",
       "\n",
       "     is_relevant  \n",
       "0              5  \n",
       "1              5  \n",
       "3              5  \n",
       "5              5  \n",
       "7              5  \n",
       "8              5  \n",
       "10             5  \n",
       "12             5  \n",
       "13             5  \n",
       "19             5  \n",
       "21             5  \n",
       "23             5  \n",
       "24             5  \n",
       "25             5  \n",
       "26             5  \n",
       "27             5  \n",
       "28             5  \n",
       "29             5  \n",
       "30             5  \n",
       "31             5  \n",
       "33             5  \n",
       "35             5  \n",
       "38             5  \n",
       "43             5  \n",
       "47             5  \n",
       "48             5  \n",
       "56             5  \n",
       "62             5  \n",
       "63             5  \n",
       "72             5  \n",
       "107            5  \n",
       "111            5  \n",
       "112            5  \n",
       "113            5  \n",
       "115            5  \n",
       "119            5  \n",
       "125            5  \n",
       "132            5  \n",
       "157            5  \n",
       "172            5  \n",
       "177            5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arixv_result_df.query('is_direct == True')\\\n",
    "    [['title','entry_id', 'summary', 'is_relevant']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
